{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiL4ZpxKxo1M"
      },
      "outputs": [],
      "source": [
        "# --- Cell 0: Fix package dependencies ---\n",
        "!pip install --force-reinstall numpy\n",
        "!pip install --force-reinstall pandas\n",
        "!pip install --force-reinstall matplotlib seaborn scikit-learn tqdm\n",
        "!pip install --force-reinstall torch torchvision\n",
        "!pip install --force-reinstall captum\n",
        "\n",
        "# Restart the runtime after running this cell\n",
        "# before running your import cell again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytViYIo5MT7_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQoz3NaKMU9q"
      },
      "outputs": [],
      "source": [
        "# --- Cell 0.B: Install Captum ---\n",
        "!pip install captum -q\n",
        "\n",
        "print(\"Captum installation attempt complete.\")\n",
        "print(\">>> CRITICAL: Please go to 'Runtime' -> 'Restart session' NOW one more time before running your main imports! <<<\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeZ-WwO5gkPh"
      },
      "outputs": [],
      "source": [
        "# --- Cell 1: Python Imports (Run this part AFTER pip installs and a manual Runtime Restart) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd # This is where the error was occurring\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
        ")\n",
        "try:\n",
        "    from captum.attr import IntegratedGradients, Saliency, NoiseTunnel\n",
        "except ImportError:\n",
        "    print(\"Captum not installed or import failed. Explanations requiring Captum will not be available.\")\n",
        "    IntegratedGradients = None\n",
        "    Saliency = None\n",
        "    NoiseTunnel = None\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "import warnings\n",
        "from typing import List, Tuple, Dict, Optional, Union, Callable, Any\n",
        "\n",
        "# General settings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "# Check if 'seaborn-v0_8-whitegrid' is a valid style, otherwise use a default like 'ggplot' or 'seaborn-v0_8-darkgrid'\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "except OSError:\n",
        "    print(\"Style 'seaborn-v0_8-whitegrid' not found, using 'ggplot'.\")\n",
        "    plt.style.use('ggplot')\n",
        "sns.set_palette('pastel')\n",
        "EPSILON = 1e-8\n",
        "\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(\"Cell 1: Python Imports executed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB-BhuXdht2C"
      },
      "outputs": [],
      "source": [
        "# --- Cell 2: Configuration and Device Setup ---\n",
        "\n",
        "# --- 1. General Settings ---\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- 2. Data Paths ---\n",
        "DRIVE_MOUNT_POINT = '/content/drive' # Or None if not using Colab/Drive\n",
        "BASE_DATA_PATH = os.path.join(DRIVE_MOUNT_POINT, 'MyDrive') if DRIVE_MOUNT_POINT and os.path.exists(os.path.join(DRIVE_MOUNT_POINT, 'MyDrive')) else './' # Default to current dir if path missing\n",
        "\n",
        "if not os.path.exists(BASE_DATA_PATH) and BASE_DATA_PATH != './':\n",
        "    print(f\"Warning: Default BASE_DATA_PATH '{BASE_DATA_PATH}' not found. Using current directory './' for data.\")\n",
        "    BASE_DATA_PATH = './'\n",
        "elif BASE_DATA_PATH != './':\n",
        "     print(f\"Using BASE_DATA_PATH: {BASE_DATA_PATH}\")\n",
        "\n",
        "\n",
        "DATA_PATHS = {\n",
        "    'mitbih_train': os.path.join(BASE_DATA_PATH, 'mitbih_train.csv'),\n",
        "    'mitbih_test': os.path.join(BASE_DATA_PATH, 'mitbih_test.csv'),\n",
        "    'ptbdb_normal': os.path.join(BASE_DATA_PATH, 'ptbdb_normal.csv'),\n",
        "    'ptbdb_abnormal': os.path.join(BASE_DATA_PATH, 'ptbdb_abnormal.csv'),\n",
        "}\n",
        "\n",
        "# --- 3. Model & Training Hyperparameters ---\n",
        "# These can be overridden for specific dataset experiments\n",
        "GENERAL_TRAINING_CONFIG = {\n",
        "    'epochs': 15, # Increased epochs, with early stopping\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 1e-4,\n",
        "    'patience': 10, # Early stopping patience\n",
        "    'checkpoint_dir': './checkpoints', # Directory to save model checkpoints\n",
        "}\n",
        "os.makedirs(GENERAL_TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
        "\n",
        "# For Stage 1 (Best Base Classifier), we set loss_beta to 0.0 and deferral_threshold_train to a very large value (effectively infinity)\n",
        "CGD_MODEL_CONFIG = {\n",
        "    'input_dim': 1,\n",
        "    'latent_dim': 64,\n",
        "    'deferral_threshold_train': 1e9,\n",
        "    'loss_alpha': 0.0,\n",
        "    'loss_beta': 0.0,\n",
        "    'defer_cost_factor': 0.3,        #\n",
        "    'max_seq_length': 187,\n",
        "}\n",
        "\n",
        "\n",
        "ENCODER_CONFIG = {\n",
        "    'embed_dim': CGD_MODEL_CONFIG['latent_dim'],\n",
        "    'num_heads': 4,\n",
        "    'num_layers': 3,\n",
        "    'dim_feedforward_factor': 4,\n",
        "    'dropout': 0.15,\n",
        "    'activation': 'gelu',\n",
        "    'aggregation_method': 'mean', # 'mean', 'last', 'cls'\n",
        "}\n",
        "\n",
        "PREDICTOR_CONFIG = {\n",
        "    # output_dim will be set per dataset\n",
        "    'hidden_dims': [CGD_MODEL_CONFIG['latent_dim'] * 2, CGD_MODEL_CONFIG['latent_dim']], # Larger hidden layers\n",
        "    'dropout': 0.2,\n",
        "    # activation will be 'linear' for CrossEntropyLoss/BCEWithLogitsLoss\n",
        "}\n",
        "\n",
        "STRUCTURAL_REGULARIZER_CONFIG = {\n",
        "    'regularization_type': 'contrastive', # 'contrastive', 'prototype', 'none'\n",
        "    'temperature': 0.1,\n",
        "    # For 'prototype'\n",
        "    # 'num_prototypes': 10,\n",
        "    # 'prototype_lambda': 0.1\n",
        "}\n",
        "\n",
        "\n",
        "# Experiment Option 2: ECG-Relevant Input Masking + Mean Displacement\n",
        "# --- In Cell 2: Example Configuration 1 ---\n",
        "PERTURBATION_CONFIG = {\n",
        "    'active_types': ['input_masking'],    # Only input masking\n",
        "    'gaussian_noise_level': 0.1,\n",
        "    'feature_dropout_rate': 0.1,\n",
        "    'temporal_swap_rate': 0.05,\n",
        "    'input_masking_rate': 0.15,           # Mask 15% of the sequence\n",
        "    'input_masking_chunk_size': 18,       # Chunk size ~10% of 187 steps, could mask a beat component\n",
        "    'num_perturbations': 15,\n",
        "}\n",
        "\n",
        "SENSITIVITY_CONFIG = {\n",
        "    'active_measures': ['mean_displacement'], # Only mean displacement\n",
        "    'aggregation_method_for_multiple_sensitivities': 'mean',\n",
        "}\n",
        "\n",
        "\n",
        "ADAPTIVE_THRESHOLD_CONFIG = {\n",
        "    'method': 'max_acc_under_budget',\n",
        "    'percentile_value_for_threshold': 90, # Not used by this method\n",
        "    'target_defer_rate_value': 0.10,  # Not used by this method\n",
        "    'max_defer_rate_budget': 0.25,    # 0.25 means don't defer more than 25%\n",
        "    'num_threshold_candidates': 200   # Usually fine\n",
        "}\n",
        "\n",
        "EXPLAINER_CONFIG = {\n",
        "    'method': 'integrated_gradients', # 'saliency', 'integrated_gradients'\n",
        "    'n_steps_ig': 25, # For Integrated Gradients\n",
        "    'noise_tunnel_nt_type': 'smoothgrad_sq', # For NoiseTunnel with Saliency\n",
        "    'noise_tunnel_stdevs': 0.1,           # For NoiseTunnel\n",
        "    'noise_tunnel_nt_samples': 5,         # For NoiseTunnel\n",
        "}\n",
        "\n",
        "DEFERRAL_HEAD_CONFIG = {\n",
        "    'hidden_dims_deferral_head': [64, 32], # Structure of the Deferral Head MLP\n",
        "    'dropout_deferral_head': 0.2,\n",
        "    'learning_rate': 1e-3,      # LR for training the Deferral Head\n",
        "    'epochs': 50,               # Max epochs for training the Deferral Head\n",
        "    'batch_size': 256,          # Batch size for Deferral Head training data\n",
        "    'patience': 5,              # Early stopping patience for Deferral Head\n",
        "    'target_type': 'error_prediction' # Currently only 'error_prediction' is fully fleshed out\n",
        "}\n",
        "\n",
        "print(\"\\n--- Configurations ---\")\n",
        "print(f\"DEVICE: {DEVICE}\")\n",
        "print(f\"DATA_PATHS: {DATA_PATHS}\")\n",
        "print(f\"GENERAL_TRAINING_CONFIG: {GENERAL_TRAINING_CONFIG}\")\n",
        "print(f\"CGD_MODEL_CONFIG: {CGD_MODEL_CONFIG}\")\n",
        "print(f\"ENCODER_CONFIG: {ENCODER_CONFIG}\")\n",
        "print(f\"PREDICTOR_CONFIG: {PREDICTOR_CONFIG}\")\n",
        "print(f\"STRUCTURAL_REGULARIZER_CONFIG: {STRUCTURAL_REGULARIZER_CONFIG}\")\n",
        "print(f\"PERTURBATION_CONFIG: {PERTURBATION_CONFIG}\")\n",
        "print(f\"SENSITIVITY_CONFIG: {SENSITIVITY_CONFIG}\")\n",
        "print(f\"ADAPTIVE_THRESHOLD_CONFIG: {ADAPTIVE_THRESHOLD_CONFIG}\")\n",
        "print(f\"EXPLAINER_CONFIG: {EXPLAINER_CONFIG}\")\n",
        "\n",
        "print(\"\\nCell 2: Configuration and Device Setup executed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVPGojCjiKXI"
      },
      "outputs": [],
      "source": [
        "# --- Revised Cell 3: Unzip Data, Load, Preprocess, and Create DataLoaders ---\n",
        "import zipfile\n",
        "\n",
        "# --- 1. Unzip Dataset ---\n",
        "ZIP_DATASET_PATH = '/content/drive/MyDrive/archive.zip'\n",
        "EXTRACTION_DIR = './ecg_data_extracted/' # Directory to extract files into\n",
        "\n",
        "if os.path.exists(ZIP_DATASET_PATH):\n",
        "    print(f\"Found dataset zip file at: {ZIP_DATASET_PATH}\")\n",
        "    os.makedirs(EXTRACTION_DIR, exist_ok=True)\n",
        "    try:\n",
        "        with zipfile.ZipFile(ZIP_DATASET_PATH, 'r') as zip_ref:\n",
        "            zip_ref.extractall(EXTRACTION_DIR)\n",
        "        print(f\"Successfully extracted dataset to: {EXTRACTION_DIR}\")\n",
        "\n",
        "        DATA_PATHS = {\n",
        "            'mitbih_train': os.path.join(EXTRACTION_DIR, 'mitbih_train.csv'),\n",
        "            'mitbih_test': os.path.join(EXTRACTION_DIR, 'mitbih_test.csv'),\n",
        "            'ptbdb_normal': os.path.join(EXTRACTION_DIR, 'ptbdb_normal.csv'),\n",
        "            'ptbdb_abnormal': os.path.join(EXTRACTION_DIR, 'ptbdb_abnormal.csv'),\n",
        "        }\n",
        "        print(f\"Updated DATA_PATHS to use extracted files: {DATA_PATHS}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error unzipping {ZIP_DATASET_PATH}: {e}\")\n",
        "        print(\"Proceeding with potentially empty or previously defined DATA_PATHS from Cell 2.\")\n",
        "else:\n",
        "    print(f\"Warning: Dataset zip file not found at {ZIP_DATASET_PATH}.\")\n",
        "    print(\"Ensure the path is correct or data is already extracted and DATA_PATHS (from Cell 2) are set accordingly.\")\n",
        "    print(\"If DATA_PATHS are not set to existing CSVs, dummy data logic might be triggered later.\")\n",
        "\n",
        "\n",
        "# --- 2. Data Loading & Preprocessing Utilities (same as before) ---\n",
        "\n",
        "def load_raw_ecg_data(data_paths: Dict[str, str]) -> Dict[str, Optional[pd.DataFrame]]:\n",
        "    \"\"\"Loads raw ECG data from specified CSV file paths.\"\"\"\n",
        "    raw_data = {}\n",
        "    print(\"\\nLoading raw ECG data...\")\n",
        "    for key, path in data_paths.items():\n",
        "        try:\n",
        "            if not os.path.exists(path):\n",
        "                print(f\"Warning: File not found for {key} at {path}. Skipping.\")\n",
        "                raw_data[key] = None\n",
        "                continue\n",
        "            raw_data[key] = pd.read_csv(path, header=None)\n",
        "            print(f\"Successfully loaded {key} from {path}, shape: {raw_data[key].shape if raw_data[key] is not None else 'N/A'}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found for {key} at {path}. Returning None for this key.\")\n",
        "            raw_data[key] = None\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while loading {key} from {path}: {e}\")\n",
        "            raw_data[key] = None\n",
        "\n",
        "    # Specifically combine PTB normal and abnormal\n",
        "    # The CSVs have 188 columns. Last col (187) is the label for MIT-BIH.\n",
        "    # For PTB, we'll add a new label column.\n",
        "    ptb_normal_df = raw_data.get('ptbdb_normal')\n",
        "    ptb_abnormal_df = raw_data.get('ptbdb_abnormal')\n",
        "\n",
        "    if ptb_normal_df is not None and ptb_abnormal_df is not None:\n",
        "        # PTB data has 188 columns, features are 0 to 186, col 187 is a heartbeat indicator (mostly 1.0)\n",
        "        # We will use cols 0-186 as features.\n",
        "        ptb_normal_df_features = ptb_normal_df.iloc[:, :CGD_MODEL_CONFIG['max_seq_length']]\n",
        "        ptb_abnormal_df_features = ptb_abnormal_df.iloc[:, :CGD_MODEL_CONFIG['max_seq_length']]\n",
        "\n",
        "        ptb_normal_df_processed = ptb_normal_df_features.copy()\n",
        "        ptb_normal_df_processed['label'] = 0  # Add normal label\n",
        "\n",
        "        ptb_abnormal_df_processed = ptb_abnormal_df_features.copy()\n",
        "        ptb_abnormal_df_processed['label'] = 1 # Add abnormal label\n",
        "\n",
        "        raw_data['ptbdb_combined'] = pd.concat([ptb_normal_df_processed, ptb_abnormal_df_processed], axis=0, ignore_index=True)\n",
        "        print(f\"Combined PTB dataset created, shape: {raw_data['ptbdb_combined'].shape}\")\n",
        "        # We will remove individual ptb normal/abnormal if combined successfully\n",
        "        if 'ptbdb_normal' in raw_data: del raw_data['ptbdb_normal']\n",
        "        if 'ptbdb_abnormal' in raw_data: del raw_data['ptbdb_abnormal']\n",
        "    elif 'ptbdb_normal' in data_paths or 'ptbdb_abnormal' in data_paths : # check if keys exist in original paths\n",
        "        print(\"Warning: Could not combine PTB datasets as one or both parts are missing/failed to load.\")\n",
        "        raw_data['ptbdb_combined'] = None\n",
        "    return raw_data\n",
        "\n",
        "def preprocess_ecg_features(\n",
        "    df: pd.DataFrame,\n",
        "    is_train: bool = True,\n",
        "    scaler: Optional[StandardScaler] = None,\n",
        "    expected_features: int = 187 # MIT-BIH and PTB datasets have 187 features\n",
        ") -> Tuple[np.ndarray, np.ndarray, Optional[StandardScaler]]:\n",
        "    \"\"\"\n",
        "    Scales the ECG features. Assumes features are in columns 0 to expected_features-1,\n",
        "    and the last column of the input df is the label.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(\"Warning: preprocess_ecg_features received an empty or None DataFrame.\")\n",
        "        return np.array([]), np.array([]), scaler\n",
        "\n",
        "    X = df.iloc[:, :expected_features].values\n",
        "    y = df.iloc[:, -1].values # Assumes label is the actual last column\n",
        "\n",
        "    if X.shape[1] != expected_features:\n",
        "        raise ValueError(f\"Expected {expected_features} features, but got {X.shape[1]} from DataFrame slice.\")\n",
        "\n",
        "    if is_train:\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "    else:\n",
        "        if scaler is None:\n",
        "            raise ValueError(\"Scaler must be provided for non-train (test/validation) data.\")\n",
        "        X_scaled = scaler.transform(X)\n",
        "    return X_scaled, y.astype(int), scaler\n",
        "\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"Generic Dataset for time series (adapted for ECG fixed length).\"\"\"\n",
        "    def __init__(self, sequences: np.ndarray, labels: np.ndarray, num_classes_for_problem: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequences: Array of sequences [num_samples, seq_length]. Expected to be scaled.\n",
        "            labels: Array of corresponding labels [num_samples].\n",
        "            num_classes_for_problem: Number of unique classes for the problem (e.g., 2 for PTB, 5 for MIT-BIH).\n",
        "        \"\"\"\n",
        "        if sequences.ndim == 2:\n",
        "            self.sequences = torch.from_numpy(sequences).float().unsqueeze(-1) # [N, S, 1]\n",
        "        elif sequences.ndim == 3 and sequences.shape[-1] == 1:\n",
        "             self.sequences = torch.from_numpy(sequences).float()\n",
        "        else:\n",
        "            raise ValueError(f\"Sequences should be 2D or 3D with last dim 1, got {sequences.shape}\")\n",
        "\n",
        "        if num_classes_for_problem == 2: # Binary classification\n",
        "             self.labels = torch.from_numpy(labels).float() # For BCEWithLogitsLoss\n",
        "        else: # Multiclass classification\n",
        "             self.labels = torch.from_numpy(labels).long()  # For CrossEntropyLoss\n",
        "        self.num_classes = num_classes_for_problem\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "def collate_fn_fixed_length(\n",
        "    batch: List[Tuple[torch.Tensor, torch.Tensor]]\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    sequences, labels = zip(*batch)\n",
        "    sequences_stacked = torch.stack(sequences)\n",
        "    labels_stacked = torch.stack(labels)\n",
        "    batch_size, seq_length, _ = sequences_stacked.shape\n",
        "    padding_mask = torch.zeros(batch_size, seq_length, dtype=torch.bool)\n",
        "    return sequences_stacked, labels_stacked, padding_mask\n",
        "\n",
        "def prepare_dataloaders(\n",
        "    X_train: np.ndarray, y_train: np.ndarray,\n",
        "    X_val: np.ndarray, y_val: np.ndarray,\n",
        "    num_classes: int, # Number of classes for the specific problem\n",
        "    X_test: Optional[np.ndarray] = None, y_test: Optional[np.ndarray] = None,\n",
        "    batch_size: int = 64, num_workers: int = 2\n",
        ") -> Dict[str, DataLoader]:\n",
        "    if X_train.size == 0 or X_val.size == 0 :\n",
        "        raise ValueError(\"Train or Validation data is empty. Cannot create DataLoaders.\")\n",
        "\n",
        "    train_dataset = TimeSeriesDataset(X_train, y_train, num_classes_for_problem=num_classes)\n",
        "    val_dataset = TimeSeriesDataset(X_val, y_val, num_classes_for_problem=num_classes)\n",
        "\n",
        "    dataloaders = {\n",
        "        'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_fixed_length, num_workers=num_workers, pin_memory=True if DEVICE.type == 'cuda' else False),\n",
        "        'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_fixed_length, num_workers=num_workers, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
        "    }\n",
        "\n",
        "    if X_test is not None and y_test is not None and X_test.size > 0:\n",
        "        test_dataset = TimeSeriesDataset(X_test, y_test, num_classes_for_problem=num_classes)\n",
        "        dataloaders['test'] = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_fixed_length, num_workers=num_workers, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
        "    elif X_test is not None or y_test is not None :\n",
        "        print(\"Warning: Test data or labels are partially provided. Test DataLoader not created.\")\n",
        "\n",
        "    print(f\"Created DataLoaders for a {num_classes}-class problem: Train batches={len(dataloaders['train'])}, Val batches={len(dataloaders['val'])}\" +\n",
        "          (f\", Test batches={len(dataloaders['test'])}\" if 'test' in dataloaders else \"\"))\n",
        "    return dataloaders\n",
        "\n",
        "# --- 3. Load and Process Data (using the actual data if unzipped) ---\n",
        "# This block will now attempt to load and process the actual data.\n",
        "# If unzipping failed or DATA_PATHS are incorrect, it might try to load non-existent files or previously created dummies.\n",
        "\n",
        "print(\"\\n--- Processing Datasets ---\")\n",
        "raw_dfs = load_raw_ecg_data(DATA_PATHS) # DATA_PATHS should be updated if unzipping was successful\n",
        "\n",
        "# Process MIT-BIH\n",
        "mitbih_train_df = raw_dfs.get('mitbih_train')\n",
        "mitbih_test_df = raw_dfs.get('mitbih_test')\n",
        "mitbih_loaders = None\n",
        "expected_mitbih_features = CGD_MODEL_CONFIG['max_seq_length'] # Should be 187\n",
        "\n",
        "if mitbih_train_df is not None and mitbih_test_df is not None:\n",
        "    print(f\"\\nProcessing MIT-BIH (expecting {expected_mitbih_features} features)...\")\n",
        "    # For MIT-BIH, the last column (index 187) is the label. Features are columns 0-186.\n",
        "    X_mtrain, y_mtrain, scaler_m = preprocess_ecg_features(mitbih_train_df, is_train=True, expected_features=expected_mitbih_features)\n",
        "    # Use a portion of the original test set as validation, and the rest as test\n",
        "    # Or, if you want to use the full mitbih_test_df as validation like in the original script:\n",
        "    X_mval_full, y_mval_full, _ = preprocess_ecg_features(mitbih_test_df, is_train=False, scaler=scaler_m, expected_features=expected_mitbih_features)\n",
        "\n",
        "    if X_mval_full.shape[0] > 10: # Ensure there's enough data to split\n",
        "        val_size_mit = int(0.5 * X_mval_full.shape[0]) # Splitting the original test set\n",
        "        X_mval, X_mtest = X_mval_full[:val_size_mit], X_mval_full[val_size_mit:]\n",
        "        y_mval, y_mtest = y_mval_full[:val_size_mit], y_mval_full[val_size_mit:]\n",
        "        print(f\"MIT-BIH original test set split into: Val ({X_mval.shape[0]} samples), Test ({X_mtest.shape[0]} samples)\")\n",
        "    else: # Not enough data, use all for validation, none for test\n",
        "        X_mval, y_mval = X_mval_full, y_mval_full\n",
        "        X_mtest, y_mtest = None, None\n",
        "        print(\"MIT-BIH original test set used entirely for validation due to small size.\")\n",
        "\n",
        "\n",
        "    if X_mtrain.size > 0 and X_mval.size > 0:\n",
        "        print(f\"MIT-BIH scaled shapes: X_train={X_mtrain.shape}, y_train={y_mtrain.shape}, X_val={X_mval.shape}, y_val={y_mval.shape}\" + (f\", X_test={X_mtest.shape}, y_test={y_mtest.shape}\" if X_mtest is not None else \"\"))\n",
        "        mitbih_loaders = prepare_dataloaders(\n",
        "            X_mtrain, y_mtrain, X_mval, y_mval, X_test=X_mtest, y_test=y_mtest,\n",
        "            num_classes=5, batch_size=GENERAL_TRAINING_CONFIG['batch_size']\n",
        "        )\n",
        "        X_b, y_b, m_b = next(iter(mitbih_loaders['train']))\n",
        "        print(f\"MIT-BIH sample train batch: X_shape={X_b.shape}, y_shape={y_b.shape}, mask_shape={m_b.shape}, y_dtype={y_b.dtype}\")\n",
        "    else:\n",
        "        print(\"Skipping MIT-BIH DataLoader creation due to empty preprocessed data.\")\n",
        "else:\n",
        "    print(\"Skipping MIT-BIH processing (raw data missing or failed to load).\")\n",
        "\n",
        "# Process PTB\n",
        "ptb_combined_df = raw_dfs.get('ptbdb_combined')\n",
        "ptb_loaders = None\n",
        "expected_ptb_features = CGD_MODEL_CONFIG['max_seq_length'] # Should be 187\n",
        "\n",
        "if ptb_combined_df is not None:\n",
        "    print(f\"\\nProcessing PTB (expecting {expected_ptb_features} features)...\")\n",
        "    # Stratified split of the combined PTB data first\n",
        "    ptb_train_val_df, ptb_test_df = train_test_split(ptb_combined_df, test_size=0.2, random_state=SEED, stratify=ptb_combined_df.iloc[:, -1])\n",
        "    ptb_train_df, ptb_val_df = train_test_split(ptb_train_val_df, test_size=0.2, random_state=SEED, stratify=ptb_train_val_df.iloc[:, -1]) # 0.2 of (0.8) = 0.16\n",
        "\n",
        "    X_ptrain, y_ptrain, scaler_p = preprocess_ecg_features(ptb_train_df, is_train=True, expected_features=expected_ptb_features)\n",
        "    X_pval, y_pval, _ = preprocess_ecg_features(ptb_val_df, is_train=False, scaler=scaler_p, expected_features=expected_ptb_features)\n",
        "    X_ptest, y_ptest, _ = preprocess_ecg_features(ptb_test_df, is_train=False, scaler=scaler_p, expected_features=expected_ptb_features)\n",
        "\n",
        "    if X_ptrain.size > 0 and X_pval.size > 0:\n",
        "        print(f\"PTB scaled shapes: X_train={X_ptrain.shape}, y_train={y_ptrain.shape}, X_val={X_pval.shape}, y_val={y_pval.shape}, X_test={X_ptest.shape}, y_test={y_ptest.shape}\")\n",
        "        ptb_loaders = prepare_dataloaders(\n",
        "            X_ptrain, y_ptrain, X_pval, y_pval, X_test=X_ptest, y_test=y_ptest,\n",
        "            num_classes=2, batch_size=GENERAL_TRAINING_CONFIG['batch_size']\n",
        "        )\n",
        "        X_b_ptb, y_b_ptb, m_b_ptb = next(iter(ptb_loaders['train']))\n",
        "        print(f\"PTB sample train batch: X_shape={X_b_ptb.shape}, y_shape={y_b_ptb.shape}, mask_shape={m_b_ptb.shape}, y_dtype={y_b_ptb.dtype}\")\n",
        "    else:\n",
        "        print(\"Skipping PTB DataLoader creation due to empty preprocessed data.\")\n",
        "else:\n",
        "    print(\"Skipping PTB processing (raw combined data missing or failed to load).\")\n",
        "\n",
        "\n",
        "print(\"\\nCell 3 (Revised): Data Unzipping, Loading & Preprocessing Utilities executed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gE1mi-nkq7t"
      },
      "outputs": [],
      "source": [
        "# --- Corrected Cell 4: Core Model Components - Encoder ---\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Standard Sinusoidal Positional Encoding.\"\"\"\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500): # Increased default max_len\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1) # [max_len, 1]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # [d_model/2]\n",
        "\n",
        "        pe = torch.zeros(max_len, 1, d_model) # Shape [max_len, 1, d_model]\n",
        "        pe.requires_grad = False\n",
        "\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, embedding_dim] (if batch_first=True for Transformer)\n",
        "        Returns:\n",
        "            Tensor with positional encoding added.\n",
        "        \"\"\"\n",
        "        # x is expected to be [batch_size, seq_len, embedding_dim]\n",
        "        # self.pe is [max_len, 1, embedding_dim]\n",
        "        # We need to add pe[ :seq_len, 0, : ] to x\n",
        "        # so pe_slice should be [seq_len, 1, embedding_dim] -> broadcasted or expanded for addition\n",
        "\n",
        "        seq_len = x.size(1)\n",
        "        if seq_len > self.pe.size(0):\n",
        "            raise ValueError(f\"Input sequence length ({seq_len}) exceeds PositionalEncoding max_len ({self.pe.size(0)})\")\n",
        "\n",
        "        # self.pe[:seq_len] gives shape [seq_len, 1, d_model]\n",
        "        # x is [batch_size, seq_len, d_model]\n",
        "        # We want to add positional encoding to each sample in the batch.\n",
        "        # Permute x to [seq_len, batch_size, d_model] for easier broadcasting with pe[:seq_len]\n",
        "        x_permuted = x.permute(1, 0, 2) # -> [seq_len, batch_size, d_model]\n",
        "        x_permuted = x_permuted + self.pe[:seq_len, :, :] # Add pe slice (broadcasting the '1' dim)\n",
        "        x_final = x_permuted.permute(1, 0, 2) # -> [batch_size, seq_len, d_model]\n",
        "\n",
        "        return self.dropout(x_final)\n",
        "\n",
        "class TimeSeriesTransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based encoder for time-series data.\n",
        "    Uses configurations from ENCODER_CONFIG.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, config: Dict[str, Any], max_seq_len_data: int): # Renamed max_seq_len to max_seq_len_data\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embed_dim = config.get('embed_dim', 64)\n",
        "        self.aggregation_method = config.get('aggregation_method', 'mean')\n",
        "\n",
        "        # 1. Input Embedding/Projection\n",
        "        self.feature_embedding = nn.Linear(input_dim, self.embed_dim)\n",
        "\n",
        "        # 2. Determine max_len for positional encoding based on potential CLS token\n",
        "        pe_max_len = max_seq_len_data\n",
        "        if self.aggregation_method == 'cls':\n",
        "            pe_max_len += 1 # Account for the CLS token\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=self.embed_dim,\n",
        "            dropout=config.get('dropout', 0.1),\n",
        "            max_len=pe_max_len\n",
        "        )\n",
        "\n",
        "        # 3. Optional CLS Token\n",
        "        if self.aggregation_method == 'cls':\n",
        "            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
        "            nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        # 4. Transformer Encoder Layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.embed_dim,\n",
        "            nhead=config.get('num_heads', 4),\n",
        "            dim_feedforward=self.embed_dim * config.get('dim_feedforward_factor', 4),\n",
        "            dropout=config.get('dropout', 0.1),\n",
        "            activation=config.get('activation', 'relu'),\n",
        "            batch_first=True,\n",
        "            norm_first=config.get('norm_first', False)\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=config.get('num_layers', 2),\n",
        "            norm=nn.LayerNorm(self.embed_dim) if config.get('norm_first', False) else None\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        batch_size, seq_length_orig, _ = x.shape\n",
        "        x_embed = self.feature_embedding(x)\n",
        "\n",
        "        current_seq_length = seq_length_orig\n",
        "        if self.aggregation_method == 'cls':\n",
        "            cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "            x_embed = torch.cat((cls_tokens, x_embed), dim=1)\n",
        "            current_seq_length += 1\n",
        "            if src_key_padding_mask is not None:\n",
        "                cls_mask = torch.zeros(batch_size, 1, dtype=torch.bool, device=x.device)\n",
        "                src_key_padding_mask = torch.cat((cls_mask, src_key_padding_mask), dim=1)\n",
        "\n",
        "        # Add positional encoding - PositionalEncoding.forward expects [B, S, E]\n",
        "        x_pe = self.pos_encoder(x_embed)\n",
        "\n",
        "        z_seq = self.transformer_encoder(x_pe, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        if self.aggregation_method == 'mean':\n",
        "            if src_key_padding_mask is not None:\n",
        "                active_elements_mask = (~src_key_padding_mask).float().unsqueeze(-1)\n",
        "                masked_sum = (z_seq * active_elements_mask).sum(dim=1)\n",
        "                valid_seq_lengths = active_elements_mask.sum(dim=1)\n",
        "                valid_seq_lengths = torch.clamp(valid_seq_lengths, min=EPSILON)\n",
        "                z = masked_sum / valid_seq_lengths\n",
        "            else:\n",
        "                z = z_seq.mean(dim=1)\n",
        "        elif self.aggregation_method == 'last':\n",
        "            if src_key_padding_mask is not None:\n",
        "                sequence_lengths = (~src_key_padding_mask).sum(dim=1) - 1\n",
        "                sequence_lengths = torch.clamp(sequence_lengths, min=0)\n",
        "                z = z_seq[torch.arange(batch_size, device=x.device), sequence_lengths]\n",
        "            else:\n",
        "                # If CLS token was added and this is 'last', it might pick CLS if it's truly last.\n",
        "                # Usually 'last' implies no CLS token, or CLS is handled by 'cls' method.\n",
        "                # Assuming effective sequence length if no padding.\n",
        "                z = z_seq[:, current_seq_length -1, :] # takes the actual last token output\n",
        "        elif self.aggregation_method == 'cls':\n",
        "            z = z_seq[:, 0, :] # CLS token is at the beginning\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported aggregation_method: {self.aggregation_method}\")\n",
        "\n",
        "        return z\n",
        "\n",
        "# --- Example Usage (can be commented out after testing) ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Testing Encoder Components (Corrected) ---\")\n",
        "    # Use configs from Cell 2\n",
        "    test_batch_size = 4\n",
        "    test_seq_length_data = CGD_MODEL_CONFIG['max_seq_length'] # 187 (original data seq length)\n",
        "    test_input_dim = CGD_MODEL_CONFIG['input_dim']     # 1\n",
        "\n",
        "    dummy_x = torch.randn(test_batch_size, test_seq_length_data, test_input_dim).to(DEVICE)\n",
        "    dummy_padding_mask = torch.zeros(test_batch_size, test_seq_length_data, dtype=torch.bool).to(DEVICE)\n",
        "    if test_batch_size > 1 and test_seq_length_data > 10:\n",
        "        dummy_padding_mask[1, -10:] = True\n",
        "\n",
        "    print(f\"Dummy input shape: {dummy_x.shape}\")\n",
        "    print(f\"Dummy padding mask shape: {dummy_padding_mask.shape}\")\n",
        "\n",
        "    # Test TimeSeriesTransformerEncoder with 'mean'\n",
        "    try:\n",
        "        mean_encoder_config = ENCODER_CONFIG.copy()\n",
        "        mean_encoder_config['aggregation_method'] = 'mean'\n",
        "        encoder_mean = TimeSeriesTransformerEncoder(\n",
        "            input_dim=test_input_dim,\n",
        "            config=mean_encoder_config,\n",
        "            max_seq_len_data=test_seq_length_data # Pass original data max length\n",
        "        ).to(DEVICE)\n",
        "        encoder_mean.eval()\n",
        "        with torch.no_grad():\n",
        "            output_z_mean = encoder_mean(dummy_x, src_key_padding_mask=dummy_padding_mask)\n",
        "        print(f\"Encoder output shape (aggregation='mean'): {output_z_mean.shape}\")\n",
        "        assert output_z_mean.shape == (test_batch_size, ENCODER_CONFIG.get('embed_dim'))\n",
        "        print(\"Encoder (mean aggregation) tested successfully.\")\n",
        "\n",
        "        # Test CLS token aggregation specifically if configured\n",
        "        cls_encoder_config = ENCODER_CONFIG.copy()\n",
        "        cls_encoder_config['aggregation_method'] = 'cls'\n",
        "        encoder_cls = TimeSeriesTransformerEncoder(\n",
        "            input_dim=test_input_dim,\n",
        "            config=cls_encoder_config,\n",
        "            max_seq_len_data=test_seq_length_data # Pass original data max length\n",
        "        ).to(DEVICE)\n",
        "        encoder_cls.eval()\n",
        "        with torch.no_grad():\n",
        "            output_z_cls = encoder_cls(dummy_x, src_key_padding_mask=dummy_padding_mask)\n",
        "        print(f\"Encoder output shape (aggregation='cls'): {output_z_cls.shape}\")\n",
        "        assert output_z_cls.shape == (test_batch_size, cls_encoder_config.get('embed_dim'))\n",
        "        print(\"Encoder (cls aggregation) tested successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during encoder component testing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\nCell 4 (Corrected): Core Model Components - Encoder executed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO_MNdN-lQ2G"
      },
      "source": [
        "# Cell 5: Core Model Components - Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgYDsxYFlSOx"
      },
      "outputs": [],
      "source": [
        "# --- Cell 5: Core Model Components - Predictor ---\n",
        "\n",
        "class CGDPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Prediction module (MLP) mapping latent space to output predictions.\n",
        "    Uses configurations from PREDICTOR_CONFIG.\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim: int, output_dim: int, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        hidden_dims = config.get('hidden_dims', [max(latent_dim // 2, 16), max(latent_dim // 4, 8)])\n",
        "        dropout_rate = config.get('dropout', 0.1)\n",
        "        # Final activation is 'linear' if using CrossEntropyLoss or BCEWithLogitsLoss\n",
        "        # as these losses prefer raw logits.\n",
        "        final_activation_name = config.get('activation', 'linear').lower()\n",
        "\n",
        "        layers = []\n",
        "        current_dim = latent_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(current_dim, h_dim))\n",
        "            layers.append(nn.BatchNorm1d(h_dim)) # Batch norm often helps stabilize MLP training\n",
        "            layers.append(nn.ReLU()) # Common activation for hidden layers\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            current_dim = h_dim\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(current_dim, output_dim))\n",
        "\n",
        "        # Add final activation if specified (and not 'linear'/'none')\n",
        "        if final_activation_name == 'sigmoid' and output_dim == 1:\n",
        "            layers.append(nn.Sigmoid())\n",
        "        elif final_activation_name == 'softmax' and output_dim > 1:\n",
        "            layers.append(nn.Softmax(dim=-1))\n",
        "        elif final_activation_name not in ['linear', 'none']:\n",
        "            print(f\"Warning: Unsupported final_activation '{final_activation_name}' for predictor, defaulting to linear.\")\n",
        "\n",
        "        self.predictor_mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            z: Latent representation [batch_size, latent_dim]\n",
        "        Returns:\n",
        "            y_pred: Prediction logits or probabilities [batch_size, output_dim]\n",
        "        \"\"\"\n",
        "        return self.predictor_mlp(z)\n",
        "\n",
        "# --- Example Usage (can be commented out after testing) ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Testing Predictor Component ---\")\n",
        "    # Use configs from Cell 2\n",
        "    test_batch_size = 4\n",
        "    test_latent_dim = CGD_MODEL_CONFIG['latent_dim'] # 64\n",
        "\n",
        "    # Test for MIT-BIH like (multiclass)\n",
        "    test_output_dim_mitbih = 5\n",
        "    mitbih_predictor_config = PREDICTOR_CONFIG.copy()\n",
        "    mitbih_predictor_config['activation'] = 'linear' # For CrossEntropyLoss\n",
        "\n",
        "    dummy_z_mitbih = torch.randn(test_batch_size, test_latent_dim).to(DEVICE)\n",
        "\n",
        "    try:\n",
        "        predictor_mitbih = CGDPredictor(\n",
        "            latent_dim=test_latent_dim,\n",
        "            output_dim=test_output_dim_mitbih,\n",
        "            config=mitbih_predictor_config\n",
        "        ).to(DEVICE)\n",
        "        predictor_mitbih.eval()\n",
        "        with torch.no_grad():\n",
        "            output_y_mitbih = predictor_mitbih(dummy_z_mitbih)\n",
        "        print(f\"Predictor output shape (MIT-BIH, 5 classes, linear activation): {output_y_mitbih.shape}\")\n",
        "        assert output_y_mitbih.shape == (test_batch_size, test_output_dim_mitbih)\n",
        "\n",
        "        # Test for PTB like (binary)\n",
        "        test_output_dim_ptb = 1\n",
        "        ptb_predictor_config = PREDICTOR_CONFIG.copy()\n",
        "        ptb_predictor_config['activation'] = 'linear' # For BCEWithLogitsLoss\n",
        "\n",
        "        dummy_z_ptb = torch.randn(test_batch_size, test_latent_dim).to(DEVICE)\n",
        "        predictor_ptb = CGDPredictor(\n",
        "            latent_dim=test_latent_dim,\n",
        "            output_dim=test_output_dim_ptb,\n",
        "            config=ptb_predictor_config\n",
        "        ).to(DEVICE)\n",
        "        predictor_ptb.eval()\n",
        "        with torch.no_grad():\n",
        "            output_y_ptb = predictor_ptb(dummy_z_ptb)\n",
        "        print(f\"Predictor output shape (PTB, 1 class, linear activation): {output_y_ptb.shape}\")\n",
        "        assert output_y_ptb.shape == (test_batch_size, test_output_dim_ptb)\n",
        "\n",
        "        print(\"Predictor component tested successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during predictor component testing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\nCell 5: Core Model Components - Predictor executed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36pOL4Y-lvdc"
      },
      "outputs": [],
      "source": [
        "# --- Cell 6: Perturbation Engine ---\n",
        "\n",
        "class PerturbationGenerator:\n",
        "    \"\"\"\n",
        "    Generates various types of perturbations on input time-series data.\n",
        "    Uses configurations from PERTURBATION_CONFIG.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.num_perturbations = config.get('num_perturbations', 10) # M\n",
        "\n",
        "    def _get_active_perturbations(self) -> List[str]:\n",
        "        \"\"\"Returns the list of perturbation types to apply.\"\"\"\n",
        "        active = self.config.get('active_types', ['gaussian_noise'])\n",
        "        if not isinstance(active, list):\n",
        "            active = [active]\n",
        "        return active\n",
        "\n",
        "    def _apply_gaussian_noise(self, X_expanded: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Adds Gaussian noise to each perturbed copy.\"\"\"\n",
        "        # X_expanded shape: [batch_size, num_perturbations, seq_length, input_dim]\n",
        "        noise_level = self.config.get('gaussian_noise_level', 0.1)\n",
        "        noise = torch.randn_like(X_expanded) * noise_level\n",
        "        return X_expanded + noise\n",
        "\n",
        "    def _apply_feature_dropout(self, X_expanded: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Applies dropout to features at each time step for each perturbed copy.\"\"\"\n",
        "        # X_expanded shape: [batch_size, num_perturbations, seq_length, input_dim]\n",
        "        dropout_rate = self.config.get('feature_dropout_rate', 0.1)\n",
        "        # Create a dropout mask for features.\n",
        "        # We want different masks for each of the M perturbations and each batch sample.\n",
        "        # Mask shape should be [B, M, S, D]\n",
        "        dropout_mask = (torch.rand_like(X_expanded) > dropout_rate).float()\n",
        "        # Apply dropout and scale (inverted dropout)\n",
        "        return (X_expanded * dropout_mask) / (1.0 - dropout_rate + EPSILON)\n",
        "\n",
        "    def _apply_temporal_swap(self, X_expanded: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Randomly swaps adjacent time steps in each perturbed copy.\"\"\"\n",
        "        # X_expanded shape: [batch_size, num_perturbations, seq_length, input_dim]\n",
        "        swap_rate = self.config.get('temporal_swap_rate', 0.05)\n",
        "        X_perturbed = X_expanded.clone()\n",
        "        batch_size, M, seq_length, _ = X_perturbed.shape\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for m in range(M):\n",
        "                for t in range(seq_length - 1):\n",
        "                    if torch.rand(1).item() < swap_rate:\n",
        "                        # Swap time steps t and t+1\n",
        "                        temp = X_perturbed[b, m, t, :].clone()\n",
        "                        X_perturbed[b, m, t, :] = X_perturbed[b, m, t + 1, :]\n",
        "                        X_perturbed[b, m, t + 1, :] = temp\n",
        "        return X_perturbed\n",
        "\n",
        "    def _apply_input_masking(self, X_expanded: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Zeros out random contiguous segments of the input time series.\"\"\"\n",
        "        # X_expanded shape: [batch_size, num_perturbations, seq_length, input_dim]\n",
        "        masking_rate = self.config.get('input_masking_rate', 0.1) # Proportion of total sequence to mask\n",
        "        chunk_size = self.config.get('input_masking_chunk_size', 10)\n",
        "\n",
        "        X_perturbed = X_expanded.clone()\n",
        "        batch_size, M, seq_length, _ = X_perturbed.shape\n",
        "\n",
        "        num_chunks_to_mask = math.ceil((seq_length * masking_rate) / chunk_size)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for m in range(M):\n",
        "                for _ in range(num_chunks_to_mask):\n",
        "                    if seq_length <= chunk_size : # Handle cases where seq_length is small\n",
        "                        start_idx = 0\n",
        "                        actual_chunk_size = seq_length\n",
        "                    else:\n",
        "                        start_idx = torch.randint(0, seq_length - chunk_size + 1, (1,)).item()\n",
        "                        actual_chunk_size = chunk_size\n",
        "\n",
        "                    X_perturbed[b, m, start_idx : start_idx + actual_chunk_size, :] = 0.0\n",
        "        return X_perturbed\n",
        "\n",
        "    def generate(self, X: torch.Tensor, padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generates perturbed versions of input X.\n",
        "        Args:\n",
        "            X: Input tensor [batch_size, seq_length, input_dim]\n",
        "            padding_mask: Boolean tensor [batch_size, seq_length] (True for padded elements).\n",
        "                          Perturbations should ideally not affect padded regions.\n",
        "        Returns:\n",
        "            X_perturbed_all: Tensor [batch_size, num_perturbations, seq_length, input_dim]\n",
        "        \"\"\"\n",
        "        if self.num_perturbations == 0:\n",
        "            return X.unsqueeze(1) # Return original if no perturbations requested\n",
        "\n",
        "        # Expand X for M perturbations: [B, S, D] -> [B, M, S, D]\n",
        "        X_expanded = X.unsqueeze(1).expand(-1, self.num_perturbations, -1, -1)\n",
        "\n",
        "        # Create a mask for non-padded elements if padding_mask is provided\n",
        "        # Shape: [B, 1, S, 1] for broadcasting, True for valid data\n",
        "        non_padding_mask_expanded = None\n",
        "        if padding_mask is not None:\n",
        "            non_padding_mask_expanded = (~padding_mask).float().unsqueeze(1).unsqueeze(-1)\n",
        "            # Ensure it can broadcast with X_expanded\n",
        "            # non_padding_mask_expanded = non_padding_mask_expanded.expand(-1, self.num_perturbations, -1, X.size(-1))\n",
        "\n",
        "        X_current_perturbations = X_expanded.clone()\n",
        "\n",
        "        active_perturb_types = self._get_active_perturbations()\n",
        "\n",
        "        for pert_type in active_perturb_types:\n",
        "            original_values = X_current_perturbations.clone() # Keep original for masked application\n",
        "            if pert_type == 'gaussian_noise':\n",
        "                X_current_perturbations = self._apply_gaussian_noise(X_current_perturbations)\n",
        "            elif pert_type == 'feature_dropout':\n",
        "                X_current_perturbations = self._apply_feature_dropout(X_current_perturbations)\n",
        "            elif pert_type == 'temporal_swap':\n",
        "                X_current_perturbations = self._apply_temporal_swap(X_current_perturbations)\n",
        "            elif pert_type == 'input_masking':\n",
        "                X_current_perturbations = self._apply_input_masking(X_current_perturbations)\n",
        "            # Add more elif for other perturbation types here\n",
        "            else:\n",
        "                print(f\"Warning: Unknown perturbation type '{pert_type}' in active_types. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Apply padding mask: restore original values in padded regions\n",
        "            if non_padding_mask_expanded is not None:\n",
        "                # Apply perturbations only to non-padded regions\n",
        "                X_current_perturbations = X_current_perturbations * non_padding_mask_expanded + \\\n",
        "                                          original_values * (1 - non_padding_mask_expanded)\n",
        "\n",
        "        return X_current_perturbations\n",
        "\n",
        "# --- Example Usage (can be commented out after testing) ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Testing Perturbation Engine ---\")\n",
        "    # Use configs from Cell 2\n",
        "    test_batch_size = 2\n",
        "    test_seq_length = CGD_MODEL_CONFIG['max_seq_length'] # 187\n",
        "    test_input_dim = CGD_MODEL_CONFIG['input_dim']     # 1\n",
        "\n",
        "    dummy_X_batch = torch.ones(test_batch_size, test_seq_length, test_input_dim).to(DEVICE) # Use ones for easy visual check of masking\n",
        "    dummy_X_batch[0, :10, :] = 5 # Make some parts different\n",
        "\n",
        "    # Create a dummy padding mask: last 5 elements of the first sample are padded\n",
        "    dummy_padding_mask_pert = torch.zeros(test_batch_size, test_seq_length, dtype=torch.bool).to(DEVICE)\n",
        "    if test_seq_length > 5:\n",
        "        dummy_padding_mask_pert[0, -5:] = True\n",
        "\n",
        "    print(f\"Original X_batch[0, -10:] before perturbation:\\n{dummy_X_batch[0, -10:].squeeze().cpu().numpy()}\")\n",
        "    if dummy_padding_mask_pert is not None:\n",
        "        print(f\"Padding mask for X_batch[0, -10:]: {dummy_padding_mask_pert[0, -10:].cpu().numpy()}\")\n",
        "\n",
        "\n",
        "    perturb_config_test = {\n",
        "        'active_types': ['gaussian_noise', 'input_masking', 'temporal_swap'],\n",
        "        'gaussian_noise_level': 0.1,\n",
        "        'input_masking_rate': 0.05, # Mask ~5%\n",
        "        'input_masking_chunk_size': 5,\n",
        "        'temporal_swap_rate': 0.1,\n",
        "        'num_perturbations': 3\n",
        "    }\n",
        "    generator = PerturbationGenerator(config=perturb_config_test)\n",
        "\n",
        "    try:\n",
        "        X_perturbed_batch = generator.generate(dummy_X_batch, padding_mask=dummy_padding_mask_pert)\n",
        "        print(f\"\\nPerturbed X_batch shape: {X_perturbed_batch.shape}\")\n",
        "        assert X_perturbed_batch.shape == (test_batch_size, perturb_config_test['num_perturbations'], test_seq_length, test_input_dim)\n",
        "\n",
        "        # Check if padded regions remained unchanged (for perturbations that are not masking)\n",
        "        # For Gaussian noise, if applied before masking, the padded part might be affected then restored.\n",
        "        # The current logic restores padded regions AFTER each perturbation type.\n",
        "        # Let's check the first sample, M=0, last 10 elements. Padded ones should be original (1.0)\n",
        "        print(f\"Perturbed X_batch[0, 0, -10:] after all perturbations:\\n{X_perturbed_batch[0, 0, -10:].squeeze().cpu().numpy()}\")\n",
        "\n",
        "        if dummy_padding_mask_pert is not None and test_seq_length > 5:\n",
        "            original_padded_values = dummy_X_batch[0, -5:].squeeze()\n",
        "            perturbed_padded_values = X_perturbed_batch[0, 0, -5:].squeeze() # Check first perturbation instance\n",
        "\n",
        "            # Due to the sequential nature and masking being last, this check needs care.\n",
        "            # The core idea is that the *final* output should respect padding for non-masking perturbations.\n",
        "            # For masking itself, it can zero out padded regions if the chunk falls there.\n",
        "            # The current restoration logic `original_values * (1 - non_padding_mask_expanded)`\n",
        "            # means that for a perturbation like gaussian noise, if it altered a padded region,\n",
        "            # that alteration would be reverted. If input_masking zeros a padded region, it stays zero.\n",
        "\n",
        "            print(f\"Original padded values (sample 0, last 5): {original_padded_values.cpu().numpy()}\")\n",
        "            print(f\"Perturbed padded values (sample 0, pert 0, last 5): {perturbed_padded_values.cpu().numpy()}\")\n",
        "            # This assertion is tricky because input_masking *can* affect padded regions if it randomly targets them.\n",
        "            # A better check would be for gaussian_noise or feature_dropout if applied *last*.\n",
        "            # For now, visual inspection is more practical for the combined effect.\n",
        "\n",
        "        print(\"Perturbation Engine tested (check output shapes and example values).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Perturbation Engine testing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\nCell 6: Perturbation Engine executed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbrp6h0lmXwt"
      },
      "source": [
        "# Cell 7: Geometric Sensitivity Calculation Module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUy0DOUVlvcy"
      },
      "outputs": [],
      "source": [
        "# --- Cell 7: Geometric Sensitivity Calculation Module ---\n",
        "\n",
        "class GeometricSensitivityCalculator:\n",
        "    \"\"\"\n",
        "    Calculates geometric sensitivity of latent representations based on perturbations.\n",
        "    Uses configurations from SENSITIVITY_CONFIG.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "\n",
        "    def _get_active_measures(self) -> List[str]:\n",
        "        \"\"\"Returns the list of sensitivity measures to calculate.\"\"\"\n",
        "        active = self.config.get('active_measures', ['mean_displacement'])\n",
        "        if not isinstance(active, list):\n",
        "            active = [active]\n",
        "        return active\n",
        "\n",
        "    def _compute_displacements(self, z: torch.Tensor, z_perturbed: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes Euclidean distances between original z and each perturbed z_m.\n",
        "        Args:\n",
        "            z: Original latent embeddings [batch_size, latent_dim]\n",
        "            z_perturbed: Perturbed latent embeddings [batch_size, num_perturbations, latent_dim]\n",
        "        Returns:\n",
        "            distances: Euclidean distances [batch_size, num_perturbations]\n",
        "        \"\"\"\n",
        "        # Expand z to match z_perturbed shape for broadcasting: [B, L] -> [B, 1, L]\n",
        "        z_expanded = z.unsqueeze(1) # [batch_size, 1, latent_dim]\n",
        "\n",
        "        # Calculate squared Euclidean distances\n",
        "        # (z_perturbed - z_expanded)**2 has shape [B, M, L]\n",
        "        # Sum over latent_dim (dim=2)\n",
        "        distances_sq = torch.sum((z_perturbed - z_expanded)**2, dim=2) # [batch_size, num_perturbations]\n",
        "\n",
        "        # Clamp for numerical stability before sqrt\n",
        "        distances = torch.sqrt(torch.clamp(distances_sq, min=EPSILON))\n",
        "        return distances\n",
        "\n",
        "    def _calculate_max_displacement(self, distances: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Max Euclidean displacement over M perturbations.\"\"\"\n",
        "        # distances shape: [batch_size, num_perturbations]\n",
        "        max_dist, _ = torch.max(distances, dim=1) # [batch_size]\n",
        "        return max_dist\n",
        "\n",
        "    def _calculate_mean_displacement(self, distances: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Mean Euclidean displacement over M perturbations.\"\"\"\n",
        "        # distances shape: [batch_size, num_perturbations]\n",
        "        mean_dist = torch.mean(distances, dim=1) # [batch_size]\n",
        "        return mean_dist\n",
        "\n",
        "    def _calculate_variance_of_displacements(self, distances: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Variance of Euclidean displacements over M perturbations.\"\"\"\n",
        "        # distances shape: [batch_size, num_perturbations]\n",
        "        if distances.size(1) < 2: # Variance requires at least 2 samples\n",
        "            return torch.zeros_like(distances[:, 0]) # Return zero variance for M < 2\n",
        "        var_dist = torch.var(distances, dim=1, unbiased=True) # [batch_size]\n",
        "        return var_dist\n",
        "\n",
        "    def _calculate_log_covariance_volume(self, z: torch.Tensor, z_perturbed: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the log determinant of the covariance matrix of perturbed points,\n",
        "        centered around the original z.\n",
        "        Args:\n",
        "            z: Original latent embeddings [batch_size, latent_dim]\n",
        "            z_perturbed: Perturbed latent embeddings [batch_size, num_perturbations, latent_dim]\n",
        "        Returns:\n",
        "            log_det: Log determinant of covariance [batch_size]\n",
        "        \"\"\"\n",
        "        batch_size, M, latent_dim = z_perturbed.shape\n",
        "\n",
        "        if M < 2:\n",
        "            #print(\"Warning: Covariance volume requires at least M>=2 perturbations. Returning large negative value for sensitivity.\")\n",
        "            return torch.full((batch_size,), -100.0, device=z.device, dtype=z.dtype)\n",
        "        if M <= latent_dim:\n",
        "            # This warning is very common if latent_dim is high and M is relatively small.\n",
        "            # print(f\"Warning: Covariance matrix might be rank-deficient (M={M} <= latent_dim={latent_dim}). Log-determinant might be -inf or unstable.\")\n",
        "            pass\n",
        "\n",
        "        # Center perturbed points around the original z\n",
        "        centered_perturbations = z_perturbed - z.unsqueeze(1) # Shape: [B, M, L]\n",
        "\n",
        "        # Calculate covariance matrix for each sample in the batch\n",
        "        # Cov = (1/(M-1)) * X_centered^T * X_centered\n",
        "        # X_centered^T: [B, L, M]\n",
        "        # X_centered:   [B, M, L]\n",
        "        # Cov:          [B, L, L]\n",
        "        # Need to ensure M-1 is not zero.\n",
        "        cov_factor = M - 1 if M > 1 else 1 # Avoid division by zero if M=1 (though M<2 handled above)\n",
        "\n",
        "        # For batch matrix multiplication: (B, n, m) @ (B, m, p) -> (B, n, p)\n",
        "        cov_matrices = torch.bmm(centered_perturbations.transpose(1, 2), centered_perturbations) / cov_factor\n",
        "\n",
        "        # Add small identity matrix for numerical stability (regularization) before logdet\n",
        "        identity = torch.eye(latent_dim, device=z.device, dtype=z.dtype).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        cov_stable = cov_matrices + identity * EPSILON # EPSILON is small (e.g., 1e-8)\n",
        "\n",
        "        # Calculate signed log determinant\n",
        "        _sign, log_determinant = torch.linalg.slogdet(cov_stable)\n",
        "\n",
        "        # Handle potential -inf or NaN values from logdet if matrix is singular despite regularization\n",
        "        log_determinant = torch.nan_to_num(log_determinant, nan=-100.0, posinf=0.0, neginf=-100.0)\n",
        "\n",
        "        return log_determinant\n",
        "\n",
        "    def calculate_sensitivities(self, z: torch.Tensor, z_perturbed: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Calculates all active sensitivity measures.\n",
        "        Args:\n",
        "            z: Original latent embeddings [batch_size, latent_dim]\n",
        "            z_perturbed: Perturbed latent embeddings [batch_size, num_perturbations, latent_dim]\n",
        "        Returns:\n",
        "            A dictionary of sensitivity scores, e.g., {'max_displacement': tensor, 'mean_displacement': tensor}\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        active_measures = self._get_active_measures()\n",
        "\n",
        "        # Pre-compute displacements if any displacement-based measure is active\n",
        "        distances = None\n",
        "        if any(m in ['max_displacement', 'mean_displacement', 'variance_of_displacements'] for m in active_measures):\n",
        "            distances = self._compute_displacements(z, z_perturbed)\n",
        "\n",
        "        for measure_type in active_measures:\n",
        "            if measure_type == 'max_displacement':\n",
        "                results[measure_type] = self._calculate_max_displacement(distances)\n",
        "            elif measure_type == 'mean_displacement':\n",
        "                results[measure_type] = self._calculate_mean_displacement(distances)\n",
        "            elif measure_type == 'variance_of_displacements':\n",
        "                results[measure_type] = self._calculate_variance_of_displacements(distances)\n",
        "            elif measure_type == 'log_covariance_volume':\n",
        "                results[measure_type] = self._calculate_log_covariance_volume(z, z_perturbed)\n",
        "            else:\n",
        "                print(f\"Warning: Unknown sensitivity measure type '{measure_type}'. Skipping.\")\n",
        "        return results\n",
        "\n",
        "    def aggregate_sensitivities(self, sensitivity_scores_dict: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Aggregates multiple sensitivity scores into a single score per sample.\n",
        "        \"\"\"\n",
        "        if not sensitivity_scores_dict:\n",
        "            raise ValueError(\"sensitivity_scores_dict is empty. Cannot aggregate.\")\n",
        "\n",
        "        scores_to_aggregate = list(sensitivity_scores_dict.values())\n",
        "\n",
        "        # Stack scores: if each is [B], stacking makes it [Num_Measures, B]\n",
        "        if not scores_to_aggregate: # Should not happen if dict is not empty, but as a safeguard\n",
        "             return torch.tensor(0.0, device=DEVICE) # Or handle appropriately\n",
        "\n",
        "        stacked_scores = torch.stack(scores_to_aggregate, dim=0) # [Num_Measures, Batch_Size]\n",
        "\n",
        "        agg_method = self.config.get('aggregation_method_for_multiple_sensitivities', 'mean')\n",
        "\n",
        "        if agg_method == 'mean':\n",
        "            final_score = torch.mean(stacked_scores, dim=0)\n",
        "        elif agg_method == 'max':\n",
        "            final_score, _ = torch.max(stacked_scores, dim=0)\n",
        "        # elif agg_method == 'weighted_sum':\n",
        "        #     weights = self.config.get('sensitivity_aggregation_weights')\n",
        "        #     if weights is None or len(weights) != len(scores_to_aggregate):\n",
        "        else:\n",
        "            print(f\"Warning: Unknown sensitivity aggregation method '{agg_method}'. Using mean.\")\n",
        "            final_score = torch.mean(stacked_scores, dim=0)\n",
        "\n",
        "        return final_score # [Batch_Size]\n",
        "\n",
        "# --- Example Usage (can be commented out after testing) ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Testing Geometric Sensitivity Calculator ---\")\n",
        "    # Use configs from Cell 2\n",
        "    test_batch_size = 4\n",
        "    test_latent_dim = CGD_MODEL_CONFIG['latent_dim'] # 64\n",
        "    test_num_perturbations = PERTURBATION_CONFIG['num_perturbations'] # 10\n",
        "\n",
        "    dummy_z = torch.randn(test_batch_size, test_latent_dim).to(DEVICE)\n",
        "    dummy_z_perturbed = torch.randn(test_batch_size, test_num_perturbations, test_latent_dim).to(DEVICE)\n",
        "    # Add some structure: make one perturbation far for one sample\n",
        "    dummy_z_perturbed[0, 0, :] += 5.0\n",
        "    # Make another sample have very little spread for its perturbations\n",
        "    dummy_z_perturbed[1, :, :] = dummy_z[1].unsqueeze(0) + torch.randn(test_num_perturbations, test_latent_dim).to(DEVICE) * 0.01\n",
        "\n",
        "\n",
        "    sensitivity_config_test = {\n",
        "        'active_measures': ['max_displacement', 'mean_displacement', 'variance_of_displacements', 'log_covariance_volume'],\n",
        "        'aggregation_method_for_multiple_sensitivities': 'mean',\n",
        "    }\n",
        "    calculator = GeometricSensitivityCalculator(config=sensitivity_config_test)\n",
        "\n",
        "    try:\n",
        "        all_scores_dict = calculator.calculate_sensitivities(dummy_z, dummy_z_perturbed)\n",
        "        print(\"\\nCalculated raw sensitivity scores (dict):\")\n",
        "        for k, v in all_scores_dict.items():\n",
        "            print(f\"  {k}: shape={v.shape}, example_values={v.detach().cpu().numpy()[:2]}\")\n",
        "            assert v.shape == (test_batch_size,)\n",
        "\n",
        "        final_sensitivity_score = calculator.aggregate_sensitivities(all_scores_dict)\n",
        "        print(f\"\\nFinal aggregated sensitivity score (method='{sensitivity_config_test['aggregation_method_for_multiple_sensitivities']}'):\")\n",
        "        print(f\"  Shape: {final_sensitivity_score.shape}, example_values={final_sensitivity_score.detach().cpu().numpy()[:2]}\")\n",
        "        assert final_sensitivity_score.shape == (test_batch_size,)\n",
        "\n",
        "        # Test with fewer perturbations for log_covariance_volume warnings\n",
        "        print(\"\\nTesting log_covariance_volume with M < L_dim:\")\n",
        "        few_pert_config = {'num_perturbations': min(5, test_latent_dim -1) if test_latent_dim > 1 else 1}\n",
        "        if few_pert_config['num_perturbations'] > 0:\n",
        "             dummy_z_perturbed_few = torch.randn(test_batch_size, few_pert_config['num_perturbations'], test_latent_dim).to(DEVICE)\n",
        "             log_cov_vol_few = calculator._calculate_log_covariance_volume(dummy_z, dummy_z_perturbed_few)\n",
        "             print(f\"  log_cov_vol with M={few_pert_config['num_perturbations']}: {log_cov_vol_few.detach().cpu().numpy()[:2]}\")\n",
        "        else:\n",
        "             print(\"  Skipping M < L_dim test as latent_dim is too small or num_perturbations is 0.\")\n",
        "\n",
        "\n",
        "        print(\"\\nGeometric Sensitivity Calculator tested successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Geometric Sensitivity Calculator testing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\nCell 7: Geometric Sensitivity Calculation Module executed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsvUY8Q1mwCV"
      },
      "source": [
        "# Cell 8: Structural Regularizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMzRrUwzmxHK"
      },
      "outputs": [],
      "source": [
        "# --- Cell 8: Structural Regularizer ---\n",
        "\n",
        "class StructuralRegularizer(nn.Module):\n",
        "    \"\"\"\n",
        "    Module for applying structural regularization in the latent space.\n",
        "    Currently supports contrastive loss.\n",
        "    Uses configurations from STRUCTURAL_REGULARIZER_CONFIG.\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim: int, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.config = config\n",
        "        self.regularization_type = config.get('regularization_type', 'contrastive').lower()\n",
        "        self.temperature = config.get('temperature', 0.1)\n",
        "\n",
        "        if self.regularization_type == 'contrastive':\n",
        "            # Projection head for contrastive loss, as is common practice (e.g., SimCLR)\n",
        "            # It maps representations to the space where contrastive loss is applied.\n",
        "            # Typically, a 2-layer MLP. Output dim can be smaller than latent_dim.\n",
        "            projection_hidden_dim = config.get('projection_hidden_dim', latent_dim) # Can be same as latent_dim\n",
        "            projection_output_dim = config.get('projection_output_dim', max(latent_dim // 2, 16)) # Example: half of latent_dim\n",
        "\n",
        "            self.projection_head = nn.Sequential(\n",
        "                nn.Linear(latent_dim, projection_hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(projection_hidden_dim, projection_output_dim)\n",
        "            )\n",
        "        elif self.regularization_type == 'prototype':\n",
        "            # Placeholder for prototype loss components if you decide to implement it\n",
        "            self.num_prototypes = config.get('num_prototypes', 10)\n",
        "            self.prototype_lambda = config.get('prototype_lambda', 0.1) # For prototype diversity\n",
        "            self.prototypes = nn.Parameter(torch.randn(self.num_prototypes, latent_dim))\n",
        "            nn.init.xavier_uniform_(self.prototypes)\n",
        "            print(\"Prototype regularizer initialized (implementation is basic).\")\n",
        "        elif self.regularization_type != 'none':\n",
        "            raise ValueError(f\"Unsupported regularization_type: {self.regularization_type}\")\n",
        "\n",
        "    def _compute_cosine_similarity(self, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Computes cosine similarity matrix between two sets of embeddings.\"\"\"\n",
        "        # z1: [N, D], z2: [M, D] -> Output: [N, M]\n",
        "        z1_norm = F.normalize(z1, p=2, dim=1)\n",
        "        z2_norm = F.normalize(z2, p=2, dim=1)\n",
        "        return torch.mm(z1_norm, z2_norm.t())\n",
        "\n",
        "    def _info_nce_loss(self, z_anchor: torch.Tensor, z_positive: torch.Tensor, temperature: float) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes InfoNCE loss.\n",
        "        z_anchor: [batch_size, proj_dim] - e.g., projections of original samples\n",
        "        z_positive: [batch_size, proj_dim] - e.g., projections of augmented/perturbed samples\n",
        "        \"\"\"\n",
        "        batch_size = z_anchor.size(0)\n",
        "        if batch_size == 0:\n",
        "            return torch.tensor(0.0, device=z_anchor.device)\n",
        "\n",
        "        # Concatenate all representations: [2*batch_size, proj_dim]\n",
        "        representations = torch.cat([z_anchor, z_positive], dim=0)\n",
        "\n",
        "        # Calculate similarity matrix: [2*batch_size, 2*batch_size]\n",
        "        similarity_matrix = self._compute_cosine_similarity(representations, representations) / temperature\n",
        "\n",
        "        # Create labels for positives:\n",
        "        # For each z_anchor[i], its positive is z_positive[i] (which is at index i + batch_size in 'representations')\n",
        "        # For each z_positive[i], its positive is z_anchor[i] (which is at index i in 'representations')\n",
        "        labels = torch.cat([\n",
        "            torch.arange(batch_size) + batch_size, # Positives for z_anchor\n",
        "            torch.arange(batch_size)               # Positives for z_positive\n",
        "        ]).to(z_anchor.device)\n",
        "\n",
        "        # Mask out self-similarity (diagonal elements) from logits before CrossEntropy\n",
        "        # Create an identity matrix, set diagonal to a very small number (-inf effectively after division by T)\n",
        "        # This ensures that a sample is not contrasted with itself.\n",
        "        mask_diag = ~torch.eye(2 * batch_size, dtype=torch.bool, device=similarity_matrix.device)\n",
        "        similarity_matrix_masked = similarity_matrix.masked_fill(~mask_diag, float('-inf'))\n",
        "\n",
        "        loss = F.cross_entropy(similarity_matrix_masked, labels)\n",
        "        return loss\n",
        "\n",
        "    def contrastive_loss(self, z: torch.Tensor, z_perturbed_single: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculates contrastive loss (SimCLR-style).\n",
        "        Args:\n",
        "            z: Original latent embeddings [batch_size, latent_dim].\n",
        "            z_perturbed_single: A single representative perturbed embedding per original sample.\n",
        "                                e.g., mean of M perturbations [batch_size, latent_dim].\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'projection_head'):\n",
        "            # This should not happen if regularization_type is 'contrastive' due to __init__\n",
        "            warnings.warn(\"Projection head not found for contrastive loss. Using raw embeddings.\", UserWarning)\n",
        "            z_proj = z\n",
        "            z_pert_proj = z_perturbed_single\n",
        "        else:\n",
        "            z_proj = self.projection_head(z)\n",
        "            z_pert_proj = self.projection_head(z_perturbed_single)\n",
        "\n",
        "        return self._info_nce_loss(z_proj, z_pert_proj, self.temperature)\n",
        "\n",
        "    def forward(self, z: torch.Tensor, z_perturbed: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the structural regularization loss.\n",
        "        Args:\n",
        "            z: Original latent embeddings [batch_size, latent_dim]\n",
        "            z_perturbed: All M perturbed latent embeddings [batch_size, num_perturbations, latent_dim]\n",
        "                         OR a single representative perturbed embedding [batch_size, latent_dim].\n",
        "        \"\"\"\n",
        "        if self.regularization_type == 'contrastive':\n",
        "            if z_perturbed is None:\n",
        "                #print(\"Warning: Contrastive loss requires perturbed representations (z_perturbed). Returning 0 loss.\")\n",
        "                return torch.tensor(0.0, device=z.device)\n",
        "\n",
        "            if z_perturbed.ndim == 3: # [B, M, L] -> Take mean over M perturbations\n",
        "                # It's common to use the mean of perturbations as the \"positive\" augmentation view\n",
        "                z_perturbed_representative = torch.mean(z_perturbed, dim=1) # -> [B, L]\n",
        "            elif z_perturbed.ndim == 2: # Already [B, L]\n",
        "                z_perturbed_representative = z_perturbed\n",
        "            else:\n",
        "                raise ValueError(f\"z_perturbed has unexpected ndim: {z_perturbed.ndim}\")\n",
        "\n",
        "            return self.contrastive_loss(z, z_perturbed_representative)\n",
        "\n",
        "        elif self.regularization_type == 'prototype':\n",
        "            # Basic prototype loss: encourage z to be close to one prototype,\n",
        "            # and prototypes to be diverse. This is a simplified version.\n",
        "            # This part is not fully developed/tested like contrastive.\n",
        "            batch_size = z.shape[0]\n",
        "            z_expanded = z.unsqueeze(1) # [B, 1, L]\n",
        "            prototypes_expanded = self.prototypes.unsqueeze(0) # [1, P, L]\n",
        "\n",
        "            distances_sq = torch.sum((z_expanded - prototypes_expanded)**2, dim=2) # [B, P]\n",
        "            min_distances_sq, _ = torch.min(distances_sq, dim=1) # [B], distance to closest prototype\n",
        "            clustering_loss = min_distances_sq.mean()\n",
        "\n",
        "            # Prototype diversity (e.g., encourage orthogonality or distance)\n",
        "            proto_norm = F.normalize(self.prototypes, dim=1)\n",
        "            proto_sim_matrix = torch.mm(proto_norm, proto_norm.t()) # [P, P]\n",
        "            # Penalize high similarity between different prototypes\n",
        "            diversity_loss = (proto_sim_matrix[~torch.eye(self.num_prototypes, dtype=bool, device=z.device)]**2).mean()\n",
        "\n",
        "            return clustering_loss + self.prototype_lambda * diversity_loss\n",
        "\n",
        "        elif self.regularization_type == 'none':\n",
        "            return torch.tensor(0.0, device=z.device)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported regularization_type: {self.regularization_type}\")\n",
        "\n",
        "# --- Example Usage (can be commented out after testing) ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Testing Structural Regularizer ---\")\n",
        "    # Use configs from Cell 2\n",
        "    test_batch_size = 16 # Contrastive loss works better with larger batches\n",
        "    test_latent_dim = CGD_MODEL_CONFIG['latent_dim'] # 64\n",
        "    test_num_perturbations = PERTURBATION_CONFIG['num_perturbations'] # 10\n",
        "\n",
        "    dummy_z_orig = torch.randn(test_batch_size, test_latent_dim).to(DEVICE)\n",
        "    # For contrastive, we usually pass the mean of perturbations, or one selected view\n",
        "    dummy_z_pert_all = torch.randn(test_batch_size, test_num_perturbations, test_latent_dim).to(DEVICE)\n",
        "    dummy_z_pert_mean = torch.mean(dummy_z_pert_all, dim=1)\n",
        "\n",
        "    reg_config_test_contrastive = STRUCTURAL_REGULARIZER_CONFIG.copy()\n",
        "    reg_config_test_contrastive['regularization_type'] = 'contrastive'\n",
        "\n",
        "    reg_config_test_proto = STRUCTURAL_REGULARIZER_CONFIG.copy()\n",
        "    reg_config_test_proto['regularization_type'] = 'prototype'\n",
        "    reg_config_test_proto['num_prototypes'] = 5\n",
        "    reg_config_test_proto['prototype_lambda'] = 0.05\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(\"\\nTesting Contrastive Regularizer:\")\n",
        "        regularizer_contrastive = StructuralRegularizer(\n",
        "            latent_dim=test_latent_dim,\n",
        "            config=reg_config_test_contrastive\n",
        "        ).to(DEVICE)\n",
        "        regularizer_contrastive.train() # Projection head has dropout/batchnorm if any\n",
        "\n",
        "        # Test with mean of perturbations\n",
        "        loss_contrastive_mean = regularizer_contrastive(dummy_z_orig, dummy_z_pert_mean)\n",
        "        print(f\"  Contrastive loss (with mean z_perturbed): {loss_contrastive_mean.item():.4f}\")\n",
        "        assert loss_contrastive_mean >= 0\n",
        "\n",
        "        # Test with all M perturbations (internally takes mean)\n",
        "        loss_contrastive_all_m = regularizer_contrastive(dummy_z_orig, dummy_z_pert_all)\n",
        "        print(f\"  Contrastive loss (with all M z_perturbed): {loss_contrastive_all_m.item():.4f}\")\n",
        "        assert loss_contrastive_all_m >= 0\n",
        "\n",
        "        # Test 'none' type\n",
        "        reg_config_none = {'regularization_type': 'none'}\n",
        "        regularizer_none = StructuralRegularizer(latent_dim=test_latent_dim, config=reg_config_none).to(DEVICE)\n",
        "        loss_none = regularizer_none(dummy_z_orig, dummy_z_pert_mean)\n",
        "        print(f\"  'None' regularizer loss: {loss_none.item():.4f}\")\n",
        "        assert loss_none.item() == 0.0\n",
        "\n",
        "        print(\"\\nTesting Prototype Regularizer (basic implementation):\")\n",
        "        regularizer_proto = StructuralRegularizer(\n",
        "            latent_dim=test_latent_dim,\n",
        "            config=reg_config_test_proto\n",
        "        ).to(DEVICE)\n",
        "        regularizer_proto.train()\n",
        "        loss_proto = regularizer_proto(dummy_z_orig) # Doesn't use z_perturbed in this simple version\n",
        "        print(f\"  Prototype loss: {loss_proto.item():.4f}\")\n",
        "        assert loss_proto >= 0\n",
        "\n",
        "\n",
        "        print(\"\\nStructural Regularizer tested successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Structural Regularizer testing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\nCell 8: Structural Regularizer executed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7nlBUaPnRfs"
      },
      "outputs": [],
      "source": [
        "# --- Cell 9: Main CGD Model Architecture (UniversalCGDModel) ---\n",
        "\n",
        "class UniversalCGDModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Universal Causal Geometric Deferral (CGD) framework.\n",
        "    Integrates encoder, predictor, perturbation, sensitivity, and regularization.\n",
        "    Deferral during training is based on a fixed threshold for loss calculation.\n",
        "    Evaluation can use an adaptive threshold.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_config: Dict[str, Any],\n",
        "        encoder_config: Dict[str, Any],\n",
        "        predictor_config: Dict[str, Any],\n",
        "        perturb_config: Dict[str, Any],\n",
        "        sensitivity_config: Dict[str, Any],\n",
        "        regularizer_config: Dict[str, Any],\n",
        "        # output_dim must be passed explicitly based on dataset\n",
        "        output_dim: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model_config = model_config\n",
        "        self.output_dim_val = output_dim # Store for predictor and loss logic\n",
        "\n",
        "        # Initialize Components\n",
        "        self.encoder = TimeSeriesTransformerEncoder(\n",
        "            input_dim=model_config['input_dim'],\n",
        "            config=encoder_config,\n",
        "            max_seq_len_data=model_config['max_seq_length']\n",
        "        )\n",
        "\n",
        "        # The encoder outputs `embed_dim` which might be different from the desired `latent_dim`\n",
        "        # for the rest of the CGD components. Add a projection if necessary.\n",
        "        encoder_actual_output_dim = encoder_config.get('embed_dim', model_config['latent_dim'])\n",
        "        self.latent_dim = model_config['latent_dim']\n",
        "\n",
        "        if encoder_actual_output_dim != self.latent_dim:\n",
        "            self.latent_projection = nn.Linear(encoder_actual_output_dim, self.latent_dim)\n",
        "            print(f\"Added latent projection from encoder output {encoder_actual_output_dim} to latent_dim {self.latent_dim}\")\n",
        "        else:\n",
        "            self.latent_projection = nn.Identity()\n",
        "\n",
        "        self.predictor = CGDPredictor(\n",
        "            latent_dim=self.latent_dim,\n",
        "            output_dim=output_dim, # Passed based on dataset\n",
        "            config=predictor_config\n",
        "        )\n",
        "        self.perturbation_generator = PerturbationGenerator(config=perturb_config)\n",
        "        self.sensitivity_calculator = GeometricSensitivityCalculator(config=sensitivity_config)\n",
        "        self.structural_regularizer = StructuralRegularizer(\n",
        "            latent_dim=self.latent_dim, # Regularizer works on the final latent_dim\n",
        "            config=regularizer_config\n",
        "        )\n",
        "\n",
        "        self.deferral_threshold_train = model_config.get('deferral_threshold_train', 0.5)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        X_batch: torch.Tensor, # [B, S, D_in]\n",
        "        padding_mask_batch: Optional[torch.Tensor] = None # [B, S]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through the UniversalCGDModel.\n",
        "        \"\"\"\n",
        "        # 1. Encode original input\n",
        "        z_encoded_original = self.encoder(X_batch, src_key_padding_mask=padding_mask_batch)\n",
        "        z_original = self.latent_projection(z_encoded_original) # -> [B, L]\n",
        "\n",
        "        # 2. Generate & encode perturbations\n",
        "        # X_perturbed_all: [B, M, S, D_in]\n",
        "        X_perturbed_all = self.perturbation_generator.generate(X_batch, padding_mask=padding_mask_batch)\n",
        "\n",
        "        batch_size, M, seq_length, input_dim = X_perturbed_all.shape\n",
        "        X_perturbed_flat = X_perturbed_all.reshape(batch_size * M, seq_length, input_dim) # [B*M, S, D_in]\n",
        "\n",
        "        pert_padding_mask_flat = None\n",
        "        if padding_mask_batch is not None:\n",
        "            pert_padding_mask_flat = padding_mask_batch.repeat_interleave(M, dim=0) # [B*M, S]\n",
        "\n",
        "        z_encoded_perturbed_flat = self.encoder(X_perturbed_flat, src_key_padding_mask=pert_padding_mask_flat)\n",
        "        z_perturbed_flat = self.latent_projection(z_encoded_perturbed_flat) # -> [B*M, L]\n",
        "        z_perturbed_all = z_perturbed_flat.reshape(batch_size, M, self.latent_dim) # -> [B, M, L]\n",
        "\n",
        "        # 3. Calculate one or more raw sensitivity scores\n",
        "        # raw_sensitivity_scores_dict: {'measure1': [B], 'measure2': [B], ...}\n",
        "        raw_sensitivity_scores_dict = self.sensitivity_calculator.calculate_sensitivities(z_original, z_perturbed_all)\n",
        "\n",
        "        # 4. Aggregate raw sensitivity scores if multiple are active\n",
        "        if not raw_sensitivity_scores_dict: # Should not happen if config is valid\n",
        "            final_sensitivity_score = torch.zeros(batch_size, device=X_batch.device)\n",
        "        elif len(raw_sensitivity_scores_dict) == 1:\n",
        "            final_sensitivity_score = next(iter(raw_sensitivity_scores_dict.values()))\n",
        "        else:\n",
        "            final_sensitivity_score = self.sensitivity_calculator.aggregate_sensitivities(raw_sensitivity_scores_dict)\n",
        "\n",
        "        # 5. Make predictions\n",
        "        y_pred_logits = self.predictor(z_original) # [B, Output_Dim]\n",
        "\n",
        "        # 6. Deferral decision *during training* (for loss calculation purposes)\n",
        "        # This uses the fixed training threshold. Evaluation will use an adaptive one.\n",
        "        defer_train_time = final_sensitivity_score > self.deferral_threshold_train # [B] boolean\n",
        "\n",
        "        return {\n",
        "            'z_original': z_original,                       # [B, L]\n",
        "            'z_perturbed_all': z_perturbed_all,             # [B, M, L]\n",
        "            'raw_sensitivity_scores_dict': raw_sensitivity_scores_dict, # Dict of [B]\n",
        "            'final_sensitivity_score': final_sensitivity_score,       # [B]\n",
        "            'y_pred_logits': y_pred_logits,                 # [B, Output_Dim]\n",
        "            'defer_train_time': defer_train_time            # [B] boolean\n",
        "        }\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        model_output: Dict[str, torch.Tensor],\n",
        "        y_true: torch.Tensor, # [B] (long for CE, float for BCE)\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Computes the multi-objective loss for the CGD model.\"\"\"\n",
        "\n",
        "        z_original = model_output['z_original']\n",
        "        z_perturbed_all = model_output['z_perturbed_all']\n",
        "        final_sensitivity_score = model_output['final_sensitivity_score']\n",
        "        y_pred_logits = model_output['y_pred_logits']\n",
        "        defer_train_time = model_output['defer_train_time'] # Based on fixed training threshold\n",
        "\n",
        "        loss_alpha = self.model_config.get('loss_alpha', 0.1)\n",
        "        loss_beta = self.model_config.get('loss_beta', 0.1)\n",
        "        defer_cost_factor = self.model_config.get('defer_cost_factor', 0.3)\n",
        "\n",
        "        batch_size = y_true.size(0)\n",
        "        if batch_size == 0: # Should not happen with valid dataloader\n",
        "            return {k: torch.tensor(0.0, device=DEVICE) for k in ['total_loss', 'pred_loss', 'struct_loss', 'defer_cost_loss', 'sens_reg_loss', 'defer_rate_train_time']}\n",
        "\n",
        "\n",
        "        # --- 1. Prediction Loss (L_pred) ---\n",
        "        # Applied only to non-deferred samples (based on training-time deferral)\n",
        "        non_deferred_mask_train = ~defer_train_time\n",
        "\n",
        "        pred_loss = torch.tensor(0.0, device=y_true.device)\n",
        "        num_non_deferred = non_deferred_mask_train.sum().item()\n",
        "\n",
        "        if num_non_deferred > 0:\n",
        "            if self.output_dim_val == 1: # Binary classification (e.g., PTB)\n",
        "                # BCEWithLogitsLoss expects float targets of shape [B] or [B,1]\n",
        "                pred_loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
        "                current_pred_loss = pred_loss_fn(\n",
        "                    y_pred_logits[non_deferred_mask_train].squeeze(-1), # Logits [N_nd]\n",
        "                    y_true[non_deferred_mask_train].float()            # Targets [N_nd]\n",
        "                )\n",
        "            else: # Multiclass classification (e.g., MIT-BIH)\n",
        "                # CrossEntropyLoss expects long targets of shape [B]\n",
        "                pred_loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "                current_pred_loss = pred_loss_fn(\n",
        "                    y_pred_logits[non_deferred_mask_train], # Logits [N_nd, C]\n",
        "                    y_true[non_deferred_mask_train].long()  # Targets [N_nd]\n",
        "                )\n",
        "            pred_loss = current_pred_loss.mean()\n",
        "\n",
        "        # --- 2. Structural Regularization Loss (L_struct) ---\n",
        "        struct_loss = self.structural_regularizer(z_original, z_perturbed_all)\n",
        "\n",
        "        # --- 3. Deferral Cost Loss (L_defer_cost) ---\n",
        "        # Cost for samples deferred during training\n",
        "        defer_cost_loss = defer_train_time.float().mean() * defer_cost_factor\n",
        "\n",
        "        # --- 4. Sensitivity Regularization Loss (L_sens_reg) ---\n",
        "        with torch.no_grad(): # Detach correct_prediction from graph for this specific loss term\n",
        "            if self.output_dim_val == 1: # Binary\n",
        "                # Ensure y_pred_logits for binary is [B,1], then squeeze for comparison\n",
        "                predicted_classes = (y_pred_logits.squeeze(-1).sigmoid() > 0.5).float()\n",
        "                correct_prediction = (predicted_classes == y_true.float()).float()\n",
        "            else: # Multiclass\n",
        "                predicted_classes = torch.argmax(y_pred_logits, dim=1)\n",
        "                correct_prediction = (predicted_classes == y_true.long()).float()\n",
        "\n",
        "        # Target for sensitivity: +1 for incorrect, -1 for correct\n",
        "        # We want to minimize (sensitivity * target_for_sensitivity)\n",
        "        # So, sensitivity should be high when target_for_sensitivity is -1 (incorrect prediction)\n",
        "        # And sensitivity should be low when target_for_sensitivity is +1 (correct prediction)\n",
        "        # The original formula: mean(sensitivity * (2 * correct_pred.float() - 1))\n",
        "        # if correct_pred = 1 -> sens * 1 (minimize sens)\n",
        "        # if correct_pred = 0 -> sens * -1 (minimize -sens -> maximize sens)\n",
        "        # This aligns sensitivity with being high for errors.\n",
        "        target_for_sensitivity_shaping = (2 * correct_prediction) - 1\n",
        "        sens_reg_loss = (final_sensitivity_score * target_for_sensitivity_shaping).mean()\n",
        "\n",
        "        # --- 5. Total Loss ---\n",
        "        total_loss = pred_loss + \\\n",
        "                     loss_alpha * struct_loss + \\\n",
        "                     loss_beta * (defer_cost_loss + sens_reg_loss)\n",
        "\n",
        "        return {\n",
        "            'total_loss': total_loss,\n",
        "            'pred_loss': pred_loss,\n",
        "            'struct_loss': struct_loss,\n",
        "            'defer_cost_loss': defer_cost_loss,\n",
        "            'sens_reg_loss': sens_reg_loss,\n",
        "            'defer_rate_train_time': defer_train_time.float().mean() # For monitoring\n",
        "        }\n",
        "\n",
        "# --- Example Usage (can be commented out after testing) ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Testing UniversalCGDModel ---\")\n",
        "    # Use configs from Cell 2\n",
        "    # Test for MIT-BIH like (multiclass)\n",
        "    mitbih_output_dim = 5\n",
        "\n",
        "    # Create dummy data batch from MIT-BIH loader if available, else create simple dummy\n",
        "    if 'mitbih_loaders' in locals() and mitbih_loaders is not None and 'train' in mitbih_loaders:\n",
        "        try:\n",
        "            dummy_X_mit, dummy_y_mit, dummy_mask_mit = next(iter(mitbih_loaders['train']))\n",
        "            dummy_X_mit, dummy_y_mit, dummy_mask_mit = dummy_X_mit.to(DEVICE), dummy_y_mit.to(DEVICE), dummy_mask_mit.to(DEVICE)\n",
        "            # Reduce batch size for faster test if loaded batch is large\n",
        "            if dummy_X_mit.size(0) > 4:\n",
        "                 dummy_X_mit = dummy_X_mit[:4]\n",
        "                 dummy_y_mit = dummy_y_mit[:4]\n",
        "                 dummy_mask_mit = dummy_mask_mit[:4]\n",
        "            print(f\"Using a batch from MIT-BIH DataLoader for testing: X_shape={dummy_X_mit.shape}\")\n",
        "        except StopIteration:\n",
        "            print(\"MIT-BIH DataLoader is empty, creating simple dummy data.\")\n",
        "            dummy_X_mit = torch.randn(4, CGD_MODEL_CONFIG['max_seq_length'], CGD_MODEL_CONFIG['input_dim']).to(DEVICE)\n",
        "            dummy_y_mit = torch.randint(0, mitbih_output_dim, (4,)).to(DEVICE)\n",
        "            dummy_mask_mit = torch.zeros(4, CGD_MODEL_CONFIG['max_seq_length'], dtype=torch.bool).to(DEVICE)\n",
        "    else:\n",
        "        print(\"MIT-BIH loader not found, creating simple dummy data for model test.\")\n",
        "        dummy_X_mit = torch.randn(4, CGD_MODEL_CONFIG['max_seq_length'], CGD_MODEL_CONFIG['input_dim']).to(DEVICE)\n",
        "        dummy_y_mit = torch.randint(0, mitbih_output_dim, (4,)).to(DEVICE)\n",
        "        dummy_mask_mit = torch.zeros(4, CGD_MODEL_CONFIG['max_seq_length'], dtype=torch.bool).to(DEVICE)\n",
        "\n",
        "    try:\n",
        "        cgd_model_mitbih = UniversalCGDModel(\n",
        "            model_config=CGD_MODEL_CONFIG,\n",
        "            encoder_config=ENCODER_CONFIG,\n",
        "            predictor_config=PREDICTOR_CONFIG,\n",
        "            perturb_config=PERTURBATION_CONFIG,\n",
        "            sensitivity_config=SENSITIVITY_CONFIG,\n",
        "            regularizer_config=STRUCTURAL_REGULARIZER_CONFIG,\n",
        "            output_dim=mitbih_output_dim\n",
        "        ).to(DEVICE)\n",
        "        cgd_model_mitbih.train() # Set to train for dropout, batchnorm etc.\n",
        "\n",
        "        model_output_mitbih = cgd_model_mitbih(dummy_X_mit, padding_mask_batch=dummy_mask_mit)\n",
        "        print(\"\\nMIT-BIH Model Output Keys:\", list(model_output_mitbih.keys()))\n",
        "        print(f\"  y_pred_logits shape: {model_output_mitbih['y_pred_logits'].shape}\")\n",
        "        assert model_output_mitbih['y_pred_logits'].shape == (dummy_X_mit.size(0), mitbih_output_dim)\n",
        "        print(f\"  final_sensitivity_score shape: {model_output_mitbih['final_sensitivity_score'].shape}\")\n",
        "        assert model_output_mitbih['final_sensitivity_score'].shape == (dummy_X_mit.size(0),)\n",
        "        print(f\"  defer_train_time shape: {model_output_mitbih['defer_train_time'].shape}, example: {model_output_mitbih['defer_train_time'].cpu().numpy()}\")\n",
        "        assert model_output_mitbih['defer_train_time'].shape == (dummy_X_mit.size(0),)\n",
        "\n",
        "\n",
        "        loss_dict_mitbih = cgd_model_mitbih.compute_loss(model_output_mitbih, dummy_y_mit)\n",
        "        print(\"\\nMIT-BIH Model Loss Dict Keys:\", list(loss_dict_mitbih.keys()))\n",
        "        for k, v in loss_dict_mitbih.items():\n",
        "            print(f\"  {k}: {v.item():.4f}\")\n",
        "        assert 'total_loss' in loss_dict_mitbih\n",
        "\n",
        "        print(\"\\nUniversalCGDModel tested successfully for MIT-BIH case.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during UniversalCGDModel testing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\nCell 9: Main CGD Model Architecture (UniversalCGDModel) executed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d-9674vn5MY"
      },
      "source": [
        "# Cell 10: Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2oC1Qz3nuhX"
      },
      "outputs": [],
      "source": [
        "# --- Cell 10: Training Loop ---\n",
        "\n",
        "def train_epoch(\n",
        "    model: UniversalCGDModel,\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epoch_num: int, # For logging\n",
        "    total_epochs: int # For logging\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Trains the model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_samples = 0\n",
        "    running_losses = {\n",
        "        'total_loss': 0.0, 'pred_loss': 0.0, 'struct_loss': 0.0,\n",
        "        'defer_cost_loss': 0.0, 'sens_reg_loss': 0.0, 'defer_rate_train_time': 0.0\n",
        "    }\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch_num+1}/{total_epochs} [Train]\")\n",
        "    for X_batch, y_batch, padding_mask_batch in pbar:\n",
        "        X_batch, y_batch, padding_mask_batch = X_batch.to(device), y_batch.to(device), padding_mask_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model_output = model(X_batch, padding_mask_batch=padding_mask_batch)\n",
        "        loss_dict = model.compute_loss(model_output, y_batch)\n",
        "\n",
        "        total_loss = loss_dict['total_loss']\n",
        "\n",
        "        if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
        "            print(f\"Warning: NaN or Inf loss detected in training epoch {epoch_num+1}. Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        total_loss.backward()\n",
        "        # Optional: Gradient clipping can help with stability for complex models/losses\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = X_batch.size(0)\n",
        "        total_samples += batch_size\n",
        "        for key in running_losses.keys():\n",
        "            running_losses[key] += loss_dict[key].item() * batch_size\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f\"{running_losses['total_loss']/total_samples:.4f}\",\n",
        "            'defer%': f\"{running_losses['defer_rate_train_time']/total_samples*100:.1f}%\"\n",
        "        })\n",
        "\n",
        "    epoch_avg_losses = {key: val / total_samples if total_samples > 0 else 0.0 for key, val in running_losses.items()}\n",
        "    return epoch_avg_losses\n",
        "\n",
        "def validate_epoch(\n",
        "    model: UniversalCGDModel,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    epoch_num: int, # For logging\n",
        "    total_epochs: int # For logging\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Validates the model for one epoch.\"\"\"\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "    running_losses = {\n",
        "        'total_loss': 0.0, 'pred_loss': 0.0, 'struct_loss': 0.0,\n",
        "        'defer_cost_loss': 0.0, 'sens_reg_loss': 0.0, 'defer_rate_train_time': 0.0\n",
        "    }\n",
        "    all_y_true = []\n",
        "    all_y_pred_logits = []\n",
        "    all_defer_train_time = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch_num+1}/{total_epochs} [Val]\")\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, padding_mask_batch in pbar:\n",
        "            X_batch, y_batch, padding_mask_batch = X_batch.to(device), y_batch.to(device), padding_mask_batch.to(device)\n",
        "\n",
        "            model_output = model(X_batch, padding_mask_batch=padding_mask_batch)\n",
        "            loss_dict = model.compute_loss(model_output, y_batch)\n",
        "\n",
        "            batch_size = X_batch.size(0)\n",
        "            total_samples += batch_size\n",
        "            for key in running_losses.keys():\n",
        "                 if key in loss_dict and loss_dict[key] is not None : # Ensure key exists\n",
        "                    running_losses[key] += loss_dict[key].item() * batch_size\n",
        "\n",
        "            all_y_true.append(y_batch.cpu())\n",
        "            all_y_pred_logits.append(model_output['y_pred_logits'].cpu())\n",
        "            all_defer_train_time.append(model_output['defer_train_time'].cpu())\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'val_loss': f\"{running_losses['total_loss']/total_samples:.4f}\",\n",
        "                'val_defer%': f\"{running_losses['defer_rate_train_time']/total_samples*100:.1f}%\"\n",
        "            })\n",
        "\n",
        "    epoch_avg_losses = {key: val / total_samples if total_samples > 0 else 0.0 for key, val in running_losses.items()}\n",
        "\n",
        "    # Calculate accuracy on non-deferred samples (using training-time deferral for this val metric)\n",
        "    y_true_cat = torch.cat(all_y_true)\n",
        "    y_pred_logits_cat = torch.cat(all_y_pred_logits)\n",
        "    defer_train_time_cat = torch.cat(all_defer_train_time)\n",
        "\n",
        "    non_deferred_mask = ~defer_train_time_cat\n",
        "    acc_nd_val = 0.0\n",
        "    if non_deferred_mask.sum().item() > 0:\n",
        "        y_true_nd = y_true_cat[non_deferred_mask]\n",
        "        y_pred_logits_nd = y_pred_logits_cat[non_deferred_mask]\n",
        "\n",
        "        if model.output_dim_val == 1: # Binary\n",
        "            preds_nd = (y_pred_logits_nd.sigmoid() > 0.5).float().squeeze()\n",
        "            acc_nd_val = accuracy_score(y_true_nd.float().numpy(), preds_nd.numpy())\n",
        "        else: # Multiclass\n",
        "            preds_nd = torch.argmax(y_pred_logits_nd, dim=1)\n",
        "            acc_nd_val = accuracy_score(y_true_nd.long().numpy(), preds_nd.numpy())\n",
        "\n",
        "    epoch_avg_losses['accuracy_nd_val_train_thresh'] = acc_nd_val # Accuracy on non-deferred (using training threshold)\n",
        "\n",
        "    return epoch_avg_losses\n",
        "\n",
        "\n",
        "def train_universal_cgd_model(\n",
        "    model: UniversalCGDModel,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    training_config: Dict[str, Any], # From GENERAL_TRAINING_CONFIG\n",
        "    model_specific_checkpoint_name: str = \"universal_cgd_best.pt\"\n",
        ") -> Tuple[UniversalCGDModel, Dict[str, List[float]]]:\n",
        "    \"\"\"\n",
        "    Main training loop for the UniversalCGDModel.\n",
        "    Includes early stopping and saving the best model.\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device # Get device from model\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=training_config['learning_rate'])\n",
        "    # Optional: Learning rate scheduler\n",
        "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=training_config['patience'] // 2, factor=0.5, verbose=True)\n",
        "\n",
        "    epochs = training_config['epochs']\n",
        "    patience = training_config['patience']\n",
        "    checkpoint_dir = training_config['checkpoint_dir']\n",
        "    best_model_path = os.path.join(checkpoint_dir, model_specific_checkpoint_name)\n",
        "\n",
        "    history_keys = [\n",
        "        'train_total_loss', 'train_pred_loss', 'train_struct_loss',\n",
        "        'train_defer_cost_loss', 'train_sens_reg_loss', 'train_defer_rate_train_time',\n",
        "        'val_total_loss', 'val_pred_loss', 'val_struct_loss',\n",
        "        'val_defer_cost_loss', 'val_sens_reg_loss', 'val_defer_rate_train_time',\n",
        "        'val_accuracy_nd_val_train_thresh' # Accuracy on non-deferred samples in validation (using training defer threshold)\n",
        "    ]\n",
        "    history = {k: [] for k in history_keys}\n",
        "\n",
        "    best_val_total_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    print(f\"Starting training for {epochs} epochs on {device}...\")\n",
        "    print(f\"Best model will be saved to: {best_model_path}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_metrics = train_epoch(model, train_loader, optimizer, device, epoch, epochs)\n",
        "        val_metrics = validate_epoch(model, val_loader, device, epoch, epochs)\n",
        "\n",
        "        # Log metrics to history\n",
        "        for key, value in train_metrics.items(): history[f'train_{key}'].append(value)\n",
        "        for key, value in val_metrics.items(): history[f'val_{key}'].append(value)\n",
        "\n",
        "        current_val_total_loss = val_metrics['total_loss']\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} Summary:\")\n",
        "        print(f\"  Train: Loss={train_metrics['total_loss']:.4f}, Defer%={train_metrics['defer_rate_train_time']*100:.1f}%\")\n",
        "        print(f\"  Val:   Loss={val_metrics['total_loss']:.4f}, Defer%={val_metrics['defer_rate_train_time']*100:.1f}%, Acc_ND_Val(TrainThresh)={val_metrics['accuracy_nd_val_train_thresh']:.4f}\")\n",
        "\n",
        "        # if scheduler: scheduler.step(current_val_total_loss)\n",
        "\n",
        "        if current_val_total_loss < best_val_total_loss:\n",
        "            best_val_total_loss = current_val_total_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"  -> New best validation loss: {best_val_total_loss:.4f}. Checkpoint saved.\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  -> Validation loss did not improve for {epochs_no_improve} epoch(s). Best: {best_val_total_loss:.4f}\")\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    if os.path.exists(best_model_path):\n",
        "        print(f\"Loading best model state from {best_model_path}\")\n",
        "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    else:\n",
        "        print(\"Warning: No best model checkpoint found. Using model from last epoch.\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAhBH3xloIkL"
      },
      "source": [
        "# Cell 11: Adaptive Deferral Threshold Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVJepB20oNWt"
      },
      "outputs": [],
      "source": [
        "# --- Cell 11: Adaptive Deferral Threshold Utilities ---\n",
        "\n",
        "def compute_adaptive_deferral_threshold(\n",
        "    model: UniversalCGDModel,\n",
        "    val_loader: DataLoader,\n",
        "    adaptive_thresh_config: Dict[str, Any], # From ADAPTIVE_THRESHOLD_CONFIG\n",
        "    device: torch.device\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes an adaptive deferral threshold based on validation set sensitivities.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_sensitivities = []\n",
        "    all_y_true_val = []\n",
        "    all_y_pred_logits_val = []\n",
        "\n",
        "    print(\"Computing adaptive deferral threshold using validation set...\")\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, padding_mask_batch in tqdm(val_loader, desc=\"Adaptive Threshold Calc\"):\n",
        "            X_batch, y_batch, padding_mask_batch = X_batch.to(device), y_batch.to(device), padding_mask_batch.to(device)\n",
        "            model_output = model(X_batch, padding_mask_batch=padding_mask_batch)\n",
        "            all_sensitivities.append(model_output['final_sensitivity_score'].cpu())\n",
        "            all_y_true_val.append(y_batch.cpu())\n",
        "            all_y_pred_logits_val.append(model_output['y_pred_logits'].cpu())\n",
        "\n",
        "    if not all_sensitivities:\n",
        "        print(\"Warning: No sensitivities collected from validation set. Returning default threshold 0.5.\")\n",
        "        return 0.5\n",
        "\n",
        "    sensitivities_np = torch.cat(all_sensitivities).numpy()\n",
        "    y_true_np = torch.cat(all_y_true_val).numpy()\n",
        "    y_pred_logits_np = torch.cat(all_y_pred_logits_val).numpy()\n",
        "\n",
        "    if sensitivities_np.size == 0:\n",
        "        print(\"Warning: Sensitivities array is empty. Returning default threshold 0.5.\")\n",
        "        return 0.5\n",
        "\n",
        "    method = adaptive_thresh_config.get('method', 'percentile')\n",
        "    num_candidates = adaptive_thresh_config.get('num_threshold_candidates', 200)\n",
        "\n",
        "    # Determine potential threshold candidates from unique sensitivity scores or linspace\n",
        "    # Using unique sorted scores is often better than linspace if distribution is skewed\n",
        "    candidate_thresholds = np.unique(sensitivities_np)\n",
        "    if len(candidate_thresholds) > num_candidates: # If too many unique values, sample them\n",
        "        candidate_thresholds = np.percentile(sensitivities_np, np.linspace(0, 100, num_candidates))\n",
        "    elif len(candidate_thresholds) == 0: # Should not happen if sensitivities_np is not empty\n",
        "         print(\"Warning: No unique sensitivity scores found. Returning default 0.5\")\n",
        "         return 0.5\n",
        "\n",
        "\n",
        "    optimal_threshold = 0.5 # Default\n",
        "\n",
        "    if method == 'percentile':\n",
        "        percentile_val = adaptive_thresh_config.get('percentile_value_for_threshold', 90)\n",
        "        # Threshold is the sensitivity score at this percentile.\n",
        "        # Samples with sensitivity *above* this threshold are deferred.\n",
        "        optimal_threshold = np.percentile(sensitivities_np, percentile_val)\n",
        "        print(f\"Adaptive threshold (percentile {percentile_val}%): {optimal_threshold:.4f}\")\n",
        "\n",
        "    elif method == 'target_defer_rate':\n",
        "        target_dr = adaptive_thresh_config.get('target_defer_rate_value', 0.10)\n",
        "        best_thresh_for_target_dr = candidate_thresholds[0] # Start with lowest sens as threshold (max defer)\n",
        "        min_dr_diff = float('inf')\n",
        "\n",
        "        for thresh in candidate_thresholds:\n",
        "            deferred_mask = sensitivities_np > thresh\n",
        "            current_dr = np.mean(deferred_mask)\n",
        "            dr_diff = abs(current_dr - target_dr)\n",
        "            if dr_diff < min_dr_diff:\n",
        "                min_dr_diff = dr_diff\n",
        "                best_thresh_for_target_dr = thresh\n",
        "            elif dr_diff == min_dr_diff and current_dr < target_dr : # Prefer lower DR if diff is same\n",
        "                best_thresh_for_target_dr = thresh\n",
        "\n",
        "\n",
        "        optimal_threshold = best_thresh_for_target_dr\n",
        "        final_dr = np.mean(sensitivities_np > optimal_threshold)\n",
        "        print(f\"Adaptive threshold (target DR ~{target_dr*100:.1f}%): {optimal_threshold:.4f} (results in actual DR: {final_dr*100:.1f}%)\")\n",
        "\n",
        "    elif method == 'max_acc_under_budget':\n",
        "        max_budget_dr = adaptive_thresh_config.get('max_defer_rate_budget', 0.20)\n",
        "        best_thresh_for_acc = candidate_thresholds[-1] # Start with highest sens as threshold (min defer)\n",
        "        max_acc_nd = -1.0\n",
        "\n",
        "        # Determine predictions for accuracy calculation\n",
        "        if model.output_dim_val == 1: # Binary\n",
        "            preds_np = (1 / (1 + np.exp(-y_pred_logits_np))).squeeze() > 0.5 # Sigmoid then threshold\n",
        "            true_labels_np = y_true_np.astype(float)\n",
        "        else: # Multiclass\n",
        "            preds_np = np.argmax(y_pred_logits_np, axis=1)\n",
        "            true_labels_np = y_true_np.astype(int)\n",
        "\n",
        "        for thresh in candidate_thresholds:\n",
        "            deferred_mask = sensitivities_np > thresh\n",
        "            current_dr = np.mean(deferred_mask)\n",
        "\n",
        "            if current_dr <= max_budget_dr: # Only consider if within budget\n",
        "                non_deferred_mask = ~deferred_mask\n",
        "                if np.sum(non_deferred_mask) > 0:\n",
        "                    acc_nd = accuracy_score(true_labels_np[non_deferred_mask], preds_np[non_deferred_mask])\n",
        "                    if acc_nd > max_acc_nd:\n",
        "                        max_acc_nd = acc_nd\n",
        "                        best_thresh_for_acc = thresh\n",
        "                    # If acc is same, prefer lower deferral rate (higher threshold)\n",
        "                    elif acc_nd == max_acc_nd and thresh > best_thresh_for_acc :\n",
        "                        best_thresh_for_acc = thresh\n",
        "\n",
        "        optimal_threshold = best_thresh_for_acc\n",
        "        final_dr = np.mean(sensitivities_np > optimal_threshold)\n",
        "        final_acc_nd = -1.0\n",
        "        if np.sum(~(sensitivities_np > optimal_threshold)) > 0:\n",
        "           final_acc_nd = accuracy_score(true_labels_np[~(sensitivities_np > optimal_threshold)], preds_np[~(sensitivities_np > optimal_threshold)])\n",
        "        print(f\"Adaptive threshold (max Acc_ND under {max_budget_dr*100:.1f}% DR): {optimal_threshold:.4f}\")\n",
        "        print(f\"  Results in: Actual DR={final_dr*100:.1f}%, Acc_ND={final_acc_nd*100:.2f}%\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Warning: Unknown adaptive threshold method '{method}'. Using default 0.5.\")\n",
        "        optimal_threshold = 0.5\n",
        "\n",
        "    return float(optimal_threshold)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgiwpVYZpSVZ"
      },
      "source": [
        "# Cell 12: Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvn-CWrupUE8"
      },
      "outputs": [],
      "source": [
        "# --- Cell 12: Evaluation Function ---\n",
        "\n",
        "def evaluate_model_with_adaptive_deferral(\n",
        "    model: UniversalCGDModel,\n",
        "    test_loader: DataLoader,\n",
        "    adaptive_threshold: float,\n",
        "    device: torch.device,\n",
        "    dataset_name: str = \"Test Set\" # For printing purposes\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluates the trained CGD model on a test set using an adaptive deferral threshold.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_y_true_test = []\n",
        "    all_y_pred_logits_test = []\n",
        "    all_final_sensitivity_test = []\n",
        "\n",
        "    print(f\"\\nEvaluating model on {dataset_name} with adaptive threshold: {adaptive_threshold:.4f}\")\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, padding_mask_batch in tqdm(test_loader, desc=f\"Evaluating {dataset_name}\"):\n",
        "            X_batch, y_batch, padding_mask_batch = X_batch.to(device), y_batch.to(device), padding_mask_batch.to(device)\n",
        "            model_output = model(X_batch, padding_mask_batch=padding_mask_batch)\n",
        "\n",
        "            all_y_true_test.append(y_batch.cpu())\n",
        "            all_y_pred_logits_test.append(model_output['y_pred_logits'].cpu())\n",
        "            all_final_sensitivity_test.append(model_output['final_sensitivity_score'].cpu())\n",
        "\n",
        "    if not all_y_true_test:\n",
        "        print(f\"Warning: No data processed during evaluation for {dataset_name}. Returning empty metrics.\")\n",
        "        return {}\n",
        "\n",
        "    y_true_np = torch.cat(all_y_true_test).numpy()\n",
        "    y_pred_logits_np = torch.cat(all_y_pred_logits_test).numpy()\n",
        "    sensitivities_np = torch.cat(all_final_sensitivity_test).numpy()\n",
        "\n",
        "    # --- Determine Predictions and Errors (Overall) ---\n",
        "    is_binary_classification = model.output_dim_val == 1\n",
        "    if is_binary_classification:\n",
        "        y_pred_probs_overall = 1 / (1 + np.exp(-y_pred_logits_np.squeeze())) # Sigmoid\n",
        "        y_pred_classes_overall = (y_pred_probs_overall > 0.5).astype(int)\n",
        "        y_true_labels_overall = y_true_np.astype(float) # For consistency if originally float\n",
        "    else: # Multiclass\n",
        "        y_pred_probs_overall = F.softmax(torch.from_numpy(y_pred_logits_np), dim=1).numpy()\n",
        "        y_pred_classes_overall = np.argmax(y_pred_logits_np, axis=1)\n",
        "        y_true_labels_overall = y_true_np.astype(int)\n",
        "\n",
        "    errors_overall = (y_pred_classes_overall != y_true_labels_overall).astype(int)\n",
        "    accuracy_overall = accuracy_score(y_true_labels_overall, y_pred_classes_overall)\n",
        "\n",
        "    # --- Apply Adaptive Deferral ---\n",
        "    defer_eval_time_mask = sensitivities_np > adaptive_threshold # True for deferred samples\n",
        "    defer_rate_eval = np.mean(defer_eval_time_mask)\n",
        "\n",
        "    non_deferred_mask_eval = ~defer_eval_time_mask\n",
        "\n",
        "    metrics = {\n",
        "        'dataset_name': dataset_name,\n",
        "        'adaptive_deferral_threshold': adaptive_threshold,\n",
        "        'accuracy_overall': accuracy_overall, # Accuracy if no deferral happened\n",
        "        'defer_rate_eval_time': defer_rate_eval,\n",
        "        'total_samples': len(y_true_np),\n",
        "        'num_deferred_eval_time': np.sum(defer_eval_time_mask),\n",
        "        'num_non_deferred_eval_time': np.sum(non_deferred_mask_eval),\n",
        "    }\n",
        "\n",
        "    # --- Metrics for Non-Deferred Samples ---\n",
        "    if np.sum(non_deferred_mask_eval) > 0:\n",
        "        y_true_nd = y_true_labels_overall[non_deferred_mask_eval]\n",
        "        y_pred_classes_nd = y_pred_classes_overall[non_deferred_mask_eval]\n",
        "        y_pred_probs_nd = y_pred_probs_overall[non_deferred_mask_eval] # For AUC\n",
        "\n",
        "        metrics['accuracy_non_deferred'] = accuracy_score(y_true_nd, y_pred_classes_nd)\n",
        "        if is_binary_classification:\n",
        "            metrics['precision_non_deferred'] = precision_score(y_true_nd, y_pred_classes_nd, zero_division=0)\n",
        "            metrics['recall_non_deferred'] = recall_score(y_true_nd, y_pred_classes_nd, zero_division=0)\n",
        "            metrics['f1_score_non_deferred'] = f1_score(y_true_nd, y_pred_classes_nd, zero_division=0)\n",
        "            if len(np.unique(y_true_nd)) > 1: # AUC requires at least two classes in true labels\n",
        "                 metrics['auc_non_deferred'] = roc_auc_score(y_true_nd, y_pred_probs_nd) # Use probabilities for AUC\n",
        "            else:\n",
        "                 metrics['auc_non_deferred'] = float('nan')\n",
        "        else: # Multiclass\n",
        "            metrics['precision_non_deferred'] = precision_score(y_true_nd, y_pred_classes_nd, average='weighted', zero_division=0)\n",
        "            metrics['recall_non_deferred'] = recall_score(y_true_nd, y_pred_classes_nd, average='weighted', zero_division=0)\n",
        "            metrics['f1_score_non_deferred'] = f1_score(y_true_nd, y_pred_classes_nd, average='weighted', zero_division=0)\n",
        "            # For multiclass AUC, use one-vs-rest and then average (macro)\n",
        "            if len(np.unique(y_true_nd)) >= model.output_dim_val and model.output_dim_val > 1 : # Check if all classes present for OvR\n",
        "                try:\n",
        "                    metrics['auc_non_deferred'] = roc_auc_score(y_true_nd, y_pred_probs_nd, multi_class='ovr', average='macro')\n",
        "                except ValueError as e_auc: # Handle cases like only one class present in y_true_nd after filtering\n",
        "                    print(f\"Warning: Could not compute multiclass AUC for non-deferred: {e_auc}\")\n",
        "                    metrics['auc_non_deferred'] = float('nan')\n",
        "            else:\n",
        "                metrics['auc_non_deferred'] = float('nan')\n",
        "    else:\n",
        "        metrics.update({\n",
        "            'accuracy_non_deferred': float('nan'), 'precision_non_deferred': float('nan'),\n",
        "            'recall_non_deferred': float('nan'), 'f1_score_non_deferred': float('nan'),\n",
        "            'auc_non_deferred': float('nan')\n",
        "        })\n",
        "\n",
        "    # --- Metrics for Deferred Samples ---\n",
        "    if np.sum(defer_eval_time_mask) > 0:\n",
        "        y_true_d = y_true_labels_overall[defer_eval_time_mask]\n",
        "        y_pred_classes_d = y_pred_classes_overall[defer_eval_time_mask]\n",
        "        metrics['accuracy_deferred'] = accuracy_score(y_true_d, y_pred_classes_d)\n",
        "    else:\n",
        "        metrics['accuracy_deferred'] = float('nan') # Or 0.0 if preferred when no samples deferred\n",
        "\n",
        "    # --- Correlation between Sensitivity and Error ---\n",
        "    if len(sensitivities_np) > 1 and len(errors_overall) > 1 and np.std(sensitivities_np) > 0 and np.std(errors_overall) > 0:\n",
        "        metrics['sensitivity_error_correlation'] = np.corrcoef(sensitivities_np, errors_overall)[0, 1]\n",
        "    else:\n",
        "        metrics['sensitivity_error_correlation'] = float('nan')\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Evaluation Metrics for {dataset_name} ---\")\n",
        "    for key, value in metrics.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {key:<35}: {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {key:<35}: {value}\")\n",
        "    print(\"--------------------------------------\")\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxUEQK-fp6zU"
      },
      "source": [
        "# Cell 13: Explanation & Visualization Utilities - Saliency Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvxAoC10p77Z"
      },
      "outputs": [],
      "source": [
        "# --- Cell 13: Explanation & Visualization Utilities - Saliency Maps ---\n",
        "\n",
        "class CGDExplainer:\n",
        "    \"\"\"\n",
        "    Explainer for the UniversalCGDModel using Captum.\n",
        "    Provides methods to explain predictions and sensitivity scores.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: UniversalCGDModel, explainer_config: Dict[str, Any]):\n",
        "        self.model = model # UniversalCGDModel instance\n",
        "        self.explainer_config = explainer_config\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "        if Saliency is None or IntegratedGradients is None:\n",
        "            print(\"Warning: Captum library not fully available. Explanation capabilities will be limited.\")\n",
        "\n",
        "\n",
        "    def _get_prediction_output_for_target_class(self, X_batch: torch.Tensor, padding_mask: Optional[torch.Tensor], target_class_idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Wrapper for Captum: Returns the logit of the target_class_idx.\n",
        "        Input X_batch is already on the correct device.\n",
        "        \"\"\"\n",
        "        model_output = self.model(X_batch, padding_mask_batch=padding_mask)\n",
        "        # y_pred_logits shape: [batch_size, num_classes]\n",
        "        return model_output['y_pred_logits'][:, target_class_idx] # Return score for the specific target class\n",
        "\n",
        "    def _get_sensitivity_score_output(self, X_batch: torch.Tensor, padding_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Wrapper for Captum: Returns the final_sensitivity_score.\n",
        "        Input X_batch is already on the correct device.\n",
        "        \"\"\"\n",
        "        model_output = self.model(X_batch, padding_mask_batch=padding_mask)\n",
        "        return model_output['final_sensitivity_score'] # Shape [batch_size]\n",
        "\n",
        "    def attribute_input(\n",
        "        self,\n",
        "        X_batch: torch.Tensor, # Expected on self.device\n",
        "        padding_mask: Optional[torch.Tensor], # Expected on self.device\n",
        "        explanation_target: str = 'prediction', # 'prediction' or 'sensitivity'\n",
        "        target_class_idx: Optional[int] = None, # Required if explanation_target is 'prediction'\n",
        "        abs_attribution: bool = True\n",
        "    ) -> Optional[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Computes feature attributions for the input batch.\n",
        "        Args:\n",
        "            X_batch: Input tensor [batch_size, seq_len, input_dim] on self.device\n",
        "            padding_mask: Optional padding mask [batch_size, seq_len] on self.device\n",
        "            explanation_target: 'prediction' or 'sensitivity'.\n",
        "            target_class_idx: The index of the class to explain for 'prediction'.\n",
        "                               If None for multiclass prediction, will use predicted class.\n",
        "            abs_attribution: Whether to take the absolute value of attributions.\n",
        "        Returns:\n",
        "            Attributions as a NumPy array [batch_size, seq_len, input_dim] or None if error.\n",
        "        \"\"\"\n",
        "        if Saliency is None and self.explainer_config.get('method') == 'saliency':\n",
        "            print(\"Cannot compute Saliency: Captum not fully available or Saliency method failed to import.\")\n",
        "            return None\n",
        "        if IntegratedGradients is None and self.explainer_config.get('method') == 'integrated_gradients':\n",
        "            print(\"Cannot compute IntegratedGradients: Captum not fully available or IG method failed to import.\")\n",
        "            return None\n",
        "\n",
        "        self.model.eval() # Ensure model is in eval mode for explanations\n",
        "        original_input_requires_grad = X_batch.requires_grad\n",
        "        X_batch.requires_grad_(True) # Input must require grad for Captum\n",
        "\n",
        "        attributions = None\n",
        "\n",
        "        forward_callable: Callable\n",
        "        method_name = self.explainer_config.get('method', 'saliency')\n",
        "\n",
        "        if explanation_target == 'prediction':\n",
        "            if target_class_idx is None: # If no target class, explain the predicted class\n",
        "                with torch.no_grad(): # Get predictions without affecting gradients for explanation\n",
        "                    model_output_temp = self.model(X_batch, padding_mask_batch=padding_mask)\n",
        "                    if self.model.output_dim_val == 1: # Binary\n",
        "                         # For binary, target_class_idx is implicitly 0 (the single output neuron)\n",
        "                        target_class_idx_eff = 0\n",
        "                    else: # Multiclass\n",
        "                        target_class_idx_eff = torch.argmax(model_output_temp['y_pred_logits'], dim=1)\n",
        "                if isinstance(target_class_idx_eff, torch.Tensor):\n",
        "                    print(f\"Warning: Explaining predicted class for a batch. Using predicted class of first sample: {target_class_idx_eff[0].item()}\")\n",
        "                    target_class_idx_eff = target_class_idx_eff[0].item()\n",
        "\n",
        "            else:\n",
        "                target_class_idx_eff = target_class_idx\n",
        "\n",
        "            # Create a partial function that fixes the target_class_idx argument\n",
        "            current_forward_callable = lambda x_b, p_mask: self._get_prediction_output_for_target_class(x_b, p_mask, target_class_idx_eff)\n",
        "\n",
        "        elif explanation_target == 'sensitivity':\n",
        "            current_forward_callable = self._get_sensitivity_score_output\n",
        "        else:\n",
        "            print(f\"Error: Unknown explanation_target '{explanation_target}'.\")\n",
        "            X_batch.requires_grad_(original_input_requires_grad) # Restore original grad status\n",
        "            return None\n",
        "\n",
        "        inputs_for_captum = (X_batch, padding_mask)\n",
        "\n",
        "        try:\n",
        "            if method_name == 'saliency' and Saliency is not None:\n",
        "                explainer_algo = Saliency(current_forward_callable)\n",
        "                # NoiseTunnel for SmoothGrad/SmoothGrad-Sq\n",
        "                use_noise_tunnel = self.explainer_config.get('noise_tunnel_nt_samples', 0) > 0\n",
        "                if use_noise_tunnel:\n",
        "                    nt_type = self.explainer_config.get('noise_tunnel_nt_type', 'smoothgrad')\n",
        "                    stdevs = self.explainer_config.get('noise_tunnel_stdevs', 0.1)\n",
        "                    nt_samples = self.explainer_config.get('noise_tunnel_nt_samples', 5)\n",
        "                    noise_tunnel = NoiseTunnel(explainer_algo)\n",
        "                    attributions = noise_tunnel.attribute(\n",
        "                        inputs_for_captum[0], # Only pass X_batch for attribution, not mask\n",
        "                        nt_type=nt_type, stdevs=stdevs, nt_samples=nt_samples,\n",
        "                        additional_forward_args=inputs_for_captum[1:], # Pass padding_mask here\n",
        "                        abs=False # Get raw grads, apply abs later if needed\n",
        "                    )\n",
        "                else:\n",
        "                    attributions = explainer_algo.attribute(\n",
        "                        inputs_for_captum[0],\n",
        "                        additional_forward_args=inputs_for_captum[1:],\n",
        "                        abs=False\n",
        "                    )\n",
        "\n",
        "            elif method_name == 'integrated_gradients' and IntegratedGradients is not None:\n",
        "                explainer_algo = IntegratedGradients(current_forward_callable)\n",
        "                baselines = torch.zeros_like(X_batch) # Common baseline for IG\n",
        "                n_steps = self.explainer_config.get('n_steps_ig', 25)\n",
        "                attributions = explainer_algo.attribute(\n",
        "                    inputs_for_captum[0],\n",
        "                    baselines=baselines,\n",
        "                    additional_forward_args=inputs_for_captum[1:],\n",
        "                    n_steps=n_steps,\n",
        "                    internal_batch_size=X_batch.size(0) # Process batch at once if possible\n",
        "                )\n",
        "            else:\n",
        "                print(f\"Warning: Explanation method '{method_name}' not supported or Captum not available.\")\n",
        "                X_batch.requires_grad_(original_input_requires_grad)\n",
        "                return None\n",
        "\n",
        "            if attributions is not None:\n",
        "                if abs_attribution:\n",
        "                    attributions = torch.abs(attributions)\n",
        "                # Sum over the input_dim if it's > 1, for ECG it's 1 so squeeze works.\n",
        "                # Attributions shape: [batch_size, seq_len, input_dim]\n",
        "                attributions_np = attributions.detach().cpu().numpy()\n",
        "                if padding_mask is not None: # Zero out attributions for padded regions\n",
        "                    pad_mask_np = padding_mask.cpu().numpy() # B, S\n",
        "                    attributions_np = attributions_np * (~pad_mask_np[:, :, np.newaxis])\n",
        "\n",
        "                X_batch.requires_grad_(original_input_requires_grad) # Restore original grad status\n",
        "                return attributions_np\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Captum attribution for method '{method_name}': {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            X_batch.requires_grad_(original_input_requires_grad) # Restore original grad status\n",
        "            return None\n",
        "\n",
        "        X_batch.requires_grad_(original_input_requires_grad)\n",
        "        return None # Should be unreachable if logic is correct\n",
        "\n",
        "print(\"\\nCell 13: Explanation Utilities executed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRfNFRzzp6x3"
      },
      "source": [
        "# Cell 14: Visualization Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOh4AWIHqroK"
      },
      "outputs": [],
      "source": [
        "# --- Corrected Cell 14: Visualization Functions ---\n",
        "\n",
        "def plot_training_history(\n",
        "    history: Dict[str, List[float]],\n",
        "    metrics_to_plot: List[Dict[str, str]], # List of dicts specifying keys and plot titles\n",
        "    save_path: Optional[str] = \"training_history.png\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots specified metrics from the training history.\n",
        "    Each dict in metrics_to_plot should be:\n",
        "    {'train_key': 'train_loss_key', 'val_key': 'val_loss_key',\n",
        "     'title': 'Plot Title', 'ylabel': 'Y-axis Label'}\n",
        "    \"\"\"\n",
        "    num_plots = len(metrics_to_plot)\n",
        "    if num_plots == 0:\n",
        "        print(\"No metrics specified for plotting training history.\")\n",
        "        return\n",
        "\n",
        "    cols = 2 if num_plots > 1 else 1\n",
        "    rows = (num_plots + cols - 1) // cols\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(7 * cols, 5 * rows), squeeze=False)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, metric_info in enumerate(metrics_to_plot):\n",
        "        ax = axes[i]\n",
        "        train_key = metric_info.get('train_key')\n",
        "        val_key = metric_info.get('val_key')\n",
        "\n",
        "        if train_key and train_key in history and len(history[train_key]) > 0:\n",
        "            ax.plot(history[train_key], label=f\"Train {metric_info.get('ylabel', 'Value')}\", marker='.')\n",
        "        if val_key and val_key in history and len(history[val_key]) > 0:\n",
        "            ax.plot(history[val_key], label=f\"Validation {metric_info.get('ylabel', 'Value')}\", marker='.')\n",
        "\n",
        "        ax.set_title(metric_info.get('title', 'Training Metric'))\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "        ax.set_ylabel(metric_info.get('ylabel', 'Value'))\n",
        "        if (train_key and train_key in history and len(history[train_key]) > 0) or \\\n",
        "           (val_key and val_key in history and len(history[val_key]) > 0) : # Only show legend if there's something to plot\n",
        "            ax.legend()\n",
        "        ax.grid(True)\n",
        "\n",
        "    for j in range(num_plots, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Training history plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix_custom(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred_classes: np.ndarray,\n",
        "    class_names: List[str],\n",
        "    title: str = \"Confusion Matrix\",\n",
        "    save_path: Optional[str] = \"confusion_matrix.png\"\n",
        "):\n",
        "    \"\"\"Plots a normalized confusion matrix.\"\"\"\n",
        "    if len(y_true) == 0 or len(y_pred_classes) == 0:\n",
        "        print(f\"Cannot plot confusion matrix for '{title}': No data provided.\")\n",
        "        return\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred_classes, labels=np.arange(len(class_names)))\n",
        "    cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + EPSILON)\n",
        "\n",
        "    plt.figure(figsize=(max(8, len(class_names)), max(6, len(class_names) * 0.8)))\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                annot_kws={\"size\": 10 if len(class_names) < 10 else 7}) # Adjust font size\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Confusion matrix plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_sensitivity_distributions(\n",
        "    sensitivities: np.ndarray,\n",
        "    errors: np.ndarray, # Boolean array (True if error)\n",
        "    defer_mask_eval: Optional[np.ndarray], # Boolean array (True if deferred by adaptive threshold)\n",
        "    adaptive_threshold: Optional[float],\n",
        "    title: str = \"Sensitivity Score Distribution\",\n",
        "    save_path: Optional[str] = \"sensitivity_distribution.png\"\n",
        "):\n",
        "    \"\"\"Plots distributions of sensitivity scores.\"\"\"\n",
        "    if sensitivities.size == 0:\n",
        "        print(f\"Cannot plot sensitivity distribution for '{title}': No sensitivity data.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(sensitivities[~errors], label=\"Correct Predictions\", color=\"green\", kde=True, stat=\"density\", element=\"step\")\n",
        "    if np.sum(errors) > 0 :\n",
        "        sns.histplot(sensitivities[errors], label=\"Incorrect Predictions\", color=\"red\", kde=True, stat=\"density\", element=\"step\")\n",
        "    if adaptive_threshold is not None:\n",
        "        plt.axvline(adaptive_threshold, color=\"purple\", linestyle=\"--\", label=f\"Adaptive Thresh ({adaptive_threshold:.3f})\")\n",
        "    plt.title(\"Sensitivity by Prediction Correctness\")\n",
        "    plt.xlabel(\"Final Sensitivity Score\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if defer_mask_eval is not None:\n",
        "        if np.sum(~defer_mask_eval) > 0:\n",
        "            sns.histplot(sensitivities[~defer_mask_eval], label=\"Non-Deferred by Eval Thresh\", color=\"blue\", kde=True, stat=\"density\", element=\"step\")\n",
        "        if np.sum(defer_mask_eval) > 0:\n",
        "            sns.histplot(sensitivities[defer_mask_eval], label=\"Deferred by Eval Thresh\", color=\"orange\", kde=True, stat=\"density\", element=\"step\")\n",
        "        if adaptive_threshold is not None:\n",
        "            plt.axvline(adaptive_threshold, color=\"purple\", linestyle=\"--\", label=f\"Adaptive Thresh ({adaptive_threshold:.3f})\")\n",
        "        plt.title(\"Sensitivity by Eval Deferral Decision\")\n",
        "    else:\n",
        "        sns.histplot(sensitivities, label=\"All Samples\", color=\"gray\", kde=True, stat=\"density\")\n",
        "        plt.title(\"Overall Sensitivity Distribution\")\n",
        "\n",
        "    plt.xlabel(\"Final Sensitivity Score\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    fig_title = title if title != \"Sensitivity Score Distribution\" else \"Sensitivity Score Distributions\"\n",
        "    plt.suptitle(fig_title, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Sensitivity distribution plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_auc_curves(\n",
        "    y_true_list: List[np.ndarray],\n",
        "    y_pred_probs_list: List[np.ndarray],\n",
        "    label_names_list: List[str],\n",
        "    output_dim: int,\n",
        "    class_names: Optional[List[str]] = None,\n",
        "    title: str = \"ROC Curves\",\n",
        "    save_path: Optional[str] = \"roc_curves.png\"\n",
        "):\n",
        "    \"\"\"Plots ROC curves for binary or multiclass (OvR) classifications.\"\"\"\n",
        "    if not y_true_list or not y_pred_probs_list or len(y_true_list) != len(y_pred_probs_list):\n",
        "        print(f\"Cannot plot ROC for '{title}': Invalid input lists.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    for i, (y_true, y_pred_probs, curve_label_name) in enumerate(zip(y_true_list, y_pred_probs_list, label_names_list)):\n",
        "        if y_true.size == 0 or y_pred_probs.size == 0:\n",
        "            print(f\"Skipping ROC curve for '{curve_label_name}': empty data.\")\n",
        "            continue\n",
        "\n",
        "        if output_dim == 1:\n",
        "            if len(np.unique(y_true)) < 2:\n",
        "                print(f\"Skipping ROC for '{curve_label_name}' (binary): only one class present in y_true.\")\n",
        "                continue\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_pred_probs)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.plot(fpr, tpr, lw=2, label=f'{curve_label_name} (AUC = {roc_auc:.3f})')\n",
        "        else:\n",
        "            fpr = dict()\n",
        "            tpr = dict()\n",
        "            roc_auc = dict()\n",
        "            valid_classes_for_auc = 0\n",
        "            auc_sum = 0\n",
        "\n",
        "            for class_idx in range(output_dim):\n",
        "                y_true_class = (y_true == class_idx).astype(int)\n",
        "                if len(np.unique(y_true_class)) < 2:\n",
        "                    continue\n",
        "                y_pred_probs_class = y_pred_probs[:, class_idx]\n",
        "                fpr[class_idx], tpr[class_idx], _ = roc_curve(y_true_class, y_pred_probs_class)\n",
        "                roc_auc[class_idx] = auc(fpr[class_idx], tpr[class_idx])\n",
        "                auc_sum += roc_auc[class_idx]\n",
        "                valid_classes_for_auc +=1\n",
        "\n",
        "            if valid_classes_for_auc > 0:\n",
        "                macro_auc = auc_sum / valid_classes_for_auc\n",
        "                first_valid_class = next((k for k in fpr if fpr[k] is not None), None)\n",
        "                if first_valid_class is not None:\n",
        "                     plt.plot(fpr[first_valid_class], tpr[first_valid_class], lw=2, label=f'{curve_label_name} (Macro AUC = {macro_auc:.3f})')\n",
        "                else:\n",
        "                     plt.plot([0,1],[0,1], linestyle='--', lw=1, label=f'{curve_label_name} (Macro AUC = {macro_auc:.3f} - No individual plottable)')\n",
        "            else:\n",
        "                 print(f\"No valid classes for ROC AUC calculation in '{curve_label_name}' (multiclass).\")\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"ROC curve plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_ecg_with_saliency(\n",
        "    ecg_signal: np.ndarray,\n",
        "    attributions: Optional[np.ndarray],\n",
        "    title: str = \"ECG Signal with Saliency\",\n",
        "    save_path: Optional[str] = None,\n",
        "    true_label_name: Optional[str] = None,\n",
        "    pred_label_name: Optional[str] = None,\n",
        "    is_deferred: Optional[bool] = None,\n",
        "    sensitivity_score: Optional[float] = None,\n",
        "    sampling_rate: int = 125\n",
        "):\n",
        "    \"\"\"Plots ECG signal with saliency attributions overlaid or shown below.\"\"\"\n",
        "    ecg_signal = ecg_signal.squeeze()\n",
        "    if attributions is not None:\n",
        "        attributions = attributions.squeeze()\n",
        "        if attributions.shape != ecg_signal.shape:\n",
        "            print(f\"Warning: Attributions shape {attributions.shape} differs from ECG signal shape {ecg_signal.shape}. Cannot overlay directly.\")\n",
        "            attributions = None\n",
        "\n",
        "    time_axis = np.arange(len(ecg_signal)) / sampling_rate\n",
        "    fig, ax1 = plt.subplots(figsize=(15, 5))\n",
        "\n",
        "    color_ecg = 'tab:blue'\n",
        "    ax1.set_xlabel(f\"Time (s) - Sample Rate: {sampling_rate}Hz\")\n",
        "    ax1.set_ylabel(\"ECG Amplitude\", color=color_ecg)\n",
        "    ax1.plot(time_axis, ecg_signal, color=color_ecg, linewidth=1.5, label=\"ECG Signal\")\n",
        "    ax1.tick_params(axis='y', labelcolor=color_ecg)\n",
        "    ax1.grid(True, axis='x', linestyle=':')\n",
        "    ax1.grid(True, axis='y', linestyle=':', color=color_ecg, alpha=0.5)\n",
        "\n",
        "    if attributions is not None:\n",
        "        ax2 = ax1.twinx()\n",
        "        color_saliency = 'tab:red'\n",
        "        ax2.set_ylabel(\"Attribution Score\", color=color_saliency)\n",
        "        ax2.bar(time_axis, attributions, color=color_saliency, alpha=0.6, width=1/sampling_rate, label=\"Saliency\")\n",
        "        ax2.tick_params(axis='y', labelcolor=color_saliency)\n",
        "        ax2.grid(True, axis='y', linestyle=':', color=color_saliency, alpha=0.5)\n",
        "\n",
        "    full_title = title\n",
        "    details = []\n",
        "    if true_label_name: details.append(f\"True: {true_label_name}\")\n",
        "    if pred_label_name: details.append(f\"Pred: {pred_label_name}\")\n",
        "    if is_deferred is not None: details.append(f\"Deferred: {is_deferred}\")\n",
        "    if sensitivity_score is not None: details.append(f\"Sens: {sensitivity_score:.3f}\")\n",
        "    if details: full_title += \"\\n(\" + \" | \".join(details) + \")\"\n",
        "    plt.title(full_title, fontsize=14)\n",
        "\n",
        "    lines, labels = ax1.get_legend_handles_labels()\n",
        "    if attributions is not None and 'ax2' in locals(): # Check if ax2 was created\n",
        "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "        ax2.legend(lines + lines2, labels + labels2, loc='upper right')\n",
        "    else:\n",
        "        ax1.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"ECG with saliency plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_deferral_performance_vs_threshold(\n",
        "    sensitivities_val: np.ndarray,\n",
        "    y_true_val: np.ndarray,\n",
        "    y_pred_logits_val: np.ndarray,\n",
        "    model_output_dim: int,\n",
        "    title: str = \"Deferral Performance vs. Sensitivity Threshold\",\n",
        "    save_path: Optional[str] = \"deferral_performance.png\",\n",
        "    num_threshold_points: int = 50\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots Accuracy_Non_Deferred vs. Deferral_Rate across a range of sensitivity thresholds.\n",
        "    \"\"\"\n",
        "    if sensitivities_val.size == 0:\n",
        "        print(f\"Cannot plot deferral performance for '{title}': No sensitivity data.\")\n",
        "        return\n",
        "\n",
        "    threshold_candidates = np.percentile(sensitivities_val, np.linspace(0, 100, num_threshold_points))\n",
        "    threshold_candidates = np.unique(threshold_candidates)\n",
        "\n",
        "    deferral_rates = []\n",
        "    accuracies_nd = []\n",
        "\n",
        "    if model_output_dim == 1:\n",
        "        preds_overall = (1 / (1 + np.exp(-y_pred_logits_val.squeeze())) > 0.5).astype(int)\n",
        "        true_labels_overall = y_true_val.astype(float)\n",
        "    else:\n",
        "        preds_overall = np.argmax(y_pred_logits_val, axis=1)\n",
        "        true_labels_overall = y_true_val.astype(int)\n",
        "\n",
        "    for thresh in tqdm(threshold_candidates, desc=\"Plotting Deferral Curve\"):\n",
        "        defer_mask = sensitivities_val > thresh\n",
        "        current_dr = np.mean(defer_mask)\n",
        "        deferral_rates.append(current_dr)\n",
        "\n",
        "        non_deferred_mask = ~defer_mask\n",
        "        if np.sum(non_deferred_mask) > 0:\n",
        "            acc_nd = accuracy_score(true_labels_overall[non_deferred_mask], preds_overall[non_deferred_mask])\n",
        "            accuracies_nd.append(acc_nd)\n",
        "        else:\n",
        "            accuracies_nd.append(0.0)\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "    color1 = 'tab:blue'\n",
        "    ax1.set_xlabel(\"Deferral Rate\")\n",
        "    ax1.set_ylabel(\"Accuracy on Non-Deferred Samples\", color=color1)\n",
        "    ax1.plot(deferral_rates, accuracies_nd, color=color1, marker='o', linestyle='-')\n",
        "    ax1.tick_params(axis='y', labelcolor=color1)\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.set_xlabel(\"Sensitivity Threshold Value\")\n",
        "    # For twinx/twiny, to align ticks, the data for the second axis plot doesn't matter as much as the limits\n",
        "    # We plot the thresholds against the accuracies_nd to get the scale, but then make line invisible\n",
        "    ax2.plot(threshold_candidates, accuracies_nd, alpha=0)\n",
        "    ax2.set_xlim(threshold_candidates.min(), threshold_candidates.max())\n",
        "\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.tight_layout() # May need adjustment if labels overlap\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Deferral performance plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nCell 14 (Corrected): Visualization Functions executed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpN5bgXEr-gL"
      },
      "source": [
        "# Cell 15: Setup and Model Configuration for MIT-BIH Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKt688sOr_vo"
      },
      "outputs": [],
      "source": [
        "# --- Cell 15: Setup and Model Configuration for MIT-BIH Dataset ---\n",
        "\n",
        "if 'mitbih_loaders' not in globals() or mitbih_loaders is None:\n",
        "    print(\"ERROR: `mitbih_loaders` not found. Please ensure Cell 3 (Revised) was run successfully and created these DataLoaders.\")\n",
        "    # mitbih_loaders = {'train': None, 'val': None, 'test': None} # Placeholder to avoid immediate crash\n",
        "else:\n",
        "    print(\"MIT-BIH DataLoaders found.\")\n",
        "    if not all(k in mitbih_loaders for k in ['train', 'val', 'test']):\n",
        "        print(\"Warning: MIT-BIH loaders might be incomplete. Expected 'train', 'val', 'test'.\")\n",
        "\n",
        "\n",
        "# 2. Define MIT-BIH Class Names\n",
        "MITBIH_CLASS_NAMES = ['Normal (N)', 'Supraventricular (S)', 'Ventricular (V)', 'Fusion (F)', 'Unknown (Q)']\n",
        "MITBIH_NUM_CLASSES = len(MITBIH_CLASS_NAMES) # Should be 5\n",
        "\n",
        "# 3. Instantiate UniversalCGDModel for MIT-BIH\n",
        "# We'll use the global configurations from Cell 2, specifying the output_dim.\n",
        "print(f\"\\nConfiguring UniversalCGDModel for MIT-BIH ({MITBIH_NUM_CLASSES} classes)...\")\n",
        "\n",
        "# Ensure all config dictionaries from Cell 2 are available\n",
        "config_vars = ['CGD_MODEL_CONFIG', 'ENCODER_CONFIG', 'PREDICTOR_CONFIG',\n",
        "               'PERTURBATION_CONFIG', 'SENSITIVITY_CONFIG', 'STRUCTURAL_REGULARIZER_CONFIG']\n",
        "missing_configs = [cv for cv in config_vars if cv not in globals()]\n",
        "if missing_configs:\n",
        "    raise NameError(f\"Missing one or more configuration dictionaries: {missing_configs}. Please re-run Cell 2.\")\n",
        "\n",
        "# Create the model instance\n",
        "try:\n",
        "    cgd_model_mitbih = UniversalCGDModel(\n",
        "        model_config=CGD_MODEL_CONFIG,\n",
        "        encoder_config=ENCODER_CONFIG,\n",
        "        predictor_config=PREDICTOR_CONFIG, # output_dim is passed separately\n",
        "        perturb_config=PERTURBATION_CONFIG,\n",
        "        sensitivity_config=SENSITIVITY_CONFIG,\n",
        "        regularizer_config=STRUCTURAL_REGULARIZER_CONFIG,\n",
        "        output_dim=MITBIH_NUM_CLASSES # Crucial: set number of output classes\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    print(\"\\nUniversalCGDModel for MIT-BIH instantiated successfully:\")\n",
        "    # Quick check on a component\n",
        "    print(f\"  Encoder aggregation: {cgd_model_mitbih.encoder.aggregation_method}\")\n",
        "    print(f\"  Predictor output features: {cgd_model_mitbih.predictor.predictor_mlp[-1].out_features}\") # Last layer of MLP\n",
        "    print(f\"  Perturbation types: {cgd_model_mitbih.perturbation_generator._get_active_perturbations()}\")\n",
        "    print(f\"  Sensitivity measures: {cgd_model_mitbih.sensitivity_calculator._get_active_measures()}\")\n",
        "    print(f\"  Regularizer type: {cgd_model_mitbih.structural_regularizer.regularization_type}\")\n",
        "    print(f\"  Model is on device: {next(cgd_model_mitbih.parameters()).device}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error instantiating UniversalCGDModel for MIT-BIH: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    cgd_model_mitbih = None # Ensure it's None if instantiation fails\n",
        "\n",
        "print(\"\\nCell 15: Setup and Model Configuration for MIT-BIH Dataset executed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2036a49asgTL"
      },
      "source": [
        "# Cell 16: Training & Evaluation - MIT-BIH Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrgACUsIRIti"
      },
      "outputs": [],
      "source": [
        "# --- Cell 16: Stage 1 MIT-BIH Training (Train Best Base Classifier) - REWRITTEN ---\n",
        "\n",
        "# Ensure necessary variables from previous cells are available\n",
        "if 'cgd_model_mitbih' not in globals() or cgd_model_mitbih is None:\n",
        "    raise NameError(\"`cgd_model_mitbih` is not defined. Please run Cell 15 with Stage 1 configurations first.\")\n",
        "if 'mitbih_loaders' not in globals() or mitbih_loaders is None or \\\n",
        "   not all(k in mitbih_loaders for k in ['train', 'val', 'test']):\n",
        "    raise NameError(\"`mitbih_loaders` are not properly defined. Please run Cell 3 (Revised) first.\")\n",
        "if 'GENERAL_TRAINING_CONFIG' not in globals() or 'CGD_MODEL_CONFIG' not in globals() or \\\n",
        "   'ADAPTIVE_THRESHOLD_CONFIG' not in globals() or 'EXPLAINER_CONFIG' not in globals(): # Ensure all needed configs are present\n",
        "    raise NameError(\"Core GNERAL_TRAINING_CONFIG or other specific configurations are missing. Please re-run Cell 2.\")\n",
        "if 'MITBIH_CLASS_NAMES' not in globals() or 'MITBIH_NUM_CLASSES' not in globals():\n",
        "    raise NameError(\"`MITBIH_CLASS_NAMES` or `MITBIH_NUM_CLASSES` not defined. Please run Cell 15 first.\")\n",
        "\n",
        "print(f\"Cell 16 (Stage 1 MIT-BIH) will use DEVICE: {DEVICE}\")\n",
        "# Verify Stage 1 specific configurations\n",
        "print(f\"Confirming CGD_MODEL_CONFIG for Stage 1: loss_beta={CGD_MODEL_CONFIG.get('loss_beta')}, deferral_threshold_train={CGD_MODEL_CONFIG.get('deferral_threshold_train')}\")\n",
        "if CGD_MODEL_CONFIG.get('loss_beta') != 0.0 or CGD_MODEL_CONFIG.get('deferral_threshold_train') < 1e8 :\n",
        "    print(\"WARNING: CGD_MODEL_CONFIG does not appear to be correctly set for Stage 1 training.\")\n",
        "    print(\"For Stage 1 (Best Base Classifier), 'loss_beta' should be 0.0 and 'deferral_threshold_train' should be float('inf') or very large.\")\n",
        "    print(\"Please ensure configurations in Cell 2 are set for Stage 1 and re-run Cell 15 before this cell.\")\n",
        "\n",
        "\n",
        "# 1. Train the Base Model for MIT-BIH (Focus on classification accuracy)\n",
        "print(\"--- Starting MIT-BIH Model Training (Stage 1 - Base Classifier) ---\")\n",
        "mitbih_stage1_checkpoint_filename = \"mitbih_base_classifier_best.pt\" # Specific name for Stage 1 model\n",
        "\n",
        "cgd_model_mitbih.to(DEVICE) # Ensure model instance from Cell 15 is on the correct device\n",
        "\n",
        "stage1_mitbih_train_config = GENERAL_TRAINING_CONFIG.copy()\n",
        "# stage1_mitbih_train_config['epochs'] = 50 # Example: more epochs for base model training\n",
        "\n",
        "# This single call trains the model and saves the best version based on validation loss\n",
        "trained_base_mitbih_model, mitbih_stage1_history = train_universal_cgd_model(\n",
        "    model=cgd_model_mitbih,\n",
        "    train_loader=mitbih_loaders['train'],\n",
        "    val_loader=mitbih_loaders['val'],\n",
        "    training_config=stage1_mitbih_train_config, # Use the potentially adjusted config\n",
        "    model_specific_checkpoint_name=mitbih_stage1_checkpoint_filename\n",
        ")\n",
        "print(\"--- MIT-BIH Model Training (Stage 1) Finished ---\")\n",
        "\n",
        "# 2. Plot Training History for Stage 1\n",
        "print(\"\\n--- Plotting MIT-BIH Stage 1 Training History ---\")\n",
        "if mitbih_stage1_history: # Check if history is not empty\n",
        "    mitbih_stage1_metrics_to_plot = [\n",
        "        {'train_key': 'train_total_loss', 'val_key': 'val_total_loss', 'title': 'MIT-BIH Stage 1 Total Loss', 'ylabel': 'Loss'},\n",
        "        {'train_key': 'train_pred_loss', 'val_key': 'val_pred_loss', 'title': 'MIT-BIH Stage 1 Prediction Loss', 'ylabel': 'Pred Loss'},\n",
        "        # Defer rate should be ~0% if deferral_threshold_train is float('inf') in CGD_MODEL_CONFIG\n",
        "        {'train_key': 'train_defer_rate_train_time', 'val_key': 'val_defer_rate_train_time', 'title': 'MIT-BIH Stage 1 Defer Rate (During Train)', 'ylabel': 'Defer Rate'},\n",
        "        # val_accuracy_nd_val_train_thresh will be the overall validation accuracy if defer rate is 0\n",
        "        {'val_key': 'val_accuracy_nd_val_train_thresh', 'title': 'MIT-BIH Stage 1 Val Acc (Effectively Overall)', 'ylabel': 'Accuracy'}\n",
        "    ]\n",
        "    plot_training_history(\n",
        "        mitbih_stage1_history,\n",
        "        mitbih_stage1_metrics_to_plot,\n",
        "        save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"mitbih_stage1_training_history.png\")\n",
        "    )\n",
        "else:\n",
        "    print(\"MIT-BIH Stage 1 training history is empty, skipping plot.\")\n",
        "\n",
        "# 3. Evaluate the Stage 1 Base Model on the Test Set\n",
        "# For Stage 1, the primary interest is overall performance without deferral.\n",
        "# We call evaluate_model_with_adaptive_deferral with a very high threshold to get 'accuracy_overall'.\n",
        "print(\"\\n--- Evaluating MIT-BIH Stage 1 Base Model on Test Set (Overall Performance) ---\")\n",
        "mitbih_stage1_eval_metrics = {} # Initialize\n",
        "if trained_base_mitbih_model is not None and 'test' in mitbih_loaders and mitbih_loaders['test'] is not None:\n",
        "    # Ensure the loaded best model is used for evaluation\n",
        "    trained_base_mitbih_model.to(DEVICE)\n",
        "    trained_base_mitbih_model.eval()\n",
        "\n",
        "    mitbih_stage1_eval_metrics = evaluate_model_with_adaptive_deferral(\n",
        "        model=trained_base_mitbih_model,\n",
        "        test_loader=mitbih_loaders['test'],\n",
        "        adaptive_threshold=float('inf'), # This ensures defer_rate_eval_time is 0.0\n",
        "        device=DEVICE,\n",
        "        dataset_name=\"MIT-BIH Test Set (Stage 1 Base Model - Overall)\"\n",
        "    )\n",
        "    print(f\"MIT-BIH Stage 1 Base Model - Test Accuracy Overall: {mitbih_stage1_eval_metrics.get('accuracy_overall', 'N/A'):.4f}\")\n",
        "    # Other metrics like AUC_non_deferred will represent overall AUC here\n",
        "    print(f\"MIT-BIH Stage 1 Base Model - Test AUC Overall: {mitbih_stage1_eval_metrics.get('auc_non_deferred', 'N/A'):.4f}\")\n",
        "else:\n",
        "    print(\"Skipping Stage 1 MIT-BIH model evaluation (trained model or test loader not available).\")\n",
        "\n",
        "# 4. Visualizations for Stage 1 Overall Performance on Test Set\n",
        "print(\"\\n--- Visualizing MIT-BIH Stage 1 Test Set Overall Performance ---\")\n",
        "if trained_base_mitbih_model is not None and 'test' in mitbih_loaders and mitbih_loaders['test'] is not None and mitbih_stage1_eval_metrics:\n",
        "    # Re-fetch data for plots to ensure consistency and have probabilities for ROC\n",
        "    all_y_true_s1_viz = []\n",
        "    all_y_pred_logits_s1_viz = []\n",
        "    trained_base_mitbih_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_b, y_b, p_mask_b in tqdm(mitbih_loaders['test'], desc=\"Fetching Test Data for Stage 1 Plots\"):\n",
        "            X_b, y_b, p_mask_b = X_b.to(DEVICE), y_b.to(DEVICE), p_mask_b.to(DEVICE)\n",
        "            output = trained_base_mitbih_model(X_b, p_mask_b)\n",
        "            all_y_true_s1_viz.append(y_b.cpu())\n",
        "            all_y_pred_logits_s1_viz.append(output['y_pred_logits'].cpu())\n",
        "\n",
        "    if all_y_true_s1_viz:\n",
        "        y_true_test_np_s1 = torch.cat(all_y_true_s1_viz).numpy().astype(int)\n",
        "        y_pred_logits_test_np_s1 = torch.cat(all_y_pred_logits_s1_viz).numpy()\n",
        "\n",
        "        y_pred_probs_test_s1 = F.softmax(torch.from_numpy(y_pred_logits_test_np_s1), dim=1).numpy()\n",
        "        y_pred_classes_test_s1 = np.argmax(y_pred_logits_test_np_s1, axis=1)\n",
        "\n",
        "        plot_confusion_matrix_custom(\n",
        "            y_true=y_true_test_np_s1,\n",
        "            y_pred_classes=y_pred_classes_test_s1,\n",
        "            class_names=MITBIH_CLASS_NAMES,\n",
        "            title=\"MIT-BIH Stage 1 Base Model CM (Overall Test)\",\n",
        "            save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"mitbih_stage1_cm_overall_test.png\")\n",
        "        )\n",
        "        plot_roc_auc_curves(\n",
        "            y_true_list=[y_true_test_np_s1],\n",
        "            y_pred_probs_list=[y_pred_probs_test_s1],\n",
        "            label_names_list=[\"MIT-BIH Stage 1 Base Model (Overall)\"],\n",
        "            output_dim=MITBIH_NUM_CLASSES,\n",
        "            class_names=MITBIH_CLASS_NAMES,\n",
        "            title=\"MIT-BIH Stage 1 Base Model ROC Curve (Overall Test)\",\n",
        "            save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"mitbih_stage1_roc_overall_test.png\")\n",
        "        )\n",
        "    else:\n",
        "        print(\"Skipping MIT-BIH Stage 1 test visualizations as test data could not be re-fetched or was empty.\")\n",
        "else:\n",
        "    print(\"Skipping MIT-BIH Stage 1 visualizations (trained base model or test loader not available, or prior evaluation failed).\")\n",
        "\n",
        "# 5. Explanations for the Stage 1 Base Model\n",
        "# This helps understand what the base model learned, independently of deferral.\n",
        "captum_ready_s1 = 'Saliency' in globals() and Saliency is not None and 'IntegratedGradients' in globals() and IntegratedGradients is not None\n",
        "explainer_class_ready_s1 = 'CGDExplainer' in globals() and callable(CGDExplainer)\n",
        "\n",
        "if captum_ready_s1 and explainer_class_ready_s1 and trained_base_mitbih_model is not None and \\\n",
        "   'mitbih_loaders' in globals() and mitbih_loaders['test'] is not None:\n",
        "    print(\"\\n--- (Optional) Visualizing Explanations for MIT-BIH Stage 1 Base Model ---\")\n",
        "    mitbih_base_explainer = CGDExplainer(trained_base_mitbih_model, EXPLAINER_CONFIG) # Using global EXPLAINER_CONFIG\n",
        "    num_samples_to_explain_s1 = 2\n",
        "    explained_count_s1 = 0\n",
        "    try:\n",
        "        for X_ex_b, y_ex_b, p_mask_ex_b in mitbih_loaders['test']:\n",
        "            if explained_count_s1 >= num_samples_to_explain_s1: break\n",
        "            samples_to_take = min(X_ex_b.size(0), num_samples_to_explain_s1 - explained_count_s1)\n",
        "\n",
        "            for i_s1 in range(samples_to_take):\n",
        "                if explained_count_s1 >= num_samples_to_explain_s1: break\n",
        "                X_s1, y_s1_true, p_mask_s1 = X_ex_b[i_s1:i_s1+1].to(DEVICE), y_ex_b[i_s1].to(DEVICE), p_mask_ex_b[i_s1:i_s1+1].to(DEVICE)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output_s1 = trained_base_mitbih_model(X_s1, p_mask_s1)\n",
        "                pred_logits_s1 = output_s1['y_pred_logits']\n",
        "                pred_class_s1 = torch.argmax(pred_logits_s1, dim=1).item()\n",
        "\n",
        "                true_lbl_name_s1 = MITBIH_CLASS_NAMES[y_s1_true.item()]\n",
        "                pred_lbl_name_s1 = MITBIH_CLASS_NAMES[pred_class_s1]\n",
        "\n",
        "                print(f\"Explaining MIT-BIH Stage 1 Sample {explained_count_s1+1}: True='{true_lbl_name_s1}', Pred='{pred_lbl_name_s1}'\")\n",
        "                attrs_pred_s1 = mitbih_base_explainer.attribute_input(\n",
        "                    X_s1, p_mask_s1, explanation_target='prediction', target_class_idx=pred_class_s1, abs_attribution=True\n",
        "                )\n",
        "                if attrs_pred_s1 is not None:\n",
        "                    plot_ecg_with_saliency(\n",
        "                        ecg_signal=X_s1.squeeze().cpu().numpy(), attributions=attrs_pred_s1.squeeze(),\n",
        "                        title=f\"MIT-BIH Stage 1 Sample {explained_count_s1+1} - Prediction Explanation\",\n",
        "                        save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], f\"mitbih_stage1_explain_pred_sample{explained_count_s1}.png\"),\n",
        "                        true_label_name=true_lbl_name_s1, pred_label_name=pred_lbl_name_s1\n",
        "                    )\n",
        "                explained_count_s1 += 1\n",
        "            if explained_count_s1 >= num_samples_to_explain_s1: break\n",
        "    except Exception as e_s1_explain:\n",
        "        print(f\"Error during MIT-BIH Stage 1 explanation: {e_s1_explain}\")\n",
        "else:\n",
        "    print(\"\\nSkipping MIT-BIH Stage 1 explanation visualization (components missing).\")\n",
        "\n",
        "\n",
        "# Final check and message for Stage 1\n",
        "final_mitbih_base_model_path = os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], mitbih_stage1_checkpoint_name)\n",
        "if trained_base_mitbih_model is not None and os.path.exists(final_mitbih_base_model_path):\n",
        "    print(f\"\\n--- MIT-BIH Stage 1 Analysis Complete. Best base model successfully saved to {final_mitbih_base_model_path} ---\")\n",
        "    print(f\"This model achieved an overall test accuracy of: {mitbih_stage1_eval_metrics.get('accuracy_overall', 'N/A'):.4f}\")\n",
        "else:\n",
        "    print(f\"\\n--- MIT-BIH Stage 1 Analysis Incomplete. ---\")\n",
        "    if trained_base_mitbih_model is None:\n",
        "         print(\"   Reason: Base model training did not complete or failed.\")\n",
        "    elif not os.path.exists(final_mitbih_base_model_path):\n",
        "         print(f\"   Reason: Best base model checkpoint NOT FOUND at {final_mitbih_base_model_path}\")\n",
        "         print(\"   Please check training logs from 'train_universal_cgd_model' to ensure the model improved and saved correctly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leReD6CWvQXN"
      },
      "outputs": [],
      "source": [
        "# --- Cell 17: Setup and Model Configuration for PTB Dataset ---\n",
        "\n",
        "# 1. Confirm DataLoaders are available\n",
        "if 'ptb_loaders' not in globals() or ptb_loaders is None:\n",
        "    print(\"ERROR: `ptb_loaders` not found. Please ensure Cell 3 (Revised) was run successfully and created these DataLoaders.\")\n",
        "    # Fallback for standalone run (not recommended for full workflow)\n",
        "    # ptb_loaders = {'train': None, 'val': None, 'test': None}\n",
        "else:\n",
        "    print(\"PTB DataLoaders found.\")\n",
        "    if not all(k in ptb_loaders for k in ['train', 'val', 'test']):\n",
        "        print(\"Warning: PTB loaders might be incomplete. Expected 'train', 'val', 'test'.\")\n",
        "\n",
        "# 2. Define PTB Class Names\n",
        "PTB_CLASS_NAMES = ['Normal', 'Myocardial Infarction (Abnormal)']\n",
        "PTB_NUM_CLASSES = 1 # For binary classification with BCEWithLogitsLoss, output_dim is 1\n",
        "# If using CrossEntropyLoss for binary, PTB_NUM_CLASSES would be 2.\n",
        "# Our UniversalCGDModel's compute_loss handles output_dim_val == 1 for BCE.\n",
        "\n",
        "# 3. Instantiate UniversalCGDModel for PTB\n",
        "print(f\"\\nConfiguring UniversalCGDModel for PTB ({PTB_NUM_CLASSES} output neuron for BCEWithLogitsLoss)...\")\n",
        "\n",
        "config_vars = ['CGD_MODEL_CONFIG', 'ENCODER_CONFIG', 'PREDICTOR_CONFIG',\n",
        "               'PERTURBATION_CONFIG', 'SENSITIVITY_CONFIG', 'STRUCTURAL_REGULARIZER_CONFIG']\n",
        "missing_configs = [cv for cv in config_vars if cv not in globals()]\n",
        "if missing_configs:\n",
        "    raise NameError(f\"Missing one or more configuration dictionaries: {missing_configs}. Please re-run Cell 2.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    cgd_model_ptb = UniversalCGDModel(\n",
        "        model_config=CGD_MODEL_CONFIG,\n",
        "        encoder_config=ENCODER_CONFIG,\n",
        "        predictor_config=PREDICTOR_CONFIG, # output_dim is passed separately\n",
        "        perturb_config=PERTURBATION_CONFIG, # Or ptb_perturb_config if defined\n",
        "        sensitivity_config=SENSITIVITY_CONFIG, # Or ptb_sensitivity_config if defined\n",
        "        regularizer_config=STRUCTURAL_REGULARIZER_CONFIG,\n",
        "        output_dim=PTB_NUM_CLASSES # Crucial: set to 1 for binary tasks using BCEWithLogitsLoss\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    print(\"\\nUniversalCGDModel for PTB instantiated successfully:\")\n",
        "    print(f\"  Encoder aggregation: {cgd_model_ptb.encoder.aggregation_method}\")\n",
        "    print(f\"  Predictor output features: {cgd_model_ptb.predictor.predictor_mlp[-1].out_features}\")\n",
        "    print(f\"  Perturbation types: {cgd_model_ptb.perturbation_generator._get_active_perturbations()}\")\n",
        "    print(f\"  Sensitivity measures: {cgd_model_ptb.sensitivity_calculator._get_active_measures()}\")\n",
        "    print(f\"  Regularizer type: {cgd_model_ptb.structural_regularizer.regularization_type}\")\n",
        "    print(f\"  Model is on device: {next(cgd_model_ptb.parameters()).device}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error instantiating UniversalCGDModel for PTB: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    cgd_model_ptb = None # Ensure it's None if instantiation fails\n",
        "\n",
        "print(\"\\nCell 17: Setup and Model Configuration for PTB Dataset executed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGEyfYbOvr1e"
      },
      "outputs": [],
      "source": [
        "# --- Cell 18: Stage 1 PTB Training (Train Best Base Classifier) - Corrected ---\n",
        "\n",
        "# Ensure necessary variables from previous cells are available\n",
        "if 'cgd_model_ptb' not in globals() or cgd_model_ptb is None:\n",
        "    raise NameError(\"`cgd_model_ptb` is not defined. Please run Cell 17 with Stage 1 configurations first.\")\n",
        "if 'ptb_loaders' not in globals() or ptb_loaders is None or \\\n",
        "   not all(k in ptb_loaders for k in ['train', 'val', 'test']):\n",
        "    raise NameError(\"`ptb_loaders` are not properly defined. Please run Cell 3 (Revised) first.\")\n",
        "if 'GENERAL_TRAINING_CONFIG' not in globals() or 'CGD_MODEL_CONFIG' not in globals() or \\\n",
        "   'ADAPTIVE_THRESHOLD_CONFIG' not in globals() or 'EXPLAINER_CONFIG' not in globals():\n",
        "    raise NameError(\"Core GENERAL_TRAINING_CONFIG or other specific configurations are missing. Please re-run Cell 2.\")\n",
        "if 'PTB_CLASS_NAMES' not in globals() or 'PTB_NUM_CLASSES' not in globals():\n",
        "    raise NameError(\"`PTB_CLASS_NAMES` or `PTB_NUM_CLASSES` not defined. Please run Cell 17 first.\")\n",
        "\n",
        "print(f\"Cell 18 (Stage 1 PTB) will use DEVICE: {DEVICE}\")\n",
        "print(f\"Confirming CGD_MODEL_CONFIG for Stage 1: loss_beta={CGD_MODEL_CONFIG.get('loss_beta')}, deferral_threshold_train={CGD_MODEL_CONFIG.get('deferral_threshold_train')}\")\n",
        "if CGD_MODEL_CONFIG.get('loss_beta') != 0.0 or CGD_MODEL_CONFIG.get('deferral_threshold_train') < 1e8 :\n",
        "    print(\"WARNING: CGD_MODEL_CONFIG does not appear to be correctly set for Stage 1 training.\")\n",
        "    print(\"For Stage 1 (Best Base Classifier), 'loss_beta' should be 0.0 and 'deferral_threshold_train' should be float('inf') or very large.\")\n",
        "    print(\"Please ensure configurations in Cell 2 are set for Stage 1 and re-run Cell 17 before this cell.\")\n",
        "\n",
        "\n",
        "# 1. Train the Base Model for PTB (Focus on classification accuracy)\n",
        "print(\"--- Starting PTB Model Training (Stage 1 - Base Classifier) ---\")\n",
        "ptb_base_checkpoint_name = \"ptb_base_classifier_best.pt\"\n",
        "\n",
        "cgd_model_ptb.to(DEVICE)\n",
        "\n",
        "# Define training configuration for Stage 1 (can adjust epochs here)\n",
        "stage1_ptb_training_config = GENERAL_TRAINING_CONFIG.copy()\n",
        "# stage1_ptb_training_config['epochs'] = 50 # Example: Train base model for more epochs if needed\n",
        "\n",
        "# This single call trains the model and saves the best version based on validation loss\n",
        "# The trained model is returned as 'trained_ptb_base_model'\n",
        "trained_ptb_base_model, ptb_stage1_history = train_universal_cgd_model(\n",
        "    model=cgd_model_ptb, # This is the instance of UniversalCGDModel for PTB\n",
        "    train_loader=ptb_loaders['train'],\n",
        "    val_loader=ptb_loaders['val'],\n",
        "    training_config=stage1_ptb_training_config,\n",
        "    model_specific_checkpoint_name=ptb_base_checkpoint_name # Pass the specific Stage 1 checkpoint name\n",
        ")\n",
        "print(\"--- PTB Model Training (Stage 1) Finished ---\")\n",
        "\n",
        "# 2. Plot Training History for Stage 1\n",
        "print(\"\\n--- Plotting PTB Stage 1 Training History ---\")\n",
        "if ptb_stage1_history: # Check if history is not empty\n",
        "    ptb_stage1_metrics_to_plot = [\n",
        "        {'train_key': 'train_total_loss', 'val_key': 'val_total_loss', 'title': 'PTB Stage 1 Total Loss', 'ylabel': 'Loss'},\n",
        "        {'train_key': 'train_pred_loss', 'val_key': 'val_pred_loss', 'title': 'PTB Stage 1 Prediction Loss', 'ylabel': 'Pred Loss'},\n",
        "        {'train_key': 'train_defer_rate_train_time', 'val_key': 'val_defer_rate_train_time', 'title': 'PTB Stage 1 Defer Rate (During Train)', 'ylabel': 'Defer Rate'},\n",
        "        {'val_key': 'val_accuracy_nd_val_train_thresh', 'title': 'PTB Stage 1 Val Acc (Effectively Overall)', 'ylabel': 'Accuracy'}\n",
        "    ]\n",
        "    plot_training_history(\n",
        "        ptb_stage1_history,\n",
        "        ptb_stage1_metrics_to_plot,\n",
        "        save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"ptb_stage1_training_history.png\")\n",
        "    )\n",
        "else:\n",
        "    print(\"PTB Stage 1 training history is empty, skipping plot.\")\n",
        "\n",
        "# 3. Evaluate the Stage 1 Base Model on the Test Set\n",
        "print(\"\\n--- Evaluating PTB Stage 1 Base Model on Test Set (Overall Performance) ---\")\n",
        "ptb_stage1_eval_metrics = {} # Initialize\n",
        "# Ensure trained_ptb_base_model is used here (it's the output of the training function)\n",
        "if trained_ptb_base_model is not None and 'test' in ptb_loaders and ptb_loaders['test'] is not None:\n",
        "    trained_ptb_base_model.to(DEVICE)\n",
        "    trained_ptb_base_model.eval()\n",
        "\n",
        "    ptb_stage1_eval_metrics = evaluate_model_with_adaptive_deferral(\n",
        "        model=trained_ptb_base_model, # Use the model returned from training\n",
        "        test_loader=ptb_loaders['test'],\n",
        "        adaptive_threshold=float('inf'), # Ensures no deferral for overall accuracy assessment\n",
        "        device=DEVICE,\n",
        "        dataset_name=\"PTB Test Set (Stage 1 Base Model - Overall)\"\n",
        "    )\n",
        "    print(f\"PTB Stage 1 Base Model - Test Accuracy Overall: {ptb_stage1_eval_metrics.get('accuracy_overall', 'N/A'):.4f}\")\n",
        "    print(f\"PTB Stage 1 Base Model - Test AUC Overall: {ptb_stage1_eval_metrics.get('auc_non_deferred', 'N/A'):.4f}\")\n",
        "else:\n",
        "    print(\"Skipping Stage 1 PTB model evaluation (trained model or test loader not available).\")\n",
        "\n",
        "# 4. Visualizations for Stage 1 Overall Performance on Test Set\n",
        "print(\"\\n--- Visualizing PTB Stage 1 Test Set Overall Performance ---\")\n",
        "# Ensure trained_ptb_base_model is used here\n",
        "if trained_ptb_base_model is not None and 'test' in ptb_loaders and ptb_loaders['test'] is not None and ptb_stage1_eval_metrics:\n",
        "    all_y_true_s1_viz_ptb = []\n",
        "    all_y_pred_logits_s1_viz_ptb = []\n",
        "    trained_ptb_base_model.eval() # Model should already be in eval if loaded, but good practice\n",
        "    with torch.no_grad():\n",
        "        for X_b, y_b, p_mask_b in tqdm(ptb_loaders['test'], desc=\"Fetching PTB Test Data for Stage 1 Plots\"):\n",
        "            X_b, y_b, p_mask_b = X_b.to(DEVICE), y_b.to(DEVICE), p_mask_b.to(DEVICE)\n",
        "            output = trained_ptb_base_model(X_b, p_mask_b)\n",
        "            all_y_true_s1_viz_ptb.append(y_b.cpu())\n",
        "            all_y_pred_logits_s1_viz_ptb.append(output['y_pred_logits'].cpu())\n",
        "\n",
        "    if all_y_true_s1_viz_ptb: # Check if list is not empty\n",
        "        y_true_test_np_s1_ptb = torch.cat(all_y_true_s1_viz_ptb).numpy().astype(float)\n",
        "        y_pred_logits_test_np_s1_ptb = torch.cat(all_y_pred_logits_s1_viz_ptb).numpy()\n",
        "\n",
        "        y_pred_probs_test_s1_ptb = 1 / (1 + np.exp(-y_pred_logits_test_np_s1_ptb.squeeze()))\n",
        "        y_pred_classes_test_s1_ptb = (y_pred_probs_test_s1_ptb > 0.5).astype(int)\n",
        "\n",
        "        plot_confusion_matrix_custom(\n",
        "            y_true=y_true_test_np_s1_ptb.astype(int),\n",
        "            y_pred_classes=y_pred_classes_test_s1_ptb,\n",
        "            class_names=PTB_CLASS_NAMES,\n",
        "            title=\"PTB Stage 1 Base Model CM (Overall Test)\",\n",
        "            save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"ptb_stage1_cm_overall_test.png\")\n",
        "        )\n",
        "        plot_roc_auc_curves(\n",
        "            y_true_list=[y_true_test_np_s1_ptb],\n",
        "            y_pred_probs_list=[y_pred_probs_test_s1_ptb],\n",
        "            label_names_list=[\"PTB Stage 1 Base Model (Overall)\"],\n",
        "            output_dim=PTB_NUM_CLASSES,\n",
        "            class_names=PTB_CLASS_NAMES,\n",
        "            title=\"PTB Stage 1 Base Model ROC Curve (Overall Test)\",\n",
        "            save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"ptb_stage1_roc_overall_test.png\")\n",
        "        )\n",
        "    else:\n",
        "        print(\"Skipping PTB Stage 1 test visualizations as test data could not be re-fetched or was empty.\")\n",
        "else:\n",
        "    print(\"Skipping PTB Stage 1 visualizations (trained base model or test loader not available, or prior evaluation failed).\")\n",
        "\n",
        "# 5.  Explanations for the Stage 1 Base Model\n",
        "captum_ready_s1_ptb = 'Saliency' in globals() and Saliency is not None and 'IntegratedGradients' in globals() and IntegratedGradients is not None\n",
        "explainer_class_ready_s1_ptb = 'CGDExplainer' in globals() and callable(CGDExplainer)\n",
        "\n",
        "if captum_ready_s1_ptb and explainer_class_ready_s1_ptb and trained_ptb_base_model is not None and \\\n",
        "   'ptb_loaders' in globals() and ptb_loaders['test'] is not None:\n",
        "    print(\"\\n--- (Optional) Visualizing Explanations for PTB Stage 1 Base Model ---\")\n",
        "    ptb_base_explainer = CGDExplainer(trained_ptb_base_model, EXPLAINER_CONFIG)\n",
        "    num_samples_to_explain_s1 = 2\n",
        "    explained_count_s1 = 0\n",
        "    try:\n",
        "        for X_ex_b, y_ex_b, p_mask_ex_b in ptb_loaders['test']:\n",
        "            if explained_count_s1 >= num_samples_to_explain_s1: break\n",
        "            samples_to_take = min(X_ex_b.size(0), num_samples_to_explain_s1 - explained_count_s1)\n",
        "\n",
        "            for i_s1 in range(samples_to_take):\n",
        "                if explained_count_s1 >= num_samples_to_explain_s1: break\n",
        "                X_s1, y_s1_true, p_mask_s1 = X_ex_b[i_s1:i_s1+1].to(DEVICE), y_ex_b[i_s1].to(DEVICE), p_mask_ex_b[i_s1:i_s1+1].to(DEVICE)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output_s1 = trained_ptb_base_model(X_s1, p_mask_s1)\n",
        "                pred_logit_s1 = output_s1['y_pred_logits'].item()\n",
        "                pred_prob_s1 = torch.sigmoid(torch.tensor(pred_logit_s1)).item()\n",
        "                pred_class_s1 = 1 if pred_prob_s1 > 0.5 else 0\n",
        "\n",
        "                true_lbl_name_s1 = PTB_CLASS_NAMES[int(y_s1_true.item())]\n",
        "                pred_lbl_name_s1 = PTB_CLASS_NAMES[pred_class_s1]\n",
        "\n",
        "                print(f\"Explaining PTB Stage 1 Sample {explained_count_s1+1}: True='{true_lbl_name_s1}', Pred='{pred_lbl_name_s1}' (Prob={pred_prob_s1:.2f})\")\n",
        "                attrs_pred_s1 = ptb_base_explainer.attribute_input(\n",
        "                    X_s1, p_mask_s1, explanation_target='prediction', target_class_idx=0, abs_attribution=True\n",
        "                )\n",
        "                if attrs_pred_s1 is not None:\n",
        "                    plot_ecg_with_saliency(\n",
        "                        ecg_signal=X_s1.squeeze().cpu().numpy(), attributions=attrs_pred_s1.squeeze(),\n",
        "                        title=f\"PTB Stage 1 Sample {explained_count_s1+1} - Prediction Explanation\",\n",
        "                        save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], f\"ptb_stage1_explain_pred_sample{explained_count_s1}.png\"),\n",
        "                        true_label_name=true_lbl_name_s1, pred_label_name=pred_lbl_name_s1\n",
        "                    )\n",
        "                explained_count_s1 += 1\n",
        "            if explained_count_s1 >= num_samples_to_explain_s1: break\n",
        "    except Exception as e_s1_explain:\n",
        "        print(f\"Error during PTB Stage 1 explanation: {e_s1_explain}\")\n",
        "else:\n",
        "    print(\"\\nSkipping PTB Stage 1 explanation visualization (components missing).\")\n",
        "\n",
        "# --- Final Check and Message for Stage 1 PTB Model ---\n",
        "# ptb_base_checkpoint_name was defined in section 1 of this cell\n",
        "final_ptb_base_model_path = os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], ptb_base_checkpoint_name)\n",
        "if trained_ptb_base_model is not None and os.path.exists(final_ptb_base_model_path):\n",
        "    print(f\"\\n--- PTB Stage 1 Analysis Complete. Best base model successfully saved to {final_ptb_base_model_path} ---\")\n",
        "    print(f\"This PTB base model achieved an overall test accuracy of: {ptb_stage1_eval_metrics.get('accuracy_overall', 'N/A'):.4f}\")\n",
        "else:\n",
        "    print(f\"\\n--- PTB Stage 1 Analysis Incomplete. ---\")\n",
        "    if trained_ptb_base_model is None: # Check if the model variable itself is None (e.g. if training call failed)\n",
        "         print(\"   Reason: Base model training might not have completed or failed (trained_ptb_base_model is None).\")\n",
        "    elif not os.path.exists(final_ptb_base_model_path): # Check if the file wasn't saved\n",
        "         print(f\"   Reason: Best base model checkpoint NOT FOUND at {final_ptb_base_model_path}\")\n",
        "         print(\"   Please check training logs from 'train_universal_cgd_model' to ensure the model improved enough to be saved by early stopping.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZbkVLCZxoSk"
      },
      "outputs": [],
      "source": [
        "# --- Cell 18.A: Define Deferral Predictor Head ---\n",
        "\n",
        "class DeferralPredictorHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A separate MLP to predict a deferral score.\n",
        "    Takes features (e.g., latent representation z_original from a base model) as input.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_feature_dim: int, config: Dict[str, Any]):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # If DEFERRAL_HEAD_CONFIG doesn't exist, use some defaults or PREDICTOR_CONFIG as a template\n",
        "        if 'DEFERRAL_HEAD_CONFIG' in globals():\n",
        "            dh_config = DEFERRAL_HEAD_CONFIG\n",
        "        else: # Fallback to using PREDICTOR_CONFIG structure if DEFERRAL_HEAD_CONFIG not set\n",
        "            print(\"Warning: DEFERRAL_HEAD_CONFIG not found in globals. Using PREDICTOR_CONFIG for DeferralPredictorHead structure.\")\n",
        "            dh_config = PREDICTOR_CONFIG\n",
        "\n",
        "        hidden_dims = dh_config.get('hidden_dims_deferral_head',\n",
        "                                    dh_config.get('hidden_dims', # Fallback to general hidden_dims\n",
        "                                                  [max(input_feature_dim // 2, 32), max(input_feature_dim // 4, 16)]))\n",
        "        dropout_rate = dh_config.get('dropout_deferral_head', dh_config.get('dropout', 0.1))\n",
        "\n",
        "        layers = []\n",
        "        current_dim = input_feature_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            if h_dim <= 0: h_dim = max(16, current_dim //2) # Ensure h_dim is positive and reasonable\n",
        "            layers.append(nn.Linear(current_dim, h_dim))\n",
        "            layers.append(nn.BatchNorm1d(h_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            current_dim = h_dim\n",
        "\n",
        "        layers.append(nn.Linear(current_dim, 1)) # Single output logit for deferral\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "        print(f\"DeferralPredictorHead initialized with input_dim={input_feature_dim}, hidden_dims={hidden_dims}\")\n",
        "\n",
        "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: Input features for deferral prediction [batch_size, input_feature_dim]\n",
        "                      (e.g., z_original from the base model's encoder)\n",
        "        Returns:\n",
        "            deferral_logits: Logits for deferral [batch_size, 1]\n",
        "        \"\"\"\n",
        "        return self.mlp(features)\n",
        "\n",
        "# Example: Add DEFERRAL_HEAD_CONFIG to Cell 2 if you haven't\n",
        "# DEFERRAL_HEAD_CONFIG = {\n",
        "print(\"\\nCell 18.A: DeferralPredictorHead class defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcjWIQYDx2Ss"
      },
      "outputs": [],
      "source": [
        "# --- Cell 18.B: Prepare Data for Deferral Head Training ---\n",
        "\n",
        "def create_deferral_head_dataset_and_loader(\n",
        "    base_model: UniversalCGDModel,      # Trained and frozen base model from Stage 1\n",
        "    original_dataloader: DataLoader,    # e.g., mitbih_loaders['train'] or mitbih_loaders['val']\n",
        "    device: torch.device,\n",
        "    target_type: str = 'error_prediction', # 'error_prediction' or 'sensitivity_regression'\n",
        "    # For 'sensitivity_regression', you might need these:\n",
        "    # strong_perturb_config: Optional[Dict] = None,\n",
        "    # strong_sensitivity_config: Optional[Dict] = None\n",
        "    batch_size: int = 128, # Use batch_size from DEFERRAL_HEAD_CONFIG if available\n",
        "    num_workers: int = 2\n",
        ") -> Optional[DataLoader]:\n",
        "    \"\"\"\n",
        "    Generates a new dataset and DataLoader for training the DeferralPredictorHead.\n",
        "    The dataset consists of (features_for_head, deferral_targets).\n",
        "    Features are typically z_original from the base_model.\n",
        "    Targets are '1' if base_model made an error, '0' otherwise (for error_prediction).\n",
        "    \"\"\"\n",
        "    base_model.eval() # Ensure base model is in eval mode\n",
        "    base_model.to(device)\n",
        "\n",
        "    all_features_for_head = []\n",
        "    all_deferral_targets = []\n",
        "\n",
        "    print(f\"Creating dataset for DeferralPredictorHead using target_type='{target_type}'...\")\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_true_batch, padding_mask_batch in tqdm(original_dataloader, desc=\"Generating Deferral Head Data\"):\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_true_batch = y_true_batch.to(device)\n",
        "            padding_mask_batch = padding_mask_batch.to(device)\n",
        "\n",
        "            # Get z_original and base model predictions\n",
        "            base_model_output = base_model(X_batch, padding_mask_batch=padding_mask_batch)\n",
        "            z_original = base_model_output['z_original'].detach().cpu() # [B, LatentDim]\n",
        "            y_pred_logits_base = base_model_output['y_pred_logits'].detach().cpu() # [B, NumClasses]\n",
        "\n",
        "            all_features_for_head.append(z_original)\n",
        "\n",
        "            if target_type == 'error_prediction':\n",
        "                if base_model.output_dim_val == 1: # Binary\n",
        "                    preds_base_classes = (y_pred_logits_base.sigmoid() > 0.5).float().squeeze()\n",
        "                    errors = (preds_base_classes != y_true_batch.cpu().float()).float()\n",
        "                else: # Multiclass\n",
        "                    preds_base_classes = torch.argmax(y_pred_logits_base, dim=1)\n",
        "                    errors = (preds_base_classes != y_true_batch.cpu().long()).float()\n",
        "                all_deferral_targets.append(errors)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported target_type for DeferralPredictorHead: {target_type}\")\n",
        "\n",
        "    if not all_features_for_head:\n",
        "        print(\"No data generated for deferral head.\")\n",
        "        return None\n",
        "\n",
        "    cat_features = torch.cat(all_features_for_head, dim=0)\n",
        "    cat_targets = torch.cat(all_deferral_targets, dim=0)\n",
        "\n",
        "    # Ensure targets are correctly shaped for BCEWithLogitsLoss if it's error prediction\n",
        "    if target_type == 'error_prediction':\n",
        "        cat_targets = cat_targets.unsqueeze(1) # [N, 1]\n",
        "\n",
        "    deferral_head_dataset = TensorDataset(cat_features, cat_targets) # Simpler Dataset for this\n",
        "    deferral_head_loader = DataLoader(deferral_head_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    print(f\"Deferral Head Dataset created: {len(deferral_head_dataset)} samples. Target '1' proportion (errors/high_sens): {cat_targets.mean().item():.3f}\")\n",
        "    return deferral_head_loader\n",
        "\n",
        "\n",
        "# Define TensorDataset if not already available from torch.utils.data\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "print(\"\\nCell 18.B: Utilities for Deferral Head Data Preparation defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjKFy7ZS4iVo"
      },
      "outputs": [],
      "source": [
        "# --- Cell 18.C: Training Loop for the Deferral Head ---\n",
        "\n",
        "def train_deferral_head_epoch_fn( # Renamed to avoid conflict with any previous train_epoch\n",
        "    deferral_head: DeferralPredictorHead,\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        "    epoch_num: int,\n",
        "    total_epochs: int\n",
        ") -> float:\n",
        "    \"\"\"Trains the DeferralPredictorHead for one epoch.\"\"\"\n",
        "    deferral_head.train()\n",
        "    running_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Deferral Head Train Epoch {epoch_num+1}/{total_epochs}\")\n",
        "    for features, targets in pbar:\n",
        "        features, targets = features.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = deferral_head(features) # deferral_logits [B, 1]\n",
        "        loss = criterion(logits, targets) # targets should be [B, 1] and float for BCEWithLogitsLoss\n",
        "\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(f\"Warning: NaN or Inf loss detected in deferral head training epoch {epoch_num+1}. Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * features.size(0)\n",
        "        total_samples += features.size(0)\n",
        "        pbar.set_postfix({'loss': f\"{running_loss/total_samples:.4f}\"})\n",
        "\n",
        "    return running_loss / total_samples if total_samples > 0 else 0.0\n",
        "\n",
        "def validate_deferral_head_epoch_fn( # Renamed\n",
        "    deferral_head: DeferralPredictorHead,\n",
        "    dataloader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        "    epoch_num: int,\n",
        "    total_epochs: int\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Validates the DeferralPredictorHead for one epoch.\"\"\"\n",
        "    deferral_head.eval()\n",
        "    running_loss = 0.0\n",
        "    total_samples = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Deferral Head Val Epoch {epoch_num+1}/{total_epochs}\")\n",
        "    with torch.no_grad():\n",
        "        for features, targets in pbar:\n",
        "            features, targets = features.to(device), targets.to(device)\n",
        "            logits = deferral_head(features)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "                running_loss += loss.item() * features.size(0)\n",
        "                total_samples += features.size(0)\n",
        "\n",
        "                # Store predictions and targets for accuracy calculation\n",
        "                # Sigmoid + threshold for binary classification (error vs not error)\n",
        "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "                all_preds.append(preds.cpu())\n",
        "                all_targets.append(targets.cpu())\n",
        "\n",
        "            pbar.set_postfix({'val_loss': f\"{running_loss/total_samples:.4f}\" if total_samples > 0 else \"N/A\"})\n",
        "\n",
        "    avg_loss = running_loss / total_samples if total_samples > 0 else float('inf')\n",
        "\n",
        "    accuracy = 0.0\n",
        "    if all_preds and all_targets:\n",
        "        cat_preds = torch.cat(all_preds)\n",
        "        cat_targets = torch.cat(all_targets)\n",
        "        if cat_targets.numel() > 0: # Ensure there are samples\n",
        "             accuracy = accuracy_score(cat_targets.numpy(), cat_preds.numpy())\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def train_separate_deferral_head_model(\n",
        "    base_model_frozen: UniversalCGDModel, # Trained and FROZEN base model\n",
        "    deferral_head_input_dim: int,         # Typically base_model.latent_dim\n",
        "    # DataLoaders for the *original* task, used to generate deferral head training data\n",
        "    original_train_loader_for_deferral_data: DataLoader,\n",
        "    original_val_loader_for_deferral_data: DataLoader,\n",
        "    # Configs\n",
        "    deferral_head_config: Dict[str, Any], # DEFERRAL_HEAD_CONFIG from Cell 2\n",
        "    general_training_config: Dict[str, Any], # For checkpoint_dir\n",
        "    device: torch.device,\n",
        "    deferral_head_checkpoint_name: str = \"deferral_head_best.pt\"\n",
        ") -> Tuple[Optional[DeferralPredictorHead], Dict[str, List[float]]]:\n",
        "    \"\"\"\n",
        "    Trains a separate DeferralPredictorHead using features from a frozen base model.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting DeferralPredictorHead Training for {deferral_head_checkpoint_name} ---\")\n",
        "\n",
        "    # Ensure base model is frozen\n",
        "    for param in base_model_frozen.parameters():\n",
        "        param.requires_grad = False\n",
        "    base_model_frozen.eval()\n",
        "\n",
        "    # 1. Prepare data for the Deferral Head\n",
        "    # Use a portion of the original training data to create training data for the deferral head\n",
        "    # And use the original validation data to create validation data for the deferral head\n",
        "    print(\"Generating training data for Deferral Head...\")\n",
        "    dh_train_loader = create_deferral_head_dataset_and_loader(\n",
        "        base_model=base_model_frozen,\n",
        "        original_dataloader=original_train_loader_for_deferral_data,\n",
        "        device=device,\n",
        "        target_type=deferral_head_config.get('target_type', 'error_prediction'),\n",
        "        batch_size=deferral_head_config.get('batch_size', 128)\n",
        "    )\n",
        "    print(\"Generating validation data for Deferral Head...\")\n",
        "    dh_val_loader = create_deferral_head_dataset_and_loader(\n",
        "        base_model=base_model_frozen,\n",
        "        original_dataloader=original_val_loader_for_deferral_data,\n",
        "        device=device,\n",
        "        target_type=deferral_head_config.get('target_type', 'error_prediction'),\n",
        "        batch_size=deferral_head_config.get('batch_size', 128)\n",
        "    )\n",
        "\n",
        "    if dh_train_loader is None or dh_val_loader is None:\n",
        "        print(\"Could not create DataLoaders for Deferral Head. Aborting training.\")\n",
        "        return None, {}\n",
        "\n",
        "    # 2. Initialize Deferral Head and Optimizer\n",
        "    deferral_head = DeferralPredictorHead(\n",
        "        input_feature_dim=deferral_head_input_dim,\n",
        "        config=deferral_head_config\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(deferral_head.parameters(), lr=deferral_head_config.get('learning_rate', 1e-3))\n",
        "\n",
        "    # For error_prediction (binary 0/1 targets), BCEWithLogitsLoss is appropriate\n",
        "    # For sensitivity_regression, MSELoss would be used.\n",
        "    if deferral_head_config.get('target_type', 'error_prediction') == 'error_prediction':\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "    # elif deferral_head_config.get('target_type') == 'sensitivity_regression':\n",
        "    #     criterion = nn.MSELoss()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported target_type in DEFERRAL_HEAD_CONFIG for loss criterion.\")\n",
        "\n",
        "    epochs = deferral_head_config.get('epochs', 10)\n",
        "    patience = deferral_head_config.get('patience', 3)\n",
        "    checkpoint_path = os.path.join(general_training_config['checkpoint_dir'], deferral_head_checkpoint_name)\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []} # Accuracy of predicting base model's errors\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    print(f\"Training DeferralPredictorHead for up to {epochs} epochs...\")\n",
        "    print(f\"Best DeferralPredictorHead will be saved to: {checkpoint_path}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_deferral_head_epoch_fn(deferral_head, dh_train_loader, optimizer, criterion, device, epoch, epochs)\n",
        "        val_loss, val_accuracy = validate_deferral_head_epoch_fn(deferral_head, dh_val_loader, criterion, device, epoch, epochs)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "        print(f\"Deferral Head Epoch {epoch+1}/{epochs} Summary: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc(ErrorPred)={val_accuracy:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(deferral_head.state_dict(), checkpoint_path)\n",
        "            print(f\"  -> New best val_loss for Deferral Head: {best_val_loss:.4f}. Checkpoint saved.\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  -> Val_loss for Deferral Head did not improve for {epochs_no_improve} epoch(s). Best: {best_val_loss:.4f}\")\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping for Deferral Head triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "    print(\"DeferralPredictorHead training finished.\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading best DeferralPredictorHead state from {checkpoint_path}\")\n",
        "        deferral_head.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    else:\n",
        "        print(\"Warning: No best DeferralPredictorHead checkpoint found. Using head from last training state.\")\n",
        "\n",
        "    return deferral_head, history\n",
        "\n",
        "# DEFERRAL_HEAD_CONFIG\n",
        "if 'DEFERRAL_HEAD_CONFIG' not in globals() :\n",
        "    DEFERRAL_HEAD_CONFIG = {\n",
        "        'hidden_dims_deferral_head': [CGD_MODEL_CONFIG['latent_dim'] // 2, CGD_MODEL_CONFIG['latent_dim'] // 4], # e.g. [32, 16]\n",
        "        'dropout_deferral_head': 0.2,\n",
        "        'learning_rate': 1e-3,\n",
        "        'epochs': 20,\n",
        "        'batch_size': 256,\n",
        "        'patience': 5,\n",
        "        'target_type': 'error_prediction'\n",
        "    }\n",
        "    print(\"Using default DEFERRAL_HEAD_CONFIG for testing Cell 18.C\")\n",
        "\n",
        "\n",
        "# --- Conceptual Test (Actual run happens in dataset-specific cells later) ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Testing Deferral Head Training Loop (Conceptual) ---\")\n",
        "    # This requires a fully trained and frozen base_model and original dataloaders.\n",
        "    # For now, just defining the functions.\n",
        "    if 'cgd_model_mitbih' in globals() and cgd_model_mitbih is not None and \\\n",
        "       'mitbih_loaders' in globals() and mitbih_loaders is not None:\n",
        "        print(\"Conceptual test: Would train a separate deferral head for MIT-BIH.\")\n",
        "        print(\"Deferral Head training loop functions defined.\")\n",
        "    else:\n",
        "        print(\"Skipping conceptual deferral head training test as base model/loaders not fully set up.\")\n",
        "\n",
        "print(\"\\nCell 18.C: Training Loop for Deferral Head defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KvMgHNl4iU5"
      },
      "outputs": [],
      "source": [
        "# --- Cell 18.D: Evaluation of Full System (Frozen Base Model + Trained Deferral Head) ---\n",
        "\n",
        "def compute_adaptive_threshold_for_deferral_head(\n",
        "    base_model_frozen: UniversalCGDModel,\n",
        "    deferral_head: DeferralPredictorHead,\n",
        "    original_val_loader: DataLoader, # e.g., mitbih_loaders['val']\n",
        "    adaptive_thresh_config: Dict[str, Any],\n",
        "    device: torch.device\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes an adaptive deferral threshold based on the DeferralPredictorHead's scores\n",
        "    on the original validation set.\n",
        "    \"\"\"\n",
        "    base_model_frozen.eval()\n",
        "    deferral_head.eval()\n",
        "\n",
        "    all_deferral_head_scores = []\n",
        "    all_y_true_val = []\n",
        "    all_y_pred_logits_base_val = [] # Base model's predictions\n",
        "\n",
        "    print(\"Computing adaptive threshold for Deferral Head using validation set...\")\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_true_batch, padding_mask_batch in tqdm(original_val_loader, desc=\"DH Adaptive Threshold Calc\"):\n",
        "            X_batch, y_true_batch, padding_mask_batch = X_batch.to(device), y_true_batch.to(device), padding_mask_batch.to(device)\n",
        "\n",
        "            # Get features for deferral head (z_original from base model)\n",
        "            base_model_output = base_model_frozen(X_batch, padding_mask_batch=padding_mask_batch)\n",
        "            z_original = base_model_output['z_original']\n",
        "            y_pred_logits_base = base_model_output['y_pred_logits']\n",
        "\n",
        "            # Get deferral head's score\n",
        "            deferral_logits_head = deferral_head(z_original)\n",
        "            deferral_scores_head = torch.sigmoid(deferral_logits_head).squeeze(-1) # Squeeze last dim if it's [B,1]\n",
        "\n",
        "            all_deferral_head_scores.append(deferral_scores_head.cpu())\n",
        "            all_y_true_val.append(y_true_batch.cpu())\n",
        "            all_y_pred_logits_base_val.append(y_pred_logits_base.cpu())\n",
        "\n",
        "    if not all_deferral_head_scores:\n",
        "        print(\"Warning: No deferral head scores collected. Returning default threshold 0.5.\")\n",
        "        return 0.5\n",
        "\n",
        "    deferral_head_scores_np = torch.cat(all_deferral_head_scores).numpy()\n",
        "    y_true_np = torch.cat(all_y_true_val).numpy()\n",
        "    y_pred_logits_base_np = torch.cat(all_y_pred_logits_base_val).numpy()\n",
        "\n",
        "    if deferral_head_scores_np.size == 0:\n",
        "        print(\"Warning: Deferral head scores array is empty. Returning default 0.5.\")\n",
        "        return 0.5\n",
        "\n",
        "    method = adaptive_thresh_config.get('method', 'percentile')\n",
        "    num_candidates = adaptive_thresh_config.get('num_threshold_candidates', 200)\n",
        "\n",
        "    candidate_thresholds = np.unique(deferral_head_scores_np) # Use unique scores from deferral head\n",
        "    if len(candidate_thresholds) > num_candidates:\n",
        "        candidate_thresholds = np.percentile(deferral_head_scores_np, np.linspace(0, 100, num_candidates))\n",
        "    elif len(candidate_thresholds) == 0:\n",
        "         print(\"Warning: No unique deferral head scores found. Returning default 0.5\")\n",
        "         return 0.5\n",
        "\n",
        "    optimal_threshold = 0.5 # Default\n",
        "\n",
        "    if method == 'percentile':\n",
        "        percentile_val = adaptive_thresh_config.get('percentile_value_for_threshold', 90)\n",
        "        # Threshold is the score at this percentile. Defer if score > threshold.\n",
        "        optimal_threshold = np.percentile(deferral_head_scores_np, percentile_val)\n",
        "        print(f\"Deferral Head adaptive threshold (percentile {percentile_val}%): {optimal_threshold:.4f}\")\n",
        "\n",
        "    elif method == 'target_defer_rate':\n",
        "        target_dr = adaptive_thresh_config.get('target_defer_rate_value', 0.10)\n",
        "        best_thresh_for_target_dr = candidate_thresholds[0]\n",
        "        min_dr_diff = float('inf')\n",
        "        for thresh in candidate_thresholds:\n",
        "            deferred_mask = deferral_head_scores_np > thresh # Using deferral head scores\n",
        "            current_dr = np.mean(deferred_mask)\n",
        "            dr_diff = abs(current_dr - target_dr)\n",
        "            if dr_diff < min_dr_diff:\n",
        "                min_dr_diff = dr_diff\n",
        "                best_thresh_for_target_dr = thresh\n",
        "            elif dr_diff == min_dr_diff and current_dr < target_dr:\n",
        "                best_thresh_for_target_dr = thresh\n",
        "        optimal_threshold = best_thresh_for_target_dr\n",
        "        final_dr = np.mean(deferral_head_scores_np > optimal_threshold)\n",
        "        print(f\"Deferral Head adaptive threshold (target DR ~{target_dr*100:.1f}%): {optimal_threshold:.4f} (results in actual DR: {final_dr*100:.1f}%)\")\n",
        "\n",
        "    elif method == 'max_acc_under_budget':\n",
        "        max_budget_dr = adaptive_thresh_config.get('max_defer_rate_budget', 0.20)\n",
        "        best_thresh_for_acc = candidate_thresholds[-1]\n",
        "        max_acc_nd = -1.0\n",
        "\n",
        "        # Base model's predictions on the validation set\n",
        "        if base_model_frozen.output_dim_val == 1: # Binary\n",
        "            base_preds_classes_val = (torch.sigmoid(torch.from_numpy(y_pred_logits_base_np)).squeeze().numpy() > 0.5).astype(int)\n",
        "            base_true_labels_val = y_true_np.astype(float)\n",
        "        else: # Multiclass\n",
        "            base_preds_classes_val = np.argmax(y_pred_logits_base_np, axis=1)\n",
        "            base_true_labels_val = y_true_np.astype(int)\n",
        "\n",
        "        for thresh in candidate_thresholds:\n",
        "            deferred_mask = deferral_head_scores_np > thresh # Using deferral head scores\n",
        "            current_dr = np.mean(deferred_mask)\n",
        "            if current_dr <= max_budget_dr:\n",
        "                non_deferred_mask = ~deferred_mask\n",
        "                if np.sum(non_deferred_mask) > 0:\n",
        "                    acc_nd = accuracy_score(base_true_labels_val[non_deferred_mask], base_preds_classes_val[non_deferred_mask])\n",
        "                    if acc_nd > max_acc_nd:\n",
        "                        max_acc_nd = acc_nd\n",
        "                        best_thresh_for_acc = thresh\n",
        "                    elif acc_nd == max_acc_nd and thresh > best_thresh_for_acc:\n",
        "                         best_thresh_for_acc = thresh\n",
        "        optimal_threshold = best_thresh_for_acc\n",
        "        final_dr = np.mean(deferral_head_scores_np > optimal_threshold)\n",
        "        final_acc_nd = -1.0\n",
        "        if np.sum(~(deferral_head_scores_np > optimal_threshold)) > 0:\n",
        "            final_acc_nd = accuracy_score(base_true_labels_val[~(deferral_head_scores_np > optimal_threshold)], base_preds_classes_val[~(deferral_head_scores_np > optimal_threshold)])\n",
        "        print(f\"Deferral Head adaptive threshold (max Acc_ND under {max_budget_dr*100:.1f}% DR): {optimal_threshold:.4f}\")\n",
        "        print(f\"  Results in Val: Actual DR={final_dr*100:.1f}%, Base Model Acc_ND={final_acc_nd*100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Warning: Unknown adaptive threshold method '{method}'. Using default 0.5.\")\n",
        "        optimal_threshold = 0.5\n",
        "\n",
        "    return float(optimal_threshold)\n",
        "\n",
        "\n",
        "def evaluate_system_with_deferral_head(\n",
        "    base_model_frozen: UniversalCGDModel,\n",
        "    deferral_head: DeferralPredictorHead,\n",
        "    test_loader: DataLoader,\n",
        "    adaptive_threshold_for_head: float,\n",
        "    device: torch.device,\n",
        "    dataset_name: str = \"Test Set\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluates the combined system: frozen base model + trained deferral head.\n",
        "    \"\"\"\n",
        "    base_model_frozen.eval()\n",
        "    deferral_head.eval()\n",
        "\n",
        "    all_y_true_test = []\n",
        "    all_y_pred_logits_base_test = [] # From the frozen base model\n",
        "    all_deferral_head_scores_test = []\n",
        "\n",
        "    print(f\"\\nEvaluating system on {dataset_name} with Deferral Head threshold: {adaptive_threshold_for_head:.4f}\")\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_true_batch, padding_mask_batch in tqdm(test_loader, desc=f\"Evaluating System on {dataset_name}\"):\n",
        "            X_batch, y_true_batch, padding_mask_batch = X_batch.to(device), y_true_batch.to(device), padding_mask_batch.to(device)\n",
        "\n",
        "            # Get features for deferral head (z_original from base model) and base predictions\n",
        "            base_model_output = base_model_frozen(X_batch, padding_mask_batch=padding_mask_batch)\n",
        "            z_original = base_model_output['z_original']\n",
        "            y_pred_logits_base = base_model_output['y_pred_logits']\n",
        "\n",
        "            # Get deferral head's score\n",
        "            deferral_logits_head = deferral_head(z_original) # Pass appropriate features\n",
        "            deferral_scores_head = torch.sigmoid(deferral_logits_head).squeeze(-1)\n",
        "\n",
        "            all_y_true_test.append(y_true_batch.cpu())\n",
        "            all_y_pred_logits_base_test.append(y_pred_logits_base.cpu())\n",
        "            all_deferral_head_scores_test.append(deferral_scores_head.cpu())\n",
        "\n",
        "    if not all_y_true_test:\n",
        "        print(f\"Warning: No data processed during system evaluation for {dataset_name}. Returning empty metrics.\")\n",
        "        return {}\n",
        "\n",
        "    y_true_np = torch.cat(all_y_true_test).numpy()\n",
        "    y_pred_logits_base_np = torch.cat(all_y_pred_logits_base_test).numpy()\n",
        "    deferral_head_scores_np = torch.cat(all_deferral_head_scores_test).numpy()\n",
        "\n",
        "    # --- Determine Base Model Predictions and Errors (Overall) ---\n",
        "    is_binary_classification = base_model_frozen.output_dim_val == 1\n",
        "    if is_binary_classification:\n",
        "        y_pred_probs_base_overall = 1 / (1 + np.exp(-y_pred_logits_base_np.squeeze())) # Sigmoid\n",
        "        y_pred_classes_base_overall = (y_pred_probs_base_overall > 0.5).astype(int)\n",
        "        y_true_labels_overall = y_true_np.astype(float)\n",
        "    else: # Multiclass\n",
        "        y_pred_probs_base_overall = F.softmax(torch.from_numpy(y_pred_logits_base_np), dim=1).numpy()\n",
        "        y_pred_classes_base_overall = np.argmax(y_pred_logits_base_np, axis=1)\n",
        "        y_true_labels_overall = y_true_np.astype(int)\n",
        "\n",
        "    errors_base_overall = (y_pred_classes_base_overall != y_true_labels_overall).astype(int)\n",
        "    accuracy_base_overall = accuracy_score(y_true_labels_overall, y_pred_classes_base_overall)\n",
        "\n",
        "    # --- Apply Deferral using Deferral Head scores and its adaptive threshold ---\n",
        "    defer_mask_eval_head = deferral_head_scores_np > adaptive_threshold_for_head\n",
        "    defer_rate_eval_head = np.mean(defer_mask_eval_head)\n",
        "    non_deferred_mask_eval_head = ~defer_mask_eval_head\n",
        "\n",
        "    metrics = {\n",
        "        'dataset_name': dataset_name,\n",
        "        'adaptive_deferral_threshold_head': adaptive_threshold_for_head,\n",
        "        'accuracy_base_overall': accuracy_base_overall, # Base model's performance if no deferral\n",
        "        'defer_rate_eval_head': defer_rate_eval_head,\n",
        "        'total_samples': len(y_true_np),\n",
        "        'num_deferred_eval_head': np.sum(defer_mask_eval_head),\n",
        "        'num_non_deferred_eval_head': np.sum(non_deferred_mask_eval_head),\n",
        "    }\n",
        "\n",
        "    # --- Metrics for Non-Deferred Samples (using Base Model's predictions) ---\n",
        "    if np.sum(non_deferred_mask_eval_head) > 0:\n",
        "        y_true_nd = y_true_labels_overall[non_deferred_mask_eval_head]\n",
        "        y_pred_classes_base_nd = y_pred_classes_base_overall[non_deferred_mask_eval_head]\n",
        "        y_pred_probs_base_nd = y_pred_probs_base_overall[non_deferred_mask_eval_head] # For AUC\n",
        "\n",
        "        metrics['accuracy_non_deferred_system'] = accuracy_score(y_true_nd, y_pred_classes_base_nd)\n",
        "        if is_binary_classification:\n",
        "            metrics['precision_non_deferred_system'] = precision_score(y_true_nd, y_pred_classes_base_nd, zero_division=0)\n",
        "            metrics['recall_non_deferred_system'] = recall_score(y_true_nd, y_pred_classes_base_nd, zero_division=0)\n",
        "            metrics['f1_score_non_deferred_system'] = f1_score(y_true_nd, y_pred_classes_base_nd, zero_division=0)\n",
        "            if len(np.unique(y_true_nd)) > 1:\n",
        "                 metrics['auc_non_deferred_system'] = roc_auc_score(y_true_nd, y_pred_probs_base_nd)\n",
        "            else:\n",
        "                 metrics['auc_non_deferred_system'] = float('nan')\n",
        "        else: # Multiclass\n",
        "            metrics['precision_non_deferred_system'] = precision_score(y_true_nd, y_pred_classes_base_nd, average='weighted', zero_division=0)\n",
        "            metrics['recall_non_deferred_system'] = recall_score(y_true_nd, y_pred_classes_base_nd, average='weighted', zero_division=0)\n",
        "            metrics['f1_score_non_deferred_system'] = f1_score(y_true_nd, y_pred_classes_base_nd, average='weighted', zero_division=0)\n",
        "            if len(np.unique(y_true_nd)) >= base_model_frozen.output_dim_val and base_model_frozen.output_dim_val > 1:\n",
        "                try:\n",
        "                    metrics['auc_non_deferred_system'] = roc_auc_score(y_true_nd, y_pred_probs_base_nd, multi_class='ovr', average='macro')\n",
        "                except ValueError as e_auc:\n",
        "                    print(f\"Warning: Could not compute multiclass AUC for non-deferred (system): {e_auc}\")\n",
        "                    metrics['auc_non_deferred_system'] = float('nan')\n",
        "            else:\n",
        "                metrics['auc_non_deferred_system'] = float('nan')\n",
        "    else:\n",
        "        metrics.update({\n",
        "            'accuracy_non_deferred_system': float('nan'), 'precision_non_deferred_system': float('nan'),\n",
        "            'recall_non_deferred_system': float('nan'), 'f1_score_non_deferred_system': float('nan'),\n",
        "            'auc_non_deferred_system': float('nan')\n",
        "        })\n",
        "\n",
        "    # --- Metrics for Deferred Samples (what the Base Model would have predicted) ---\n",
        "    if np.sum(defer_mask_eval_head) > 0:\n",
        "        y_true_d = y_true_labels_overall[defer_mask_eval_head]\n",
        "        y_pred_classes_base_d = y_pred_classes_base_overall[defer_mask_eval_head]\n",
        "        metrics['accuracy_base_on_deferred_samples'] = accuracy_score(y_true_d, y_pred_classes_base_d)\n",
        "    else:\n",
        "        metrics['accuracy_base_on_deferred_samples'] = float('nan')\n",
        "\n",
        "    # --- Correlation between Deferral Head Score and Base Model Error ---\n",
        "    if len(deferral_head_scores_np) > 1 and len(errors_base_overall) > 1 and \\\n",
        "       np.std(deferral_head_scores_np) > 0 and np.std(errors_base_overall) > 0:\n",
        "        metrics['deferral_head_score_error_correlation'] = np.corrcoef(deferral_head_scores_np, errors_base_overall)[0, 1]\n",
        "    else:\n",
        "        metrics['deferral_head_score_error_correlation'] = float('nan')\n",
        "\n",
        "    print(f\"\\n--- System Evaluation Metrics for {dataset_name} (Base Model + Deferral Head) ---\")\n",
        "    for key, value in metrics.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {key:<45}: {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {key:<45}: {value}\")\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# --- Conceptual Test (Actual run happens in dataset-specific Stage 2 execution cells) ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Testing Deferral Head Evaluation Utilities (Conceptual) ---\")\n",
        "    # This requires a trained base_model, trained deferral_head, and dataloaders.\n",
        "    if 'trained_mitbih_model' in globals() and trained_mitbih_model is not None and \\\n",
        "       'mitbih_loaders' in globals() and mitbih_loaders is not None and 'val' in mitbih_loaders and \\\n",
        "       'DeferralPredictorHead' in globals() and 'DEFERRAL_HEAD_CONFIG' in globals():\n",
        "\n",
        "        print(\"Conceptual test: Would compute adaptive threshold for a dummy deferral head and evaluate system.\")\n",
        "        print(\"Deferral Head evaluation and thresholding functions defined.\")\n",
        "    else:\n",
        "        print(\"Skipping conceptual Deferral Head evaluation test: missing components.\")\n",
        "\n",
        "print(\"\\nCell 18.D: Evaluation Utilities for System with Deferral Head defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PtSELKn5mL_"
      },
      "outputs": [],
      "source": [
        "# --- Cell 19: Execute Stage 2 for MIT-BIH (Train Deferral Head & Evaluate System) ---\n",
        "\n",
        "print(\"--- Starting Stage 2 for MIT-BIH Dataset ---\")\n",
        "\n",
        "# Ensure necessary variables from previous cells are available\n",
        "if 'UniversalCGDModel' not in globals() or 'DeferralPredictorHead' not in globals() or \\\n",
        "   'train_separate_deferral_head_model' not in globals() or \\\n",
        "   'compute_adaptive_threshold_for_deferral_head' not in globals() or \\\n",
        "   'evaluate_system_with_deferral_head' not in globals():\n",
        "    raise NameError(\"One or more required model/training/evaluation functions are not defined. Please run Cells 9, 18.A, 18.C, 18.D.\")\n",
        "\n",
        "if 'mitbih_loaders' not in globals() or mitbih_loaders is None or \\\n",
        "   not all(k in mitbih_loaders for k in ['train', 'val', 'test']):\n",
        "    raise NameError(\"`mitbih_loaders` are not properly defined. Please ensure Cell 3 (Revised) was run successfully.\")\n",
        "\n",
        "if 'CGD_MODEL_CONFIG' not in globals() or 'ENCODER_CONFIG' not in globals() or \\\n",
        "   'PREDICTOR_CONFIG' not in globals() or 'DEFERRAL_HEAD_CONFIG' not in globals() or \\\n",
        "   'ADAPTIVE_THRESHOLD_CONFIG' not in globals() or 'GENERAL_TRAINING_CONFIG' not in globals() or \\\n",
        "   'MITBIH_NUM_CLASSES' not in globals() or 'MITBIH_CLASS_NAMES' not in globals():\n",
        "    raise NameError(\"One or more required configurations or dataset-specific variables are missing. Please re-run relevant setup cells (2, 15, 18.A example).\")\n",
        "\n",
        "# --- 1. Load the Best Trained Base Classifier from Stage 1 ---\n",
        "base_model_mitbih_path = os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"mitbih_base_classifier_best.pt\")\n",
        "trained_base_mitbih_model = None\n",
        "\n",
        "if os.path.exists(base_model_mitbih_path):\n",
        "    print(f\"Loading Stage 1 base MIT-BIH model from: {base_model_mitbih_path}\")\n",
        "    trained_base_mitbih_model = UniversalCGDModel(\n",
        "        model_config=CGD_MODEL_CONFIG, # Should have loss_beta=0 for Stage 1 training\n",
        "        encoder_config=ENCODER_CONFIG,\n",
        "        predictor_config=PREDICTOR_CONFIG,\n",
        "        perturb_config=PERTURBATION_CONFIG, # Perturb config used during Stage 1 (might be mild)\n",
        "        sensitivity_config=SENSITIVITY_CONFIG, # Sensitivity config used during Stage 1\n",
        "        regularizer_config=STRUCTURAL_REGULARIZER_CONFIG,\n",
        "        output_dim=MITBIH_NUM_CLASSES\n",
        "    ).to(DEVICE)\n",
        "    try:\n",
        "        trained_base_mitbih_model.load_state_dict(torch.load(base_model_mitbih_path, map_location=DEVICE))\n",
        "        trained_base_mitbih_model.eval() # Set to eval mode\n",
        "        # Freeze all parameters of the base model\n",
        "        for param in trained_base_mitbih_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"Stage 1 MIT-BIH base model loaded and frozen successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Stage 1 MIT-BIH base model: {e}. Cannot proceed with Stage 2.\")\n",
        "        trained_base_mitbih_model = None\n",
        "else:\n",
        "    print(f\"ERROR: Stage 1 MIT-BIH base model checkpoint not found at {base_model_mitbih_path}. Please complete Stage 1 training first.\")\n",
        "\n",
        "# --- 2. Train the Separate Deferral Predictor Head ---\n",
        "trained_deferral_head_mitbih = None\n",
        "mitbih_deferral_head_history = {}\n",
        "\n",
        "if trained_base_mitbih_model is not None:\n",
        "    print(\"\\n--- Training Separate Deferral Head for MIT-BIH ---\")\n",
        "    deferral_head_checkpoint_name = \"mitbih_deferral_head_best.pt\"\n",
        "\n",
        "    dh_input_dim = trained_base_mitbih_model.latent_dim\n",
        "\n",
        "    trained_deferral_head_mitbih, mitbih_deferral_head_history = train_separate_deferral_head_model(\n",
        "        base_model_frozen=trained_base_mitbih_model,\n",
        "        deferral_head_input_dim=dh_input_dim,\n",
        "        original_train_loader_for_deferral_data=mitbih_loaders['train'],\n",
        "        original_val_loader_for_deferral_data=mitbih_loaders['val'],\n",
        "        deferral_head_config=DEFERRAL_HEAD_CONFIG,\n",
        "        general_training_config=GENERAL_TRAINING_CONFIG,\n",
        "        device=DEVICE,\n",
        "        deferral_head_checkpoint_name=deferral_head_checkpoint_name\n",
        "    )\n",
        "    print(\"--- MIT-BIH Deferral Head Training Finished ---\")\n",
        "\n",
        "    if trained_deferral_head_mitbih and mitbih_deferral_head_history:\n",
        "        plot_training_history(\n",
        "            mitbih_deferral_head_history,\n",
        "            metrics_to_plot=[\n",
        "                {'train_key': 'train_loss', 'val_key': 'val_loss', 'title': 'MIT-BIH Deferral Head Loss', 'ylabel': 'BCE Loss'},\n",
        "                {'val_key': 'val_accuracy', 'title': 'MIT-BIH Deferral Head Val Acc (Error Pred.)', 'ylabel': 'Accuracy'}\n",
        "            ],\n",
        "            save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"mitbih_deferral_head_history.png\")\n",
        "        )\n",
        "else:\n",
        "    print(\"Skipping Deferral Head training as Stage 1 base model was not loaded.\")\n",
        "\n",
        "# --- 3. Compute Adaptive Threshold for the Trained Deferral Head ---\n",
        "mitbih_adaptive_threshold_for_head = None\n",
        "if trained_base_mitbih_model is not None and trained_deferral_head_mitbih is not None:\n",
        "    print(\"\\n--- Computing Adaptive Threshold for MIT-BIH Deferral Head ---\")\n",
        "    mitbih_adaptive_threshold_for_head = compute_adaptive_threshold_for_deferral_head(\n",
        "        base_model_frozen=trained_base_mitbih_model,\n",
        "        deferral_head=trained_deferral_head_mitbih,\n",
        "        original_val_loader=mitbih_loaders['val'],\n",
        "        adaptive_thresh_config=ADAPTIVE_THRESHOLD_CONFIG,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    print(f\"Computed MIT-BIH Adaptive Threshold for Deferral Head: {mitbih_adaptive_threshold_for_head:.4f}\")\n",
        "else:\n",
        "    print(\"Skipping adaptive threshold computation for deferral head (model or head missing).\")\n",
        "\n",
        "\n",
        "# --- 4. Evaluate the Full System (Base Model + Deferral Head) on Test Set ---\n",
        "mitbih_system_eval_metrics = {} # Initialize to ensure it exists\n",
        "if trained_base_mitbih_model is not None and trained_deferral_head_mitbih is not None and mitbih_adaptive_threshold_for_head is not None:\n",
        "    print(\"\\n--- Evaluating Full System (Base Model + Deferral Head) on MIT-BIH Test Set ---\")\n",
        "    mitbih_system_eval_metrics = evaluate_system_with_deferral_head(\n",
        "        base_model_frozen=trained_base_mitbih_model,\n",
        "        deferral_head=trained_deferral_head_mitbih,\n",
        "        test_loader=mitbih_loaders['test'],\n",
        "        adaptive_threshold_for_head=mitbih_adaptive_threshold_for_head,\n",
        "        device=DEVICE,\n",
        "        dataset_name=\"MIT-BIH Test System\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping full system evaluation (components missing).\")\n",
        "\n",
        "\n",
        "# --- 5. Visualize Final System Performance on MIT-BIH Test Set ---\n",
        "if mitbih_system_eval_metrics:\n",
        "    print(\"\\n--- Visualizing Final MIT-BIH System Test Set Performance ---\")\n",
        "\n",
        "    all_y_true_test_viz_sys = []\n",
        "    all_y_pred_logits_base_test_viz_sys = []\n",
        "    all_deferral_head_scores_test_viz_sys = []\n",
        "\n",
        "    # Ensure models are in eval mode for this visualization data fetching\n",
        "    if trained_base_mitbih_model: trained_base_mitbih_model.eval()\n",
        "    if trained_deferral_head_mitbih: trained_deferral_head_mitbih.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_b, y_b, p_mask_b in tqdm(mitbih_loaders['test'], desc=\"Re-fetching Test Data for System Plots\"):\n",
        "            X_b, y_b, p_mask_b = X_b.to(DEVICE), y_b.to(DEVICE), p_mask_b.to(DEVICE)\n",
        "            base_output = trained_base_mitbih_model(X_b, p_mask_b)\n",
        "            z_original_b = base_output['z_original']\n",
        "            deferral_logits_head_b = trained_deferral_head_mitbih(z_original_b)\n",
        "            deferral_scores_head_b = torch.sigmoid(deferral_logits_head_b).squeeze(-1)\n",
        "\n",
        "            all_y_true_test_viz_sys.append(y_b.cpu())\n",
        "            all_y_pred_logits_base_test_viz_sys.append(base_output['y_pred_logits'].cpu())\n",
        "            all_deferral_head_scores_test_viz_sys.append(deferral_scores_head_b.cpu())\n",
        "\n",
        "    if all_y_true_test_viz_sys:\n",
        "        y_true_test_np_sys = torch.cat(all_y_true_test_viz_sys).numpy().astype(int)\n",
        "        y_pred_logits_base_test_np_sys = torch.cat(all_y_pred_logits_base_test_viz_sys).numpy()\n",
        "        deferral_head_scores_test_np_sys = torch.cat(all_deferral_head_scores_test_viz_sys).numpy()\n",
        "\n",
        "        y_pred_probs_base_test_overall_sys = F.softmax(torch.from_numpy(y_pred_logits_base_test_np_sys), dim=1).numpy()\n",
        "        y_pred_classes_base_test_overall_sys = np.argmax(y_pred_logits_base_test_np_sys, axis=1)\n",
        "\n",
        "        defer_mask_test_eval_head_sys = deferral_head_scores_test_np_sys > mitbih_adaptive_threshold_for_head\n",
        "        non_deferred_mask_test_eval_head_sys = ~defer_mask_test_eval_head_sys\n",
        "\n",
        "        if np.sum(non_deferred_mask_test_eval_head_sys) > 0:\n",
        "            plot_confusion_matrix_custom(\n",
        "                y_true=y_true_test_np_sys[non_deferred_mask_test_eval_head_sys],\n",
        "                y_pred_classes=y_pred_classes_base_test_overall_sys[non_deferred_mask_test_eval_head_sys],\n",
        "                class_names=MITBIH_CLASS_NAMES,\n",
        "                title=\"MIT-BIH System CM (Non-Deferred by Head, Preds by Base)\",\n",
        "                save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"mitbih_system_cm_nd_test.png\")\n",
        "            )\n",
        "            plot_roc_auc_curves(\n",
        "                y_true_list=[y_true_test_np_sys[non_deferred_mask_test_eval_head_sys]],\n",
        "                y_pred_probs_list=[y_pred_probs_base_test_overall_sys[non_deferred_mask_test_eval_head_sys]],\n",
        "                label_names_list=[\"System Non-Deferred Test Samples\"],\n",
        "                output_dim=MITBIH_NUM_CLASSES,\n",
        "                class_names=MITBIH_CLASS_NAMES,\n",
        "                title=\"MIT-BIH System ROC Curve (Non-Deferred by Head, Preds by Base)\",\n",
        "                save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"mitbih_system_roc_nd_test.png\")\n",
        "            )\n",
        "        else:\n",
        "            print(\"No non-deferred samples in MIT-BIH test set for system CM/ROC plots.\")\n",
        "\n",
        "        print(\"\\n--- Plotting MIT-BIH System Deferral Performance Curve (using Validation Set data & Deferral Head scores) ---\")\n",
        "        all_y_true_val_dh_viz = []\n",
        "        all_y_pred_logits_base_val_dh_viz = []\n",
        "        all_deferral_head_scores_val_viz = []\n",
        "\n",
        "        if trained_base_mitbih_model: trained_base_mitbih_model.eval() # Ensure eval mode\n",
        "        if trained_deferral_head_mitbih: trained_deferral_head_mitbih.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_b, y_b, p_mask_b in tqdm(mitbih_loaders['val'], desc=\"Fetching Val Data for System Deferral Curve\"):\n",
        "                X_b, y_b, p_mask_b = X_b.to(DEVICE), y_b.to(DEVICE), p_mask_b.to(DEVICE)\n",
        "                base_output = trained_base_mitbih_model(X_b, p_mask_b)\n",
        "                z_original_b = base_output['z_original']\n",
        "                deferral_logits_head_b = trained_deferral_head_mitbih(z_original_b)\n",
        "                deferral_scores_head_b = torch.sigmoid(deferral_logits_head_b).squeeze(-1)\n",
        "\n",
        "                all_y_true_val_dh_viz.append(y_b.cpu())\n",
        "                all_y_pred_logits_base_val_dh_viz.append(base_output['y_pred_logits'].cpu())\n",
        "                all_deferral_head_scores_val_viz.append(deferral_scores_head_b.cpu())\n",
        "\n",
        "        if all_deferral_head_scores_val_viz: # Check if list is not empty\n",
        "            y_true_val_np_dh_viz = torch.cat(all_y_true_val_dh_viz).numpy()\n",
        "            y_pred_logits_base_val_np_dh_viz = torch.cat(all_y_pred_logits_base_val_dh_viz).numpy()\n",
        "            deferral_head_scores_val_np_viz = torch.cat(all_deferral_head_scores_val_viz).numpy()\n",
        "\n",
        "            plot_deferral_performance_vs_threshold(\n",
        "                sensitivities_val=deferral_head_scores_val_np_viz,\n",
        "                y_true_val=y_true_val_np_dh_viz,\n",
        "                y_pred_logits_val=y_pred_logits_base_val_np_dh_viz,\n",
        "                model_output_dim=MITBIH_NUM_CLASSES,\n",
        "                title=\"MIT-BIH System: Acc_ND (Base Model) vs. Deferral Rate (Deferral Head)\",\n",
        "                save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"mitbih_system_deferral_curve_val.png\")\n",
        "            )\n",
        "        else:\n",
        "            print(\"Could not plot system deferral performance curve: No validation deferral head scores collected.\")\n",
        "    else:\n",
        "        print(\"Skipping system-level test visualizations as test data could not be re-fetched for plots.\")\n",
        "\n",
        "# --- 6. Visualize Explanations for a few Test Samples for the Deferral Head's decision ---\n",
        "# Define explainer_class_ready and captum_ready before the if condition\n",
        "captum_ready = False\n",
        "if 'Saliency' in globals() and Saliency is not None and \\\n",
        "   'IntegratedGradients' in globals() and IntegratedGradients is not None:\n",
        "    captum_ready = True\n",
        "\n",
        "explainer_class_ready = False\n",
        "if 'CGDExplainer' in globals() and callable(CGDExplainer):\n",
        "    explainer_class_ready = True\n",
        "\n",
        "if captum_ready and explainer_class_ready and trained_base_mitbih_model is not None and trained_deferral_head_mitbih is not None:\n",
        "    print(\"\\n--- Visualizing Explanations for MIT-BIH Deferral Head Decisions ---\")\n",
        "\n",
        "    def mitbih_deferral_head_score_for_explainer(X_b, p_mask_b): # Renamed to be specific\n",
        "        # Ensure models are on the correct device within this function's scope if called by Captum\n",
        "        trained_base_mitbih_model.to(X_b.device)\n",
        "        trained_deferral_head_mitbih.to(X_b.device)\n",
        "        base_output = trained_base_mitbih_model(X_b, p_mask_b)\n",
        "        z_orig = base_output['z_original']\n",
        "        dh_logits = trained_deferral_head_mitbih(z_orig)\n",
        "        return torch.sigmoid(dh_logits).squeeze(-1)\n",
        "\n",
        "    try:\n",
        "        mitbih_dh_saliency_explainer = Saliency(mitbih_deferral_head_score_for_explainer)\n",
        "    except Exception as e_captum_init_mitbih_dh:\n",
        "        print(f\"Could not initialize Captum Saliency for MIT-BIH Deferral Head: {e_captum_init_mitbih_dh}\")\n",
        "        mitbih_dh_saliency_explainer = None\n",
        "\n",
        "    num_samples_to_explain = 3\n",
        "    explained_count = 0\n",
        "    try:\n",
        "        for X_explain_batch, y_explain_batch, p_mask_explain_batch in mitbih_loaders['test']:\n",
        "            if explained_count >= num_samples_to_explain: break\n",
        "            samples_to_take_from_batch = min(X_explain_batch.size(0), num_samples_to_explain - explained_count)\n",
        "\n",
        "            X_explain_current = X_explain_batch[:samples_to_take_from_batch].to(DEVICE)\n",
        "            y_explain_current = y_explain_batch[:samples_to_take_from_batch].to(DEVICE)\n",
        "            p_mask_explain_current = p_mask_explain_batch[:samples_to_take_from_batch].to(DEVICE)\n",
        "\n",
        "            for i_spl in range(X_explain_current.size(0)):\n",
        "                if explained_count >= num_samples_to_explain: break\n",
        "                X_s, y_s_true, p_mask_s = X_explain_current[i_spl:i_spl+1], y_explain_current[i_spl], p_mask_explain_current[i_spl:i_spl+1]\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    base_out_s = trained_base_mitbih_model(X_s, p_mask_s)\n",
        "                    z_s = base_out_s['z_original']\n",
        "                    pred_logits_base_s = base_out_s['y_pred_logits']\n",
        "                    pred_class_base_s = torch.argmax(pred_logits_base_s, dim=1).item()\n",
        "                    dh_score_s = torch.sigmoid(trained_deferral_head_mitbih(z_s)).squeeze().item()\n",
        "\n",
        "                is_deferred_by_head = dh_score_s > (mitbih_adaptive_threshold_for_head if mitbih_adaptive_threshold_for_head is not None else 0.5)\n",
        "                true_lbl_name = MITBIH_CLASS_NAMES[y_s_true.item()]\n",
        "                pred_lbl_name = MITBIH_CLASS_NAMES[pred_class_base_s]\n",
        "\n",
        "                print(f\"MIT-BIH System Test Sample {explained_count+1}: True='{true_lbl_name}', BasePred='{pred_lbl_name}', DH_Score={dh_score_s:.3f}, DeferredByHead={is_deferred_by_head}\")\n",
        "\n",
        "                attrs_dh_sens = None\n",
        "                if mitbih_dh_saliency_explainer: # Check if explainer was initialized\n",
        "                    X_s.requires_grad_(True)\n",
        "                    try:\n",
        "                        attrs_dh_sens = mitbih_dh_saliency_explainer.attribute(X_s, additional_forward_args=(p_mask_s,), abs=True)\n",
        "                        attrs_dh_sens = attrs_dh_sens.squeeze().cpu().numpy()\n",
        "                    except Exception as e_attr_dh:\n",
        "                        print(f\"  Error getting saliency for DH score on MIT-BIH sample: {e_attr_dh}\")\n",
        "                        attrs_dh_sens = None\n",
        "                    X_s.requires_grad_(False) # Reset grad requirement\n",
        "\n",
        "                plot_ecg_with_saliency(\n",
        "                        ecg_signal=X_s.squeeze().cpu().numpy(), attributions=attrs_dh_sens,\n",
        "                        title=f\"MIT-BIH Sample {explained_count+1} - Deferral Head Score Explanation\",\n",
        "                        save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], f\"mitbih_system_explain_dh_sample{explained_count}.png\"),\n",
        "                        true_label_name=true_lbl_name, pred_label_name=pred_lbl_name,\n",
        "                        is_deferred=is_deferred_by_head, sensitivity_score=dh_score_s\n",
        "                    )\n",
        "                explained_count += 1\n",
        "            if explained_count >= num_samples_to_explain: break\n",
        "    except Exception as e_final_explain:\n",
        "        print(f\"Error during system explanation visualization for MIT-BIH: {e_final_explain}\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"\\nSkipping MIT-BIH system explanation visualization (components missing or Captum not fully ready).\")\n",
        "\n",
        "\n",
        "print(f\"\\n--- MIT-BIH Stage 2 Analysis Complete. Check '{GENERAL_TRAINING_CONFIG['checkpoint_dir']}' for saved artifacts. ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AATe7f4LGXxX"
      },
      "outputs": [],
      "source": [
        "# --- Cell 20: Execute Stage 2 for PTB (Train Deferral Head & Evaluate System) ---\n",
        "\n",
        "print(\"--- Starting Stage 2 for PTB Dataset ---\")\n",
        "\n",
        "# Ensure necessary variables from previous cells are available\n",
        "if 'UniversalCGDModel' not in globals() or 'DeferralPredictorHead' not in globals() or \\\n",
        "   'train_separate_deferral_head_model' not in globals() or \\\n",
        "   'compute_adaptive_threshold_for_deferral_head' not in globals() or \\\n",
        "   'evaluate_system_with_deferral_head' not in globals():\n",
        "    raise NameError(\"One or more required model/training/evaluation functions are not defined. Please run Cells 9, 18.A, 18.C, 18.D.\")\n",
        "\n",
        "if 'ptb_loaders' not in globals() or ptb_loaders is None or \\\n",
        "   not all(k in ptb_loaders for k in ['train', 'val', 'test']):\n",
        "    raise NameError(\"`ptb_loaders` are not properly defined. Please ensure Cell 3 (Revised) was run successfully and created these DataLoaders for PTB.\")\n",
        "\n",
        "if 'CGD_MODEL_CONFIG' not in globals() or 'ENCODER_CONFIG' not in globals() or \\\n",
        "   'PREDICTOR_CONFIG' not in globals() or 'DEFERRAL_HEAD_CONFIG' not in globals() or \\\n",
        "   'ADAPTIVE_THRESHOLD_CONFIG' not in globals() or 'GENERAL_TRAINING_CONFIG' not in globals() or \\\n",
        "   'PTB_NUM_CLASSES' not in globals() or 'PTB_CLASS_NAMES' not in globals():\n",
        "    raise NameError(\"One or more required configurations or PTB-specific variables are missing. Please re-run relevant setup cells (2, 17, 18.A example).\")\n",
        "\n",
        "# --- 1. Load the Best Trained Base Classifier for PTB from Stage 1 ---\n",
        "#  Ensure this checkpoint name matches what you used for saving the Stage 1 PTB model\n",
        "base_model_ptb_path = os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"ptb_base_classifier_best.pt\")\n",
        "trained_base_ptb_model = None\n",
        "\n",
        "if os.path.exists(base_model_ptb_path):\n",
        "    print(f\"Loading Stage 1 base PTB model from: {base_model_ptb_path}\")\n",
        "    trained_base_ptb_model = UniversalCGDModel(\n",
        "        model_config=CGD_MODEL_CONFIG, # This should be the Stage 1 config (loss_beta=0)\n",
        "        encoder_config=ENCODER_CONFIG,\n",
        "        predictor_config=PREDICTOR_CONFIG,\n",
        "        perturb_config=PERTURBATION_CONFIG, # Config used during Stage 1 base model training\n",
        "        sensitivity_config=SENSITIVITY_CONFIG, # Config used during Stage 1 base model training\n",
        "        regularizer_config=STRUCTURAL_REGULARIZER_CONFIG,\n",
        "        output_dim=PTB_NUM_CLASSES # Should be 1 for PTB with BCE\n",
        "    ).to(DEVICE)\n",
        "    try:\n",
        "        trained_base_ptb_model.load_state_dict(torch.load(base_model_ptb_path, map_location=DEVICE))\n",
        "        trained_base_ptb_model.eval() # Set to eval mode\n",
        "        for param in trained_base_ptb_model.parameters(): # Freeze all parameters\n",
        "            param.requires_grad = False\n",
        "        print(\"Stage 1 PTB base model loaded and frozen successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Stage 1 PTB base model: {e}. Cannot proceed with Stage 2.\")\n",
        "        trained_base_ptb_model = None\n",
        "else:\n",
        "    print(f\"ERROR: Stage 1 PTB base model checkpoint not found at {base_model_ptb_path}. Please complete Stage 1 training for PTB first.\")\n",
        "\n",
        "# --- 2. Train the Separate Deferral Predictor Head for PTB ---\n",
        "trained_deferral_head_ptb = None\n",
        "ptb_deferral_head_history = {}\n",
        "\n",
        "if trained_base_ptb_model is not None:\n",
        "    print(\"\\n--- Training Separate Deferral Head for PTB ---\")\n",
        "    ptb_deferral_head_checkpoint_name = \"ptb_deferral_head_best.pt\"\n",
        "\n",
        "    dh_input_dim_ptb = trained_base_ptb_model.latent_dim\n",
        "\n",
        "    trained_deferral_head_ptb, ptb_deferral_head_history = train_separate_deferral_head_model(\n",
        "        base_model_frozen=trained_base_ptb_model,\n",
        "        deferral_head_input_dim=dh_input_dim_ptb,\n",
        "        original_train_loader_for_deferral_data=ptb_loaders['train'],\n",
        "        original_val_loader_for_deferral_data=ptb_loaders['val'],\n",
        "        deferral_head_config=DEFERRAL_HEAD_CONFIG, # From Cell 2 or defined in 18.A\n",
        "        general_training_config=GENERAL_TRAINING_CONFIG,\n",
        "        device=DEVICE,\n",
        "        deferral_head_checkpoint_name=ptb_deferral_head_checkpoint_name\n",
        "    )\n",
        "    print(\"--- PTB Deferral Head Training Finished ---\")\n",
        "\n",
        "    if trained_deferral_head_ptb and ptb_deferral_head_history:\n",
        "        plot_training_history(\n",
        "            ptb_deferral_head_history,\n",
        "            metrics_to_plot=[\n",
        "                {'train_key': 'train_loss', 'val_key': 'val_loss', 'title': 'PTB Deferral Head Loss', 'ylabel': 'BCE Loss'},\n",
        "                {'val_key': 'val_accuracy', 'title': 'PTB Deferral Head Val Acc (Error Pred.)', 'ylabel': 'Accuracy'}\n",
        "            ],\n",
        "            save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"ptb_deferral_head_history.png\")\n",
        "        )\n",
        "else:\n",
        "    print(\"Skipping PTB Deferral Head training as Stage 1 base model was not loaded.\")\n",
        "\n",
        "# --- 3. Compute Adaptive Threshold for the Trained PTB Deferral Head ---\n",
        "ptb_adaptive_threshold_for_head = None\n",
        "if trained_base_ptb_model is not None and trained_deferral_head_ptb is not None:\n",
        "    print(\"\\n--- Computing Adaptive Threshold for PTB Deferral Head ---\")\n",
        "    ptb_adaptive_threshold_for_head = compute_adaptive_threshold_for_deferral_head(\n",
        "        base_model_frozen=trained_base_ptb_model,\n",
        "        deferral_head=trained_deferral_head_ptb,\n",
        "        original_val_loader=ptb_loaders['val'],\n",
        "        adaptive_thresh_config=ADAPTIVE_THRESHOLD_CONFIG, # From Cell 2\n",
        "        device=DEVICE\n",
        "    )\n",
        "    print(f\"Computed PTB Adaptive Threshold for Deferral Head: {ptb_adaptive_threshold_for_head:.4f}\")\n",
        "else:\n",
        "    print(\"Skipping adaptive threshold computation for PTB deferral head (model or head missing).\")\n",
        "\n",
        "# --- 4. Evaluate the Full System (Base Model + Deferral Head) on PTB Test Set ---\n",
        "ptb_system_eval_metrics = {}\n",
        "if trained_base_ptb_model is not None and trained_deferral_head_ptb is not None and ptb_adaptive_threshold_for_head is not None:\n",
        "    print(\"\\n--- Evaluating Full System (Base Model + Deferral Head) on PTB Test Set ---\")\n",
        "    ptb_system_eval_metrics = evaluate_system_with_deferral_head(\n",
        "        base_model_frozen=trained_base_ptb_model,\n",
        "        deferral_head=trained_deferral_head_ptb,\n",
        "        test_loader=ptb_loaders['test'],\n",
        "        adaptive_threshold_for_head=ptb_adaptive_threshold_for_head,\n",
        "        device=DEVICE,\n",
        "        dataset_name=\"PTB Test System\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping full system evaluation for PTB (components missing).\")\n",
        "\n",
        "# --- 5. Visualize Final System Performance on PTB Test Set ---\n",
        "if ptb_system_eval_metrics:\n",
        "    print(\"\\n--- Visualizing Final PTB System Test Set Performance ---\")\n",
        "\n",
        "    all_y_true_test_ptb_sys = []\n",
        "    all_y_pred_logits_base_test_ptb_sys = []\n",
        "    all_deferral_head_scores_test_ptb_sys = []\n",
        "\n",
        "    trained_base_ptb_model.eval()\n",
        "    trained_deferral_head_ptb.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_b, y_b, p_mask_b in tqdm(ptb_loaders['test'], desc=\"Re-fetching PTB Test Data for System Plots\"):\n",
        "            X_b, y_b, p_mask_b = X_b.to(DEVICE), y_b.to(DEVICE), p_mask_b.to(DEVICE)\n",
        "            base_output = trained_base_ptb_model(X_b, p_mask_b)\n",
        "            z_original_b = base_output['z_original']\n",
        "            deferral_logits_head_b = trained_deferral_head_ptb(z_original_b)\n",
        "            deferral_scores_head_b = torch.sigmoid(deferral_logits_head_b).squeeze(-1)\n",
        "\n",
        "            all_y_true_test_ptb_sys.append(y_b.cpu())\n",
        "            all_y_pred_logits_base_test_ptb_sys.append(base_output['y_pred_logits'].cpu())\n",
        "            all_deferral_head_scores_test_ptb_sys.append(deferral_scores_head_b.cpu())\n",
        "\n",
        "    if all_y_true_test_ptb_sys:\n",
        "        y_true_test_np_ptb_sys = torch.cat(all_y_true_test_ptb_sys).numpy().astype(float) # PTB labels are float\n",
        "        y_pred_logits_base_test_np_ptb_sys = torch.cat(all_y_pred_logits_base_test_ptb_sys).numpy()\n",
        "        deferral_head_scores_test_np_ptb_sys = torch.cat(all_deferral_head_scores_test_ptb_sys).numpy()\n",
        "\n",
        "        y_pred_probs_base_test_overall_ptb_sys = 1 / (1 + np.exp(-y_pred_logits_base_test_np_ptb_sys.squeeze()))\n",
        "        y_pred_classes_base_test_overall_ptb_sys = (y_pred_probs_base_test_overall_ptb_sys > 0.5).astype(int)\n",
        "\n",
        "        defer_mask_test_eval_head_ptb_sys = deferral_head_scores_test_np_ptb_sys > ptb_adaptive_threshold_for_head\n",
        "        non_deferred_mask_test_eval_head_ptb_sys = ~defer_mask_test_eval_head_ptb_sys\n",
        "\n",
        "        if np.sum(non_deferred_mask_test_eval_head_ptb_sys) > 0:\n",
        "            plot_confusion_matrix_custom(\n",
        "                y_true=y_true_test_np_ptb_sys[non_deferred_mask_test_eval_head_ptb_sys].astype(int),\n",
        "                y_pred_classes=y_pred_classes_base_test_overall_ptb_sys[non_deferred_mask_test_eval_head_ptb_sys],\n",
        "                class_names=PTB_CLASS_NAMES,\n",
        "                title=\"PTB System CM (Non-Deferred by Head, Preds by Base)\",\n",
        "                save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"ptb_system_cm_nd_test.png\")\n",
        "            )\n",
        "            plot_roc_auc_curves(\n",
        "                y_true_list=[y_true_test_np_ptb_sys[non_deferred_mask_test_eval_head_ptb_sys]],\n",
        "                y_pred_probs_list=[y_pred_probs_base_test_overall_ptb_sys[non_deferred_mask_test_eval_head_ptb_sys]],\n",
        "                label_names_list=[\"System Non-Deferred PTB Test Samples\"],\n",
        "                output_dim=PTB_NUM_CLASSES, # Should be 1\n",
        "                class_names=PTB_CLASS_NAMES,\n",
        "                title=\"PTB System ROC Curve (Non-Deferred by Head, Preds by Base)\",\n",
        "                save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"ptb_system_roc_nd_test.png\")\n",
        "            )\n",
        "        else:\n",
        "            print(\"No non-deferred samples in PTB test set for system plots (CM, ROC).\")\n",
        "\n",
        "        # Deferral Performance Curve for the Deferral Head's scores on Validation set\n",
        "        print(\"\\n--- Plotting PTB System Deferral Performance Curve (using Validation Set data & Deferral Head scores) ---\")\n",
        "        all_y_true_val_dh_ptb_viz = []\n",
        "        all_y_pred_logits_base_val_dh_ptb_viz = []\n",
        "        all_deferral_head_scores_val_ptb_viz = []\n",
        "\n",
        "        trained_base_ptb_model.eval()\n",
        "        trained_deferral_head_ptb.eval()\n",
        "        with torch.no_grad():\n",
        "            for X_b, y_b, p_mask_b in tqdm(ptb_loaders['val'], desc=\"Fetching PTB Val Data for System Deferral Curve\"):\n",
        "                X_b, y_b, p_mask_b = X_b.to(DEVICE), y_b.to(DEVICE), p_mask_b.to(DEVICE)\n",
        "                base_output = trained_base_ptb_model(X_b, p_mask_b)\n",
        "                z_original_b = base_output['z_original']\n",
        "                deferral_logits_head_b = trained_deferral_head_ptb(z_original_b)\n",
        "                deferral_scores_head_b = torch.sigmoid(deferral_logits_head_b).squeeze(-1)\n",
        "\n",
        "                all_y_true_val_dh_ptb_viz.append(y_b.cpu())\n",
        "                all_y_pred_logits_base_val_dh_ptb_viz.append(base_output['y_pred_logits'].cpu())\n",
        "                all_deferral_head_scores_val_ptb_viz.append(deferral_scores_head_b.cpu())\n",
        "\n",
        "        if all_deferral_head_scores_val_ptb_viz:\n",
        "            y_true_val_np_dh_ptb_viz = torch.cat(all_y_true_val_dh_ptb_viz).numpy()\n",
        "            y_pred_logits_base_val_np_dh_ptb_viz = torch.cat(all_y_pred_logits_base_val_dh_ptb_viz).numpy()\n",
        "            deferral_head_scores_val_np_ptb_viz = torch.cat(all_deferral_head_scores_val_ptb_viz).numpy()\n",
        "\n",
        "            plot_deferral_performance_vs_threshold(\n",
        "                sensitivities_val=deferral_head_scores_val_np_ptb_viz, # Using deferral head's scores\n",
        "                y_true_val=y_true_val_np_dh_ptb_viz,\n",
        "                y_pred_logits_val=y_pred_logits_base_val_np_dh_ptb_viz, # Base model's predictions\n",
        "                model_output_dim=PTB_NUM_CLASSES, # Should be 1\n",
        "                title=\"PTB System: Acc_ND (Base Model) vs. Deferral Rate (Deferral Head)\",\n",
        "                save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], \"ptb_system_deferral_curve_val.png\")\n",
        "            )\n",
        "        else:\n",
        "            print(\"Could not plot PTB system deferral performance curve: No validation deferral head scores collected.\")\n",
        "    else:\n",
        "        print(\"Skipping PTB system-level test visualizations as test data could not be re-fetched for plots.\")\n",
        "\n",
        "# 6. Visualize Explanations for a few PTB Test Samples\n",
        "if 'CGDExplainer' in globals() and 'Saliency' in globals() and Saliency is not None and \\\n",
        "   'IntegratedGradients' in globals() and IntegratedGradients is not None and \\\n",
        "   trained_base_ptb_model is not None and trained_deferral_head_ptb is not None:\n",
        "    print(\"\\n--- Visualizing Explanations for PTB Deferral Head Decisions ---\")\n",
        "\n",
        "    # Define the callable for Captum to explain deferral head scores\n",
        "    def ptb_deferral_head_score_for_explainer(X_b, p_mask_b):\n",
        "        trained_base_ptb_model.to(X_b.device) # Ensure base model is on same device as input\n",
        "        trained_deferral_head_ptb.to(X_b.device) # Ensure deferral head is on same device\n",
        "        base_output = trained_base_ptb_model(X_b, p_mask_b)\n",
        "        z_orig = base_output['z_original']\n",
        "        dh_logits = trained_deferral_head_ptb(z_orig)\n",
        "        return torch.sigmoid(dh_logits).squeeze(-1)\n",
        "\n",
        "    try:\n",
        "        ptb_dh_saliency_explainer = Saliency(ptb_deferral_head_score_for_explainer)\n",
        "    except Exception as e_captum_init:\n",
        "        print(f\"Could not initialize Captum Saliency for PTB Deferral Head: {e_captum_init}\")\n",
        "        ptb_dh_saliency_explainer = None\n",
        "\n",
        "    num_samples_to_explain = 3\n",
        "    explained_count = 0\n",
        "    try:\n",
        "        for X_explain_batch, y_explain_batch, p_mask_explain_batch in ptb_loaders['test']:\n",
        "            if explained_count >= num_samples_to_explain: break\n",
        "            samples_to_take_from_batch = min(X_explain_batch.size(0), num_samples_to_explain - explained_count)\n",
        "\n",
        "            X_explain_current = X_explain_batch[:samples_to_take_from_batch].to(DEVICE)\n",
        "            y_explain_current = y_explain_batch[:samples_to_take_from_batch].to(DEVICE)\n",
        "            p_mask_explain_current = p_mask_explain_batch[:samples_to_take_from_batch].to(DEVICE)\n",
        "\n",
        "            for i_spl in range(X_explain_current.size(0)):\n",
        "                if explained_count >= num_samples_to_explain: break\n",
        "                X_s, y_s_true, p_mask_s = X_explain_current[i_spl:i_spl+1], y_explain_current[i_spl], p_mask_explain_current[i_spl:i_spl+1]\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    base_out_s = trained_base_ptb_model(X_s, p_mask_s)\n",
        "                    z_s = base_out_s['z_original']\n",
        "                    pred_logits_base_s = base_out_s['y_pred_logits']\n",
        "                    pred_prob_base_s = torch.sigmoid(pred_logits_base_s).item()\n",
        "                    pred_class_base_s = 1 if pred_prob_base_s > 0.5 else 0\n",
        "                    dh_score_s = torch.sigmoid(trained_deferral_head_ptb(z_s)).squeeze().item()\n",
        "\n",
        "                is_deferred_by_head = dh_score_s > ptb_adaptive_threshold_for_head\n",
        "                true_lbl_name = PTB_CLASS_NAMES[int(y_s_true.item())]\n",
        "                pred_lbl_name = PTB_CLASS_NAMES[pred_class_base_s]\n",
        "\n",
        "                print(f\"PTB System Test Sample {explained_count+1}: True='{true_lbl_name}', BasePred='{pred_lbl_name}' (Prob={pred_prob_base_s:.2f}), DH_Score={dh_score_s:.3f}, DeferredByHead={is_deferred_by_head}\")\n",
        "\n",
        "                attrs_dh_sens = None\n",
        "                if ptb_dh_saliency_explainer:\n",
        "                    X_s.requires_grad_(True)\n",
        "                    try:\n",
        "                        attrs_dh_sens = ptb_dh_saliency_explainer.attribute(X_s, additional_forward_args=(p_mask_s,), abs=True)\n",
        "                        attrs_dh_sens = attrs_dh_sens.squeeze().cpu().numpy()\n",
        "                    except Exception as e_attr:\n",
        "                        print(f\"  Error getting saliency for DH score: {e_attr}\")\n",
        "                        attrs_dh_sens = None\n",
        "                    X_s.requires_grad_(False)\n",
        "\n",
        "                plot_ecg_with_saliency(\n",
        "                    ecg_signal=X_s.squeeze().cpu().numpy(),\n",
        "                    attributions=attrs_dh_sens, # Attributions for Deferral Head's score\n",
        "                    title=f\"PTB Sample {explained_count+1} - Deferral Head Score Explanation\",\n",
        "                    save_path=os.path.join(GENERAL_TRAINING_CONFIG['checkpoint_dir'], f\"ptb_system_explain_dh_sample{explained_count}.png\"),\n",
        "                    true_label_name=true_lbl_name, pred_label_name=pred_lbl_name,\n",
        "                    is_deferred=is_deferred_by_head, sensitivity_score=dh_score_s # Using DH_Score\n",
        "                )\n",
        "                explained_count += 1\n",
        "            if explained_count >= num_samples_to_explain: break\n",
        "    except Exception as e_final_explain:\n",
        "        print(f\"Error during PTB system explanation visualization: {e_final_explain}\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"\\nSkipping PTB system explanation visualization (components missing or Captum not fully ready).\")\n",
        "\n",
        "print(f\"\\n--- PTB Stage 2 Analysis Complete. Check '{GENERAL_TRAINING_CONFIG['checkpoint_dir']}' for saved artifacts. ---\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}